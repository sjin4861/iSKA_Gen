import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel
from pathlib import Path
import json
import pandas as pd
import yaml

# ==============================================================================
# 1. ì„¤ì •
# ==============================================================================
RM_BASE_MODEL_PATH = "K-intelligence/Midm-2.0-Mini-Instruct"
INPUT_FILE = Path("src/data/rm_testing/ranking_test_data.jsonl") # í‰ê°€í•  ì§€ë¬¸ì´ ë‹´ê¸´ íŒŒì¼
OUTPUT_FILE = Path("src/outputs/ranked_passages_result.jsonl") # ìµœì¢… ê²°ê³¼ ì €ì¥ íŒŒì¼
PROMPT_YAML_PATH = Path("src/config/prompts/iska/preference_eval.yaml")
RM_SAVES_DIR= Path("saves")  # Reward Model ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬
device = "cuda" if torch.cuda.is_available() else "cpu"

# --- âœ¨ ì¤‘ìš”: ê° ë£¨ë¸Œë¦­ê³¼ ì‹¤ì œ í›ˆë ¨ëœ ëª¨ë¸ í´ë”/í”„ë¡¬í”„íŠ¸ ì´ë¦„ì„ ì—°ê²° ---
RM_INFO = {
    "r1_score": {"folder_name": "completeness_for_guidelines", "prompt_key": "completeness_for_guidelines"},
    "r2_score": {"folder_name": "clarity_of_core_theme", "prompt_key": "clarity_of_core_theme"},
    "r3_score": {"folder_name": "reference_groundedness", "prompt_key": "reference_groundedness"},
    "r4_score": {"folder_name": "logical_flow", "prompt_key": "logical_flow"},
    "r5_score": {"folder_name": "korean_quality", "prompt_key": "korean_quality"},
    "r6_score": {"folder_name": "l2_learner_suitability", "prompt_key": "l2_learner_suitability"}
}

# --- âœ¨ í”„ë¡¬í”„íŠ¸ Placeholderë¥¼ ì±„ìš¸ ì •ì  ë°ì´í„° ---
# í‰ê°€ ëŒ€ìƒ ì§€ë¬¸ë“¤ì´ 'íšŒì‹ ë¬¸í™”'ì— ëŒ€í•œ ê²ƒì´ë¯€ë¡œ, ê´€ë ¨ ì •ë³´ë¥¼ ë¯¸ë¦¬ ì •ì˜í•©ë‹ˆë‹¤.
STATIC_SOURCE_ITEM = {
    "korean_topic": "íšŒì‹ ë¬¸í™”",
    "korean_context": "íšŒì‹ì€ í•œêµ­ ì§ì¥ ë¬¸í™”ì˜ ì¤‘ìš”í•œ ë¶€ë¶„ìœ¼ë¡œ, ì—…ë¬´ê°€ ëë‚œ í›„ ë™ë£Œë“¤ê³¼ í•¨ê»˜ ì‹ì‚¬í•˜ë©° ì¹œëª©ì„ ë‹¤ì§€ëŠ” í™œë™ì…ë‹ˆë‹¤...",
    "foreign_topic": "Happy Hour Culture",
    "foreign_context": "Happy hour is a social tradition in many Western countries where colleagues gather at a bar or pub after work...",
    "problem_type1": "ì œëª©ì„ ë¶™ì¸ ê·¼ê±° ì„¤ëª…í•˜ê¸°",
    "problem_type2": "ìë¬¸í™”ì™€ ë¹„êµí•˜ê¸°", 
    "problem_type3": "ì›ì¸ê³¼ ì „ë§ ì˜ˆì¸¡í•˜ê¸°",
    "eval_goal1": "ê¸€ì˜ ì „ì²´ì ì¸ ì£¼ì œ íŒŒì•… ëŠ¥ë ¥ í‰ê°€",
    "eval_goal2": "ë¬¸í™” ë¹„êµ ì„¤ëª… ëŠ¥ë ¥ í‰ê°€",
    "eval_goal3": "ì›ì¸ ì¶”ë¡  ë° ì „ë§ ì˜ˆì¸¡ ëŠ¥ë ¥ í‰ê°€"
}
# --------------------------------------------------

# ==============================================================================
# 2. í—¬í¼ í•¨ìˆ˜
# ==============================================================================
def load_prompts(yaml_path):
    """YAML íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(yaml_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def format_prompt(template, source_item):
    """Jinja2ì™€ ìœ ì‚¬í•œ í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì±„ì›ë‹ˆë‹¤."""
    for key, value in source_item.items():
        template = template.replace(f"{{{{ {key} }}}}", str(value))
    return template

def load_and_merge_model(base_path, adapter_path):
    # ... (ì´ì „ê³¼ ë™ì¼í•œ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜)
    model = AutoModelForSequenceClassification.from_pretrained(
        base_path, num_labels=1, torch_dtype=torch.bfloat16, trust_remote_code=True
    )
    model = PeftModel.from_pretrained(model, adapter_path)
    model = model.merge_and_unload()
    tokenizer = AutoTokenizer.from_pretrained(adapter_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.eos_token_id
    model.to(device).eval()
    return model, tokenizer

def cleanup_model(model, tokenizer):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•©ë‹ˆë‹¤."""
    if model is not None:
        del model
    if tokenizer is not None:
        del tokenizer
    torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬

def get_rm_score(prompt, passage, rm_model, rm_tokenizer):
    """ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì™€ ì§€ë¬¸ìœ¼ë¡œ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    full_text = f"<|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{passage}<|eot_id|>"
    inputs = rm_tokenizer(full_text, return_tensors="pt", truncation=True, max_length=1024).to(device)
    inputs.pop("token_type_ids", None)
    with torch.no_grad():
        return rm_model(**inputs).logits[0].item()

# ==============================================================================
# 3. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# ==============================================================================
if __name__ == "__main__":
    try:
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            passages_data = [json.loads(line) for line in f if line.strip()]
    except FileNotFoundError:
        print(f"âŒ í‰ê°€í•  ì§€ë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_FILE}")
        exit()

    prompts = load_prompts(PROMPT_YAML_PATH)
    
    # ê° RMì— ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ ìƒì„±
    formatted_prompts = {
        score_key: format_prompt(prompts["preference_evaluation"][info['prompt_key']], STATIC_SOURCE_ITEM)
        for score_key, info in RM_INFO.items()
    }

    print("\n" + "="*60)
    print("ìˆœì°¨ì ìœ¼ë¡œ Reward Modelì„ ë¡œë”©í•˜ì—¬ ì±„ì í•©ë‹ˆë‹¤...")
    
    # ê° ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¡œë“œí•˜ê³  ì±„ì 
    for score_key, info in RM_INFO.items():
        print(f"\nğŸ”„ '{info['folder_name']}' ëª¨ë¸ ë¡œë”© ì¤‘...")
        checkpoint_path = max(list((RM_SAVES_DIR / info['folder_name']).glob("checkpoint-*")), key=lambda p: int(p.name.split('-')[-1]))
        
        if checkpoint_path:
            # ëª¨ë¸ ë¡œë“œ
            model, tokenizer = load_and_merge_model(RM_BASE_MODEL_PATH, checkpoint_path)
            prompt_to_use = formatted_prompts[score_key]
            
            print(f"  âœ… '{info['folder_name']}' ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            print(f"  ğŸ”„ ëª¨ë“  ì§€ë¬¸ ì±„ì  ì¤‘...")
            
            # ëª¨ë“  ì§€ë¬¸ì— ëŒ€í•´ í˜„ì¬ ëª¨ë¸ë¡œ ì±„ì 
            for i, data in enumerate(passages_data):
                passage = data['passage']
                score = get_rm_score(prompt_to_use, passage, model, tokenizer)
                data[score_key] = score
                if (i + 1) % 10 == 0 or i == len(passages_data) - 1:
                    print(f"    - ì§„í–‰ìƒí™©: {i+1}/{len(passages_data)} ì§€ë¬¸ ì™„ë£Œ")
            
            print(f"  âœ… '{info['folder_name']}' ëª¨ë¸ ì±„ì  ì™„ë£Œ!")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_model(model, tokenizer)
            print(f"  ğŸ§¹ '{info['folder_name']}' ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ!")
        else:
            print(f"  âŒ '{info['folder_name']}' ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            
    print("\nâœ… ëª¨ë“  ëª¨ë¸ ì±„ì  ì™„ë£Œ!")
    
    # ì´í•© ì ìˆ˜ ê³„ì‚° ë° ìˆœìœ„ ë¶€ì—¬
    df = pd.DataFrame(passages_data)
    score_columns = list(RM_INFO.keys())
    df['total_score'] = df[score_columns].sum(axis=1)
    df = df.sort_values(by="total_score", ascending=False)
    df['rm_ranking'] = range(1, len(df) + 1)
    df = df.drop(columns=['total_score'])
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    df.to_json(OUTPUT_FILE, orient='records', lines=True, force_ascii=False)

    print("\n\n" + "="*60)
    print("ğŸ† ìµœì¢… ì±„ì  ë° ìˆœìœ„ ë¶€ì—¬ ì™„ë£Œ!")
    print("="*60)
    pd.set_option('display.max_colwidth', 50)
    pd.set_option('display.width', 150)
    print(df[['rm_ranking'] + score_columns + ['passage']].round(4))
    print("\nâœ… ëª¨ë“  ê²°ê³¼ê°€ ì•„ë˜ íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
    print(f"   {OUTPUT_FILE.resolve()}")