import json
import numpy as np
import pandas as pd
from pathlib import Path
import re  # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•œ ë¬¸ì¥ ë¶„ë¦¬

def split_sentences(text: str):
    """
    ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ ë¬¸ì¥ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    """
    # í•œêµ­ì–´ ë¬¸ì¥ ë ê¸°í˜¸: ë§ˆì¹¨í‘œ(.), ë¬¼ìŒí‘œ(?), ëŠë‚Œí‘œ(!)
    # ë”°ì˜´í‘œë‚˜ ê´„í˜¸ ë’¤ì— ì˜¤ëŠ” ê²½ìš°ë„ ê³ ë ¤
    sentence_pattern = r'[.!?]+[\s]*'
    sentences = re.split(sentence_pattern, text)
    # ë¹ˆ ë¬¸ìì—´ ì œê±° ë° ê³µë°± ì •ë¦¬
    sentences = [s.strip() for s in sentences if s.strip()]
    return sentences

def preprocess_passage(text: str) -> str:
    """
    ì§€ë¬¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ì—°ì†ëœ ê°œí–‰ ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬
    """
    if not text:
        return text
    
    # ì—°ì†ëœ ê°œí–‰ ë¬¸ì(\n\n, \n\n\n ë“±)ë¥¼ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    cleaned_text = re.sub(r'\n+', ' ', text)
    
    # ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    cleaned_text = cleaned_text.strip()
    
    return cleaned_text

def find_long_passages(file_path: str, length_threshold: int = 500):
    """
    ì§€ì •ëœ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ì§€ë¬¸ë“¤ì„ ì°¾ì•„ì„œ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    input_path = Path(file_path)
    if not input_path.exists():
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {input_path}")
        return

    # JSON íŒŒì¼ ë¡œë“œ
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (json.JSONDecodeError, FileNotFoundError) as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return
    
    if not data:
        print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nğŸ” ê¸¸ì´ê°€ {length_threshold}ìë¥¼ ì´ˆê³¼í•˜ëŠ” ì§€ë¬¸ ê²€ìƒ‰ ê²°ê³¼")
    print("=" * 80)
    
    long_passages = []
    
    for idx, item in enumerate(data):
        if 'generated_passage' in item:
            original_passage = item['generated_passage']
            preprocessed_passage = preprocess_passage(original_passage)
            
            original_length = len(original_passage)
            preprocessed_length = len(preprocessed_passage)
            
            if original_length > length_threshold:
                # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
                korean_topic = "N/A"
                foreign_topic = "N/A"
                if 'source_item' in item:
                    source = item['source_item']
                    korean_topic = source.get('korean_topic', 'N/A')
                    foreign_topic = source.get('foreign_topic', 'N/A')
                
                long_passages.append({
                    'index': idx,
                    'korean_topic': korean_topic,
                    'foreign_topic': foreign_topic,
                    'original_length': original_length,
                    'preprocessed_length': preprocessed_length,
                    'original_passage': original_passage,
                    'preprocessed_passage': preprocessed_passage
                })
    
    if not long_passages:
        print(f"âœ… {length_threshold}ìë¥¼ ì´ˆê³¼í•˜ëŠ” ì§€ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“Š ì´ {len(long_passages)}ê°œì˜ ê¸´ ì§€ë¬¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\n")
    
    for i, passage_info in enumerate(long_passages, 1):
        print(f"ğŸ”¸ {i}ë²ˆì§¸ ê¸´ ì§€ë¬¸ (ì¸ë±ìŠ¤: {passage_info['index']})")
        print(f"   ğŸ“š ì£¼ì œ: {passage_info['korean_topic']} vs {passage_info['foreign_topic']}")
        print(f"   ğŸ“ ì›ë³¸ ê¸¸ì´: {passage_info['original_length']}ì")
        print(f"   ğŸ“ ì „ì²˜ë¦¬ í›„ ê¸¸ì´: {passage_info['preprocessed_length']}ì")
        print(f"   ğŸ“‰ ê¸¸ì´ ê°ì†Œ: {passage_info['original_length'] - passage_info['preprocessed_length']}ì")
        
        print(f"\n   ğŸ“ ì›ë³¸ ì§€ë¬¸:")
        print(f"   {passage_info['original_passage'][:200]}...")
        if len(passage_info['original_passage']) > 200:
            print(f"   ... (ì´ {passage_info['original_length']}ì)")
        
        print(f"\n   âœ¨ ì „ì²˜ë¦¬ í›„ ì§€ë¬¸:")
        print(f"   {passage_info['preprocessed_passage'][:200]}...")
        if len(passage_info['preprocessed_passage']) > 200:
            print(f"   ... (ì´ {passage_info['preprocessed_length']}ì)")
        
        print("\n" + "-" * 80 + "\n")
    
    # í†µê³„ ìš”ì•½
    original_lengths = [p['original_length'] for p in long_passages]
    preprocessed_lengths = [p['preprocessed_length'] for p in long_passages]
    length_reductions = [p['original_length'] - p['preprocessed_length'] for p in long_passages]
    
    print(f"ğŸ“ˆ ê¸´ ì§€ë¬¸ í†µê³„ ìš”ì•½:")
    print(f"   ğŸ“ ì›ë³¸ ê¸¸ì´ - í‰ê· : {np.mean(original_lengths):.1f}ì, ìµœëŒ€: {np.max(original_lengths)}ì, ìµœì†Œ: {np.min(original_lengths)}ì")
    print(f"   âœ¨ ì „ì²˜ë¦¬ í›„ ê¸¸ì´ - í‰ê· : {np.mean(preprocessed_lengths):.1f}ì, ìµœëŒ€: {np.max(preprocessed_lengths)}ì, ìµœì†Œ: {np.min(preprocessed_lengths)}ì")
    print(f"   ğŸ“‰ í‰ê·  ê¸¸ì´ ê°ì†Œ: {np.mean(length_reductions):.1f}ì")
    
    return long_passages

def analyze_passage_statistics(file_path: str):
    """
    JSON í˜•ì‹ì˜ ì§€ë¬¸ ë°ì´í„°ì…‹ í†µê³„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í‘œë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    input_path = Path(file_path)
    if not input_path.exists():
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {input_path}")
        return

    # JSON íŒŒì¼ ë¡œë“œ
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (json.JSONDecodeError, FileNotFoundError) as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return
    
    if not data:
        print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì§€ë¬¸ ë°ì´í„° ì¶”ì¶œ
    passages = []
    korean_topics = []
    foreign_topics = []
    
    for item in data:
        if 'generated_passage' in item:
            passages.append(item['generated_passage'])
        
        # source_itemì—ì„œ í† í”½ ì •ë³´ ì¶”ì¶œ
        if 'source_item' in item:
            source = item['source_item']
            if 'korean_topic' in source:
                korean_topics.append(source['korean_topic'])
            if 'foreign_topic' in source:
                foreign_topics.append(source['foreign_topic'])
    
    if not passages:
        print("ë¶„ì„í•  ì§€ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 1. ê¸°ë³¸ í†µê³„ ê³„ì‚°
    char_counts = [len(p) for p in passages]
    word_counts = [len(p.split()) for p in passages]
    
    # 2. ê°€ë…ì„± í†µê³„ ê³„ì‚° (ë¬¸ì¥ ë¶„ë¦¬)
    sentence_counts = []
    avg_sentence_lengths = []
    for p in passages:
        sentences = split_sentences(p)
        sentence_counts.append(len(sentences))
        if sentences:  # ë¬¸ì¥ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ í‰ê·  ê³„ì‚°
            avg_sentence_lengths.append(np.mean([len(s.split()) for s in sentences]))
        else:
            avg_sentence_lengths.append(0)

    # 3. ì–´íœ˜ ë‹¤ì–‘ì„± í†µê³„ ê³„ì‚°
    all_words = [word for p in passages for word in p.split()]
    total_tokens = len(all_words)
    unique_types = len(set(all_words))
    ttr = unique_types / total_tokens if total_tokens > 0 else 0

    # 4. ê²°ê³¼ ì •ë¦¬ (í•µì‹¬ 5ê°œ ì§€í‘œë§Œ)
    summary = {
        "ì§€í‘œ (Metric)": [
            "ì´ ì§€ë¬¸ ìˆ˜ (Num Passages)",
            "ê¸€ì ìˆ˜ (Characters)",
            "ë‹¨ì–´ ìˆ˜ (Words)",
            "ë¬¸ì¥ ìˆ˜ (Sentences)",
            "í‰ê·  ë¬¸ì¥ ê¸¸ì´ (Avg Sent Length)"
        ],
        "í‰ê·  (Mean)": [
            f"{len(passages)}ê°œ",
            f"{np.mean(char_counts):.2f}",
            f"{np.mean(word_counts):.2f}",
            f"{np.mean(sentence_counts):.2f}",
            f"{np.mean(avg_sentence_lengths):.2f} ë‹¨ì–´"
        ],
        "í‘œì¤€í¸ì°¨ (Std)": [
            "-",
            f"{np.std(char_counts):.2f}",
            f"{np.std(word_counts):.2f}",
            f"{np.std(sentence_counts):.2f}",
            f"{np.std(avg_sentence_lengths):.2f}"
        ],
        "ìµœì†Œ (Min)": [
            "-",
            np.min(char_counts),
            np.min(word_counts),
            np.min(sentence_counts),
            f"{np.min(avg_sentence_lengths):.2f}"
        ],
        "ìµœëŒ€ (Max)": [
            "-",
            np.max(char_counts),
            np.max(word_counts),
            np.max(sentence_counts),
            f"{np.max(avg_sentence_lengths):.2f}"
        ]
    }
    
    df = pd.DataFrame(summary)
    print(f"\nğŸ“Š '{input_path.name}' íŒŒì¼ í†µê³„ ë¶„ì„ ê²°ê³¼")
    print(df.to_string(index=False))
    
    # LaTeX í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
    print(f"\nğŸ“‹ LaTeX í˜•ì‹:")
    print("\\begin{table}[h]")
    print("\\centering")
    print("\\begin{tabular}{|l|c|c|c|c|}")
    print("\\hline")
    print("ì§€í‘œ (Metric) & í‰ê·  (Mean) & í‘œì¤€í¸ì°¨ (Std) & ìµœì†Œ (Min) & ìµœëŒ€ (Max) \\\\")
    print("\\hline")
    
    for i in range(len(summary["ì§€í‘œ (Metric)"])):
        metric = summary["ì§€í‘œ (Metric)"][i]
        mean = summary["í‰ê·  (Mean)"][i]
        std = summary["í‘œì¤€í¸ì°¨ (Std)"][i]
        min_val = summary["ìµœì†Œ (Min)"][i]
        max_val = summary["ìµœëŒ€ (Max)"][i]
        print(f"{metric} & {mean} & {std} & {min_val} & {max_val} \\\\")
    
    print("\\hline")
    print("\\end{tabular}")
    print("\\caption{ì§€ë¬¸ ë°ì´í„°ì…‹ í†µê³„ ë¶„ì„ ê²°ê³¼}")
    print("\\label{tab:passage_statistics}")
    print("\\end{table}")

def analyze_rubric_scores_by_model(evaluation_dir: str):
    """
    ê° ëª¨ë¸ë³„ë¡œ 6ê°œ ë£¨ë¸Œë¦­ì˜ í‰ê·  ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ LaTeX í‘œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        evaluation_dir: í‰ê°€ ê²°ê³¼ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ (src/data/evaluations/2025-08-05/misc)
    """
    eval_path = Path(evaluation_dir)
    if not eval_path.exists():
        print(f"âŒ í‰ê°€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {evaluation_dir}")
        return
    
    # ë£¨ë¸Œë¦­ ì •ì˜
    rubrics = [
        "completeness_for_guidelines",
        "clarity_of_core_theme", 
        "reference_groundedness",
        "logical_flow",
        "korean_quality",
        "l2_learner_suitability"
    ]
    
    # ë£¨ë¸Œë¦­ í•œêµ­ì–´ ì´ë¦„
    rubric_names = {
        "completeness_for_guidelines": "í‰ê°€ ì§€ì¹¨ ì™„ì „ì„±",
        "clarity_of_core_theme": "í•µì‹¬ ì£¼ì œ ëª…í™•ì„±", 
        "reference_groundedness": "ì°¸ê³ ìë£Œ ê¸°ë°˜ì„±",
        "logical_flow": "ë…¼ë¦¬ì  íë¦„",
        "korean_quality": "í•œêµ­ì–´ í’ˆì§ˆ",
        "l2_learner_suitability": "L2 í•™ìŠµì ì í•©ì„±"
    }
    
    model_scores = {}
    
    # ê° ëª¨ë¸ë³„ í‰ê°€ íŒŒì¼ ì°¾ê¸°
    for eval_file in eval_path.rglob("*.json"):
        if "eval_rubric" in eval_file.name:
            # ëª¨ë¸ëª… ì¶”ì¶œ
            parts = eval_file.parts
            model_name = "unknown"
            for part in parts:
                if "_evaluation" in part:
                    model_name = part.replace("_evaluation", "")
                    break
            
            if model_name == "unknown":
                continue
                
            try:
                with open(eval_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if model_name not in model_scores:
                    model_scores[model_name] = {rubric: [] for rubric in rubrics}
                
                # ê° í•­ëª©ì—ì„œ ì ìˆ˜ ì¶”ì¶œ
                for item in data:
                    if 'evaluation' not in item:
                        continue
                    
                    evaluation = item['evaluation']
                    for rubric in rubrics:
                        score_key = f"{rubric}_score"
                        if score_key in evaluation:
                            model_scores[model_name][rubric].append(evaluation[score_key])
                            
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {eval_file.name} - {e}")
    
    if not model_scores:
        print("âŒ í‰ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í‰ê·  ê³„ì‚°
    model_averages = {}
    for model_name, scores in model_scores.items():
        model_averages[model_name] = {}
        for rubric in rubrics:
            if scores[rubric]:
                model_averages[model_name][rubric] = np.mean(scores[rubric])
            else:
                model_averages[model_name][rubric] = 0.0
    
    # ëª¨ë¸ëª… ì •ë¦¬ (ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ í•˜ì´í”ˆìœ¼ë¡œ ë³€ê²½)
    clean_model_names = {}
    for model_name in model_averages.keys():
        clean_name = model_name.replace("_", "-")
        clean_model_names[model_name] = clean_name
    
    print(f"\nğŸ“Š ëª¨ë¸ë³„ ë£¨ë¸Œë¦­ ì ìˆ˜ ë¶„ì„ ê²°ê³¼")
    print("=" * 100)
    
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬
    df_data = {}
    df_data['ë£¨ë¸Œë¦­'] = [rubric_names[rubric] for rubric in rubrics]
    
    for model_name in sorted(model_averages.keys()):
        clean_name = clean_model_names[model_name]
        df_data[clean_name] = [f"{model_averages[model_name][rubric]:.2f}" for rubric in rubrics]
    
    df = pd.DataFrame(df_data)
    print(df.to_string(index=False))
    
    # LaTeX í‘œ ìƒì„±
    print(f"\nğŸ“‹ LaTeX í‘œ í˜•ì‹:")
    print("\\begin{table}[h]")
    print("\\centering")
    print("\\small")  # í‘œ í¬ê¸° ì¡°ì •
    
    # ëª¨ë¸ ìˆ˜ì— ë”°ë¼ ì»¬ëŸ¼ ìˆ˜ ì¡°ì •
    model_names_sorted = sorted([clean_model_names[name] for name in model_averages.keys()])
    num_models = len(model_names_sorted)
    
    # í…Œì´ë¸” í—¤ë” ìƒì„±
    col_spec = "|l|" + "c|" * num_models
    print(f"\\begin{{tabular}}{{{col_spec}}}")
    print("\\hline")
    
    # í—¤ë” í–‰ ìƒì„±
    header = "ë£¨ë¸Œë¦­ (Rubric)"
    for model_name in model_names_sorted:
        header += f" & {model_name}"
    header += " \\\\"
    print(header)
    print("\\hline")
    
    # ë°ì´í„° í–‰ ìƒì„±
    for i, rubric in enumerate(rubrics):
        row = rubric_names[rubric]
        for model_name in model_averages.keys():
            clean_name = clean_model_names[model_name]
            if clean_name in model_names_sorted:
                score = model_averages[model_name][rubric]
                row += f" & {score:.2f}"
        row += " \\\\"
        print(row)
    
    print("\\hline")
    
    # ì „ì²´ í‰ê·  ê³„ì‚° ë° ì¶”ê°€
    overall_averages = {}
    for model_name in model_averages.keys():
        scores = [model_averages[model_name][rubric] for rubric in rubrics]
        overall_averages[model_name] = np.mean(scores)
    
    # ì „ì²´ í‰ê·  í–‰
    avg_row = "\\textbf{ì „ì²´ í‰ê· }"
    for model_name in model_averages.keys():
        clean_name = clean_model_names[model_name]
        if clean_name in model_names_sorted:
            avg_score = overall_averages[model_name]
            avg_row += f" & \\textbf{{{avg_score:.2f}}}"
    avg_row += " \\\\"
    print(avg_row)
    print("\\hline")
    
    print("\\end{tabular}")
    print("\\caption{ëª¨ë¸ë³„ ë£¨ë¸Œë¦­ í‰ê°€ ì ìˆ˜ ë¹„êµ}")
    print("\\label{tab:model_rubric_scores}")
    print("\\end{table}")
    
    # í†µê³„ ìš”ì•½
    print(f"\nğŸ“ˆ í†µê³„ ìš”ì•½:")
    for model_name in sorted(model_averages.keys()):
        clean_name = clean_model_names[model_name]
        total_items = sum(len(model_scores[model_name][rubric]) for rubric in rubrics)
        avg_score = overall_averages[model_name]
        print(f"   {clean_name}: í‰ê·  {avg_score:.2f}ì  (ì´ {total_items}ê°œ í‰ê°€)")
    
    return model_averages

def analyze_passage_quality(file_path: str):
    """
    ì§€ë¬¸ì˜ í’ˆì§ˆ ê´€ë ¨ ì„¸ë¶€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (ì‚¬ìš© ì•ˆí•¨)
    """
    pass

# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    DATASET_PATH = "src/data/raw_outputs/2025-08-05/passage/A.X-4.0-Light/create_passage_rubric_aware/benchmark_2_v1.0.0_passage_agent.create_passage_rubric_aware.json"
    DATASET_PATH_2 = "src/data/raw_outputs/2025-07-28/passage/Gemini-2.5-Pro/create_passage_rubric_aware/benchmark_1_v1.0.0_passage_agent.create_passage_rubric_aware.json"
    EVALUATION_DIR = "src/data/evaluations/2025-08-05/misc"  # í‰ê°€ ê²°ê³¼ ë””ë ‰í† ë¦¬
    
    # 1. ê¸°ë³¸ í†µê³„ ë¶„ì„
    # analyze_passage_statistics(DATASET_PATH_2)
    
    # 2. ê¸¸ì´ 500ì ì´ˆê³¼ ì§€ë¬¸ ê²€ìƒ‰ ë° ì „ì²˜ë¦¬
    # print("\n" + "=" * 80)
    # find_long_passages(DATASET_PATH_2, length_threshold=500)
    
    # 3. ëª¨ë¸ë³„ ë£¨ë¸Œë¦­ ì ìˆ˜ ë¶„ì„
    analyze_rubric_scores_by_model(EVALUATION_DIR)

