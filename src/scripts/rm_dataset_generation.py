import sys
from pathlib import Path
import json
import random
from datetime import datetime
from utils.rm_dataset_generator import (
    RMDatasetGenerator,
    create_spf_dataset,
    create_imp_dataset,
    create_icp_dataset,
)

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append(str(Path.cwd().parent))
sys.path.append(str(Path.cwd().parent / 'modules'))
sys.path.append(str(Path.cwd().parent / 'utils'))
print("âœ… ê²½ë¡œ ì„¤ì • ì™„ë£Œ")

# ê¸°ë³¸ ì„¤ì •
OPENAI_MODEL = 'gpt-4o'
TARGET_PAIRS_PER_RUBRIC = 50  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ê²Œ ì„¤ì •
RANDOM_SEED = 42
random.seed(RANDOM_SEED)

print(f'ğŸ¯ ì„¤ì • ì™„ë£Œ:')
print(f'   OpenAI ëª¨ë¸: {OPENAI_MODEL}')
print(f'   ë£¨ë¸Œë¦­ë‹¹ ëª©í‘œ ìŒ ìˆ˜: {TARGET_PAIRS_PER_RUBRIC}')
print(f'   ëœë¤ ì‹œë“œ: {RANDOM_SEED}')

# ìƒ˜í”Œ ê¸°ë³¸ ì§€ë¬¸ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ê¸°ì¡´ ìƒì„±ëœ ì§€ë¬¸ì„ ì‚¬ìš©)
sample_base_passages = [
    {
        'id': f'passage_{i:03d}',
        'text': f'ì´ê²ƒì€ ìƒ˜í”Œ ì§€ë¬¸ {i}ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì‘ì„±ëœ êµìœ¡ìš© í…ìŠ¤íŠ¸ë¡œ, ë‹¤ì–‘í•œ ì£¼ì œë¥¼ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ë¬¸ë²•ì ìœ¼ë¡œ ì •í™•í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, êµìœ¡ ëª©ì ì— ì í•©í•œ ë‚´ìš©ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì§€ë¬¸ì€ í•™ìŠµìë“¤ì´ í•œêµ­ì–´ ì½ê¸° ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.',
        'topic': f'ì£¼ì œ_{i%10}',
        'source': 'sample_generation',
        'created_at': datetime.now().isoformat()
    }
    for i in range(1, 101)
]
sample_file_path = Path.cwd().parent / 'data' / 'base_passages' / 'sample_passages_100.json'
sample_file_path.parent.mkdir(exist_ok=True)
with open(sample_file_path, 'w', encoding='utf-8') as f:
    json.dump(sample_base_passages, f, ensure_ascii=False, indent=2)
print(f'ğŸ“ ìƒ˜í”Œ ê¸°ë³¸ ì§€ë¬¸ ìƒì„± ì™„ë£Œ: {len(sample_base_passages)}ê°œ')
print(f'ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {sample_file_path}')

# SPF ë°ì´í„°ì…‹ ìƒì„±
print('ğŸš€ SPF ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...')
spf_dataset, spf_files = create_spf_dataset(
    passages_file='sample_passages_100.json',
    target_pairs_per_rubric=TARGET_PAIRS_PER_RUBRIC,
    openai_model=OPENAI_MODEL,
)
print('\nâœ… SPF ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!')
print('ğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:')
for rubric, file_path in spf_files.items():
    print(f'   {rubric}: {Path(file_path).name}')

# IMP ë°ì´í„°ì…‹ ìƒì„±
high_perf_passages = [
    {
        'id': f'high_perf_{i:03d}',
        'text': f'ê³ ì„±ëŠ¥ ëª¨ë¸ì´ ìƒì„±í•œ ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ì§€ë¬¸ {i}ì…ë‹ˆë‹¤. ë¬¸ë²•ì ìœ¼ë¡œ ì •í™•í•˜ê³ , ì£¼ì œì— ì¶©ì‹¤í•˜ë©°, ë…¼ë¦¬ì ìœ¼ë¡œ ì˜ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í‘œí˜„ì´ ìì—°ìŠ¤ëŸ½ê³  ì™„ì „í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.',
        'model': 'high_performance_7B',
        'rubric_achievement_rate': 0.98,
        'created_at': datetime.now().isoformat()
    }
    for i in range(1, 51)
]
low_perf_passages = [
    {
        'id': f'low_perf_{i:03d}',
        'text': f'ì €ì„±ëŠ¥ ëª¨ë¸ì´ ìƒì„±í•œ ì§€ë¬¸ {i}ì…ë‹ˆë‹¤. ë¬¸ë²•ì— ì¼ë¶€ ì˜¤ë¥˜ê°€ ìˆê³ , ì£¼ì œì—ì„œ ë²—ì–´ë‚˜ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë©°, ë…¼ë¦¬ì  êµ¬ì„±ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
        'model': 'low_performance_3B',
        'rubric_achievement_rate': 0.40,
        'created_at': datetime.now().isoformat()
    }
    for i in range(1, 51)
]

high_perf_path = Path.cwd().parent / 'data' / 'base_passages' / 'high_performance_passages.json'
low_perf_path = Path.cwd().parent / 'data' / 'base_passages' / 'low_performance_passages.json'

with open(high_perf_path, 'w', encoding='utf-8') as f:
    json.dump(high_perf_passages, f, ensure_ascii=False, indent=2)
with open(low_perf_path, 'w', encoding='utf-8') as f:
    json.dump(low_perf_passages, f, ensure_ascii=False, indent=2)

print(f'ğŸ“ ê³ ì„±ëŠ¥ ëª¨ë¸ ì§€ë¬¸: {len(high_perf_passages)}ê°œ')
print(f'ğŸ“ ì €ì„±ëŠ¥ ëª¨ë¸ ì§€ë¬¸: {len(low_perf_passages)}ê°œ')
print(f'ğŸ’¾ ì €ì¥ ì™„ë£Œ')
print('ğŸš€ IMP ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...')

imp_dataset, imp_files = create_imp_dataset(
    high_perf_file='high_performance_passages.json',
    low_perf_file='low_performance_passages.json',
    target_pairs_per_rubric=TARGET_PAIRS_PER_RUBRIC,
    openai_model=OPENAI_MODEL,
)
print('\nâœ… IMP ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!')
print('ğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:')
for rubric, file_path in imp_files.items():
    print(f'   {rubric}: {Path(file_path).name}')

# ICP ë°ì´í„°ì…‹ ìƒì„±
print('ğŸš€ ICP ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...')
icp_dataset, icp_files = create_icp_dataset(
    base_passages_file='sample_passages_100.json',
    target_pairs_per_rubric=TARGET_PAIRS_PER_RUBRIC,
    openai_model=OPENAI_MODEL,
)
print('\nâœ… ICP ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!')
print('ğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:')
for rubric, file_path in icp_files.items():
    print(f'   {rubric}: {Path(file_path).name}')

# ë°ì´í„°ì…‹ í†µê³„ ì¶œë ¥
generator = RMDatasetGenerator(OPENAI_MODEL)
print('ğŸ“Š ìƒì„±ëœ ë°ì´í„°ì…‹ í†µê³„ ìš”ì•½:')
print('\n1. SPF ë°ì´í„°ì…‹:')
spf_stats = generator.get_dataset_stats(spf_dataset)
for rubric, count in spf_stats['pairs_per_rubric'].items():
    print(f'   {rubric}: {count}ê°œ ìŒ')
print(f'   ì´í•©: {spf_stats['total_pairs']}ê°œ ìŒ')
print('\n2. IMP ë°ì´í„°ì…‹:')
imp_stats = generator.get_dataset_stats(imp_dataset)
for rubric, count in imp_stats['pairs_per_rubric'].items():
    print(f'   {rubric}: {count}ê°œ ìŒ')
print(f'   ì´í•©: {imp_stats['total_pairs']}ê°œ ìŒ')
print('\n3. ICP ë°ì´í„°ì…‹:')
icp_stats = generator.get_dataset_stats(icp_dataset)
for rubric, count in icp_stats['pairs_per_rubric'].items():
    print(f'   {rubric}: {count}ê°œ ìŒ')
print(f'   ì´í•©: {icp_stats['total_pairs']}ê°œ ìŒ')
total_pairs = spf_stats['total_pairs'] + imp_stats['total_pairs'] + icp_stats['total_pairs']
print(f'\nğŸ¯ ì „ì²´ ìƒì„±ëœ ì„ í˜¸ë„ ìŒ: {total_pairs}ê°œ')

# ìƒ˜í”Œ ë°ì´í„° í™•ì¸
rubric_keys = [
    'completeness_for_guidelines',
    'core_theme_clarity',
    'reference_groundedness',
    'logical_flow_and_structure',
    'korean_quality',
    'l2_learner_suitability'
]
print('ğŸ“ ìƒ˜í”Œ ë°ì´í„° í™•ì¸:\n')
# SPF ìƒ˜í”Œ
for rubric in rubric_keys:
    if rubric in spf_dataset and spf_dataset[rubric]:
        spf_sample = spf_dataset[rubric][0]
        print(f'1. SPF ìƒ˜í”Œ ({rubric}):')
        print(f'   ìŒ ID: {spf_sample['pair_id']}')
        print(f'   Chosen: {spf_sample['chosen'][:100]}...')
        print(f'   Rejected: {spf_sample['rejected'][:100]}...')
        print()
# IMP ìƒ˜í”Œ
for rubric in rubric_keys:
    if rubric in imp_dataset and imp_dataset[rubric]:
        imp_sample = imp_dataset[rubric][0]
        print(f'2. IMP ìƒ˜í”Œ ({rubric}):')
        print(f'   ìŒ ID: {imp_sample['pair_id']}')
        print(f'   Chosen: {imp_sample['chosen'][:100]}...')
        print(f'   Rejected: {imp_sample['rejected'][:100]}...')
        print()
# ICP ìƒ˜í”Œ
for rubric in rubric_keys:
    if rubric in icp_dataset and icp_dataset[rubric]:
        icp_sample = icp_dataset[rubric][0]
        print(f'3. ICP ìƒ˜í”Œ ({rubric}):')
        print(f'   ìŒ ID: {icp_sample['pair_id']}')
        print(f'   Chosen: {icp_sample['chosen'][:100]}...')
        print(f'   Rejected: {icp_sample['rejected'][:100]}...')
        print()

print("\nìƒì„±ëœ ë°ì´í„°ì…‹ì€ ë‹¤ìŒê³¼ ê°™ì´ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
print("1. Reward Model í›ˆë ¨: ê° ë£¨ë¸Œë¦­ë³„ë¡œ ìƒì„±ëœ ì„ í˜¸ë„ ìŒì„ ì‚¬ìš©í•˜ì—¬ RM í›ˆë ¨")
print("2. ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦: ìƒì„±ëœ ìŒì˜ í’ˆì§ˆì„ ìˆ˜ë™ìœ¼ë¡œ ê²€ì¦")
print("3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •: ë” ë§ì€ ë°ì´í„°ë‚˜ ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ ì¬ìƒì„±")
print("4. ëª¨ë¸ í‰ê°€: í›ˆë ¨ëœ RMì˜ ì„±ëŠ¥ í‰ê°€")
print("\në°ì´í„°ì…‹ íŒŒì¼ë“¤ì€ src/data/rm_training/ ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤.")
