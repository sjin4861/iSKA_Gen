import json
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel
import yaml
import sys
from pathlib import Path
# --- 1. í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì • ---
PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

# --- 2. ëª¨ë“ˆ ì„í¬íŠ¸ ---
from src.model_loader import load_model_for_reward_training

BASE_MODEL = "K-intelligence/Midm-2.0-Mini-Instruct"
ADAPTER_PATH = "./saves/quick_test_rm_chat/checkpoint-50" # í›ˆë ¨ ê²°ê³¼ ê²½ë¡œ
TRAIN_DATA_PATH = "src/data/korean_rm_chat_train.jsonl"
EVAL_DATA_PATH = "src/data/korean_rm_chat_eval.jsonl"
device = "cuda" if torch.cuda.is_available() else "cpu"

def load_and_merge_model(base_path, adapter_path):
    """ë² ì´ìŠ¤ ëª¨ë¸ì— ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•˜ê³  ë³‘í•©í•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    print(f"\nğŸ”„ '{adapter_path}'ì—ì„œ ëª¨ë¸ ë¡œë”© ë° ë³‘í•© ì‹œì‘...")
    
    # 1. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    model = AutoModelForSequenceClassification.from_pretrained(
        base_path,
        num_labels=1,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    # 2. LoRA ì–´ëŒ‘í„° ì ìš©
    model = PeftModel.from_pretrained(model, adapter_path)
    
    # 3. ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ì™„ì „íˆ ë³‘í•©
    model = model.merge_and_unload()
    print("  - âœ… ëª¨ë¸ ë³‘í•© ì™„ë£Œ!")
    
    # 4. í† í¬ë‚˜ì´ì € ë¡œë“œ ë° pad_token ì„¤ì •
    tokenizer = AutoTokenizer.from_pretrained(adapter_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.eos_token_id
        
    model.to(device).eval()
    return model, tokenizer

def get_score(prompt: str, response: str, model, tokenizer):
    """ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µìœ¼ë¡œ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    # í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ f-string í…œí”Œë¦¿ê³¼ ë™ì¼í•˜ê²Œ êµ¬ì„±
    full_text = (
        f"<|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|>"
        f"<|start_header_id|>assistant<|end_header_id|>\n\n{response}<|eot_id|>"
    )
    inputs = tokenizer(full_text, return_tensors="pt", truncation=True, max_length=1024).to(device)
    inputs.pop("token_type_ids", None)
    
    with torch.no_grad():
        return model(**inputs).logits[0].item()

def evaluate_dataset(data_path: str, model, tokenizer):
    """ì£¼ì–´ì§„ ë°ì´í„°ì…‹ íŒŒì¼ë¡œ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
    correct_predictions = 0
    total_predictions = 0
    detailed_results = [] # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    with open(data_path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            data = json.loads(line)
            prompt = data["prompt"]
            chosen = data["chosen"]
            rejected = data["rejected"]
            
            score_chosen = get_score(prompt, chosen, model, tokenizer)
            score_rejected = get_score(prompt, rejected, model, tokenizer)
            
            # âœ¨ **í•µì‹¬ ë³€ê²½: ì ìˆ˜ ì°¨ì´ ê³„ì‚° ë° ìƒì„¸ ê²°ê³¼ ì €ì¥**
            score_diff = score_chosen - score_rejected
            
            prediction = "Correct" if score_diff > 0 else "Incorrect"
            
            detailed_results.append({
                "pair_id": i + 1,
                "chosen_score": score_chosen,
                "rejected_score": score_rejected,
                "score_difference": score_diff,
                "prediction": prediction,
            })
            # ---------------------------------------------
            
            if score_chosen > score_rejected:
                correct_predictions += 1
            total_predictions += 1
            
    accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0
    return accuracy, total_predictions, detailed_results
# ==============================================================================
# 3. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# ==============================================================================

if __name__ == "__main__":
    try:
        model, tokenizer = load_and_merge_model(BASE_MODEL, ADAPTER_PATH)
        # --- í›ˆë ¨ ë°ì´í„°ì…‹ í‰ê°€ ---
        print("\n--- í›ˆë ¨ ë°ì´í„°ì…‹ í‰ê°€ ì‹œì‘ ---")
        train_accuracy, train_total, train_results = evaluate_dataset(TRAIN_DATA_PATH, model, tokenizer)
        
        # --- ê²€ì¦ ë°ì´í„°ì…‹ í‰ê°€ ---
        print("\n--- ê²€ì¦ ë°ì´í„°ì…‹ í‰ê°€ ì‹œì‘ ---")
        eval_accuracy, eval_total, eval_results = evaluate_dataset(EVAL_DATA_PATH, model, tokenizer)

        # --- ìµœì¢… ê²°ê³¼ ì¶œë ¥ ---
        print("\n" + "="*50)
        print("ğŸ† ìµœì¢… í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print("="*50)
        
        # í›ˆë ¨ì…‹ ê²°ê³¼ ìš”ì•½
        if train_total > 0:
            train_avg_diff = sum(r['score_difference'] for r in train_results) / len(train_results)
            print(f"í›ˆë ¨ ë°ì´í„°ì…‹ ({train_total}ê°œ ìƒ˜í”Œ):")
            print(f"  - ì •í™•ë„: {train_accuracy:.2f}%")
            print(f"  - í‰ê·  ì ìˆ˜ ì°¨ì´ (Chosen - Rejected): {train_avg_diff:.4f}")

        # ê²€ì¦ì…‹ ê²°ê³¼ ìš”ì•½
        if eval_total > 0:
            eval_avg_diff = sum(r['score_difference'] for r in eval_results) / len(eval_results)
            print(f"\nê²€ì¦ ë°ì´í„°ì…‹ ({eval_total}ê°œ ìƒ˜í”Œ):")
            print(f"  - ì •í™•ë„: {eval_accuracy:.2f}%")
            print(f"  - í‰ê·  ì ìˆ˜ ì°¨ì´ (Chosen - Rejected): {eval_avg_diff:.4f}")

        # ê²€ì¦ì…‹ ìƒì„¸ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
        print("\n--- ê²€ì¦ì…‹ ìƒì„¸ ê²°ê³¼ ìƒ˜í”Œ ---")
        for result in eval_results[:3]: # ì²˜ìŒ 3ê°œ ìƒ˜í”Œë§Œ ì¶œë ¥
            print(
                f"Pair #{result['pair_id']}: "
                f"Chosen Score={result['chosen_score']:.2f}, "
                f"Rejected Score={result['rejected_score']:.2f}, "
                f"Diff={result['score_difference']:.2f} "
                f"-> {result['prediction']}"
            )
        print("="*50)
        
    except Exception as e:
        print(f"\nâŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")