import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Any, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path.cwd().parent.parent))
sys.path.append(str(Path.cwd().parent / 'modules'))

from modules.iska.passage_eval import PassageEvaluator
from modules.model_client import OpenAIModelClient, LocalModelClient
from utils.benchmark_loader import get_guideline_by_id
from utils.output_loader import load_passages, debug_available_files
from utils.output_saver import save_model_output, DEFAULT_EVALUATION_DIR


def evaluate_passages(
    benchmark_file: str,
    passage_model_name: str,
    evaluator_model: str = "gpt-4o-mini",
    benchmark_version: str = "v1.0.0",
    template_key: str = "passage_eval.binary_rubric",
    passage_template_key: Optional[str] = None,
    BENCH_ID_LIST: List[int] = [1, 2, 3, 4, 5],
    date_str: Optional[str] = None
) -> Dict[int, List[Dict[str, Any]]]:
    """
    íŠ¹ì • ëª¨ë¸ì´ ìƒì„±í•œ passageë“¤ì„ ë²¤ì¹˜ë§ˆí¬ì— ë”°ë¼ í‰ê°€í•©ë‹ˆë‹¤.
    
    Args:
        benchmark_file (str): ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ ì´ë¦„
        passage_model_name (str): í‰ê°€í•  ëª¨ë¸ ì´ë¦„ (passage ìƒì„±í•œ ëª¨ë¸)
        evaluator_model (str): í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: gpt-4o-mini)
        benchmark_version (str): ë²¤ì¹˜ë§ˆí¬ ë²„ì „
        template_key (str): í‰ê°€ í…œí”Œë¦¿ í‚¤
        passage_template_key (Optional[str]): passage ìƒì„±ì— ì‚¬ìš©ëœ í…œí”Œë¦¿ í‚¤ (Noneì´ë©´ ìë™ ê²€ìƒ‰)
        BENCH_ID_LIST (List[int]): í‰ê°€í•  ë²¤ì¹˜ë§ˆí¬ ID ë¦¬ìŠ¤íŠ¸
        date_str (Optional[str]): íŠ¹ì • ë‚ ì§œì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•  ë•Œ ì‚¬ìš© (Noneì´ë©´ ìµœì‹  ë°ì´í„°)
        
    Returns:
        Dict[int, List[Dict[str, Any]]]: ë²¤ì¹˜ë§ˆí¬ IDë³„ í‰ê°€ ê²°ê³¼
    """
    
    # 1. í‰ê°€ì ì„¤ì •
    if evaluator_model.startswith("gpt"):
        evaluator_client = OpenAIModelClient(model_name=evaluator_model)
    else:
        evaluator_client = LocalModelClient(model_name=evaluator_model)
    
    evaluator = PassageEvaluator(llm_client=evaluator_client)
    
    # 2. ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ

    all_results = {}
    
    for benchmark_id in BENCH_ID_LIST:
        print(f"\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ ID {benchmark_id} í‰ê°€ ì¤‘...")
        
        # 3. ë²¤ì¹˜ë§ˆí¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        guideline = get_guideline_by_id(benchmark_file, benchmark_id)
        if not guideline:
            print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ID {benchmark_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
        problem_types = guideline["problem_types"]
        eval_goals = guideline["eval_goals"]

        # 4. í•´ë‹¹ ëª¨ë¸ì´ ìƒì„±í•œ passage ë°ì´í„° ë¡œë“œ
        passages = load_passages(
            model_name=passage_model_name,
            benchmark_id=benchmark_id,
            benchmark_version=benchmark_version,
            template_key=passage_template_key,  # ì§€ì •ëœ í…œí”Œë¦¿ í‚¤ ì‚¬ìš© (Noneì´ë©´ ìë™ ê²€ìƒ‰)
            date_str=date_str
        )
        
        if not passages:
            print(f"âŒ ëª¨ë¸ '{passage_model_name}'ì˜ ë²¤ì¹˜ë§ˆí¬ ID {benchmark_id} passage ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ” ë””ë²„ê¹… ì •ë³´:")
            debug_available_files(passage_model_name)
            continue
        
        print(f"âœ… {len(passages)}ê°œì˜ passage ë¡œë“œ ì™„ë£Œ")
        
        # 5. ê° passage í‰ê°€
        results = []
        for i, passage_data in enumerate(passages):
            print(f"  ğŸ“ Passage {i+1}/{len(passages)} í‰ê°€ ì¤‘...")
            
            source_item = passage_data["source_item"]
            korean_topic = source_item['korean_topic']
            foreign_topic = source_item['foreign_topic']
            korean_context = source_item['korean_context']
            foreign_context = source_item['foreign_context']
            passage = passage_data['generated_passage']

            # í‰ê°€ ì‹¤í–‰
            if "binary" in template_key:
                result = evaluator.evaluate_binary_rubric(
                    problem_types=problem_types,
                    eval_goals=eval_goals,
                    korean_topic=korean_topic,
                    foreign_topic=foreign_topic,
                    korean_context=korean_context,
                    foreign_context=foreign_context,
                    passage=passage,
                    template_key=template_key
                )
            else:
                result = evaluator.evaluate_passage_metrics(
                    problem_types=problem_types,
                    eval_goals=eval_goals,
                    home_topic=korean_topic,
                    foreign_topic=foreign_topic,
                    home_context=korean_context,
                    foreign_context=foreign_context,
                    passage=passage,
                )

            # ì›ë³¸ ë°ì´í„°ì™€ í‰ê°€ ê²°ê³¼ë¥¼ í•¨ê»˜ ì €ì¥
            evaluation_result = {
                "source_item": source_item,
                "generated_passage": passage,
                "evaluation": result
            }
            results.append(evaluation_result)
        
        # 6. ê²°ê³¼ ì €ì¥ (evaluations ë””ë ‰í† ë¦¬ì— ì €ì¥)
        saved_file = save_model_output(
            model_name=f"{passage_model_name}_evaluation",
            benchmark_id=benchmark_id,
            benchmark_version=benchmark_version,
            template_key=f"eval_{template_key.split('.')[-1]}",  # eval_binary_rubric
            data=results,
            base_dir=DEFAULT_EVALUATION_DIR,  # evaluations ë””ë ‰í† ë¦¬ ì‚¬ìš©
            date_str=date_str
        )
        
        print(f"âœ… ë²¤ì¹˜ë§ˆí¬ ID {benchmark_id} í‰ê°€ ì™„ë£Œ ë° ì €ì¥: {saved_file}")
        
        # ì²« ë²ˆì§¸ ê²°ê³¼ ì˜ˆì‹œ ì¶œë ¥
        if results:
            print(f"ğŸ“‹ ì²« ë²ˆì§¸ í‰ê°€ ê²°ê³¼ ì˜ˆì‹œ:")
            print(json.dumps(results[0]["evaluation"], ensure_ascii=False, indent=2))
        
        all_results[benchmark_id] = results
    
    return all_results


def evaluate_single_benchmark(
    benchmark_file: str,
    passage_model_name: str,
    benchmark_id: int,
    evaluator_model: str = "gpt-4o-mini",
    benchmark_version: str = "v1.0.0",
    template_key: str = "passage_eval.binary_rubric",
    passage_template_key: Optional[str] = None,
    date_str: Optional[str] = None
) -> Optional[List[Dict[str, Any]]]:
    """
    íŠ¹ì • ë²¤ì¹˜ë§ˆí¬ IDì— ëŒ€í•´ì„œë§Œ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        benchmark_file (str): ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ ì´ë¦„
        passage_model_name (str): í‰ê°€í•  ëª¨ë¸ ì´ë¦„
        benchmark_id (int): í‰ê°€í•  ë²¤ì¹˜ë§ˆí¬ ID
        evaluator_model (str): í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸
        benchmark_version (str): ë²¤ì¹˜ë§ˆí¬ ë²„ì „
        template_key (str): í‰ê°€ í…œí”Œë¦¿ í‚¤
        passage_template_key (Optional[str]): passage ìƒì„±ì— ì‚¬ìš©ëœ í…œí”Œë¦¿ í‚¤ (Noneì´ë©´ ìë™ ê²€ìƒ‰)
        date_str (Optional[str]): íŠ¹ì • ë‚ ì§œì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•  ë•Œ ì‚¬ìš© (Noneì´ë©´ ìµœì‹  ë°ì´í„°)
        
    Returns:
        Optional[List[Dict[str, Any]]]: í‰ê°€ ê²°ê³¼ ë˜ëŠ” None
    """
    results = evaluate_passages(
        benchmark_file=benchmark_file,
        passage_model_name=passage_model_name,
        evaluator_model=evaluator_model,
        benchmark_version=benchmark_version,
        template_key=template_key,
        passage_template_key=passage_template_key,
        BENCH_ID_LIST=[benchmark_id],
        date_str=date_str
    )
    
    return results.get(benchmark_id)


# --- ì‹¤í–‰ ì˜ˆì‹œ ---
if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    MODEL_LIST = ["EXAONE-3.5B", "Qwen-8B"]  # í‰ê°€í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
    BENCH_ID_LIST = [1, 2, 3]  # í‰ê°€í•  ë²¤ì¹˜ë§ˆí¬ ID ë¦¬ìŠ¤íŠ¸
    
    print("ğŸ” Passage í‰ê°€ ì‹œì‘...")
    
    for passage_model_name in MODEL_LIST:
        print(f"\nğŸ¤– ëª¨ë¸ '{passage_model_name}' í‰ê°€ ì¤‘...")
        
        try:
            results = evaluate_passages(
                benchmark_file="v1/iSKA-Gen_Benchmark_v1.0.0_20250725_Initial.json",
                passage_model_name=passage_model_name,
                evaluator_model="gpt-4o-mini",
                benchmark_version="v1.0.0",
                template_key="passage_eval.binary_rubric",  # ì˜¬ë°”ë¥¸ í…œí”Œë¦¿ í‚¤ ì‚¬ìš©
                BENCH_ID_LIST=BENCH_ID_LIST
            )
            
            print(f"âœ… ëª¨ë¸ '{passage_model_name}' í‰ê°€ ì™„ë£Œ")
            
            # ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥
            for benchmark_id, benchmark_results in results.items():
                print(f"  ğŸ“Š ë²¤ì¹˜ë§ˆí¬ {benchmark_id}: {len(benchmark_results)}ê°œ í‰ê°€ ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ '{passage_model_name}' í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    print("\nğŸ‰ ëª¨ë“  í‰ê°€ ì™„ë£Œ!")