import sys
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path.cwd().parent))
sys.path.append(str(Path.cwd().parent / 'modules'))
sys.path.append(str(Path.cwd().parent / 'utils'))

from modules.iska.reward_model import RewardModel, compare_passage_pairs_with_reward_model
from modules.model_client import OpenAIModelClient


def load_preference_ranking_benchmark(
    benchmark_file: str
) -> List[Dict[str, Any]]:
    """
    Preference Ranking ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        benchmark_file (str): ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ëª… (ì˜ˆ: "v1/iSKA-Gen_Benchmark_v1.0.0_20250726_PreferenceRanking.json")
        
    Returns:
        List[Dict[str, Any]]: ë¡œë“œëœ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°
    """
    try:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
        data_dir = Path.cwd().parent / "data" / "benchmarks"
        file_path = data_dir / benchmark_file
        
        if not file_path.exists():
            raise FileNotFoundError(f"ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if not isinstance(raw_data, list):
            raw_data = [raw_data]
        
        print(f"âœ… Preference Ranking ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ ì™„ë£Œ: {len(raw_data)}ê°œ í•­ëª©")
        return raw_data
        
    except Exception as e:
        print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


def convert_to_pairwise_format(
    preference_data: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Preference Ranking ë°ì´í„°ë¥¼ pairwise ë¹„êµ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        preference_data (List[Dict[str, Any]]): ì›ë³¸ preference ë°ì´í„°
        
    Returns:
        List[Dict[str, Any]]: pairwise í˜•íƒœë¡œ ë³€í™˜ëœ ë°ì´í„°
    """
    # ê³ í’ˆì§ˆê³¼ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ ë¶„ë¦¬
    high_quality = [item for item in preference_data if item.get("quality") == "high"]
    low_quality = [item for item in preference_data if item.get("quality") == "low"]
    
    print(f"ğŸ“Š ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸: {len(high_quality)}ê°œ, ì €í’ˆì§ˆ í…ìŠ¤íŠ¸: {len(low_quality)}ê°œ")
    
    # ì£¼ì œë³„ë¡œ ë§¤ì¹­
    pairs = []
    matched_topics = set()
    
    for high_item in high_quality:
        # ì£¼ì œ ì¶”ì¶œ - "ê³ í’ˆì§ˆ_ìˆ«ì_" ë¶€ë¶„ ì œê±°
        high_name = high_item["name"]
        if high_name.startswith("ê³ í’ˆì§ˆ_"):
            # "ê³ í’ˆì§ˆ_01_íšŒì‹_ë¬¸í™”" -> "íšŒì‹_ë¬¸í™”"
            topic = "_".join(high_name.split("_")[2:])
            
            # ê°™ì€ ì£¼ì œì˜ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ ì°¾ê¸°
            matching_low = None
            for low_item in low_quality:
                low_name = low_item["name"]
                if low_name.startswith("ì €í’ˆì§ˆ_"):
                    low_topic = "_".join(low_name.split("_")[2:])
                    if low_topic == topic:
                        matching_low = low_item
                        break
        
            if matching_low:
                pair = {
                    "pair_id": f"preference_pair_{topic}",
                    "topic": topic,
                    "chosen": high_item["text"],
                    "rejected": matching_low["text"],
                    "chosen_quality": "high",
                    "rejected_quality": "low",
                    "metadata": {
                        "chosen_name": high_item["name"],
                        "rejected_name": matching_low["name"]
                    }
                }
                pairs.append(pair)
                matched_topics.add(topic)
            else:
                print(f"âš ï¸ ì£¼ì œ '{topic}'ì— ëŒ€í•œ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"âœ… {len(pairs)}ê°œì˜ pairwise ë¹„êµ ìŒ ìƒì„± ì™„ë£Œ.")
    return pairs


def evaluate_gpt4_preference_alignment(
    pairwise_data: List[Dict[str, Any]],
    openai_model: str = "gpt-4o"
) -> Dict[str, Any]:
    """
    GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ í‰ê°€í•˜ê³  preference alignmentë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
    
    Args:
        pairwise_data (List[Dict[str, Any]]): pairwise í˜•íƒœì˜ í‰ê°€ ë°ì´í„°
        openai_model (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸ (ê¸°ë³¸ê°’: "gpt-4o")
        
    Returns:
        Dict[str, Any]: GPT-4o í‰ê°€ ê²°ê³¼
    """
    if not pairwise_data:
        return {"error": "í‰ê°€í•  pairwise ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    print(f"ğŸ¤– GPT-4o í’ˆì§ˆ í‰ê°€ ì‹œì‘ (ëª¨ë¸: {openai_model})...")
    
    try:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = OpenAIModelClient(
            model_name=openai_model,
            api_key=None  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ
        )
        
        evaluation_prompt = """
ë‹¤ìŒ ë‘ í…ìŠ¤íŠ¸ë¥¼ ë¹„êµí•˜ì—¬ ì–´ëŠ ê²ƒì´ ë” ë†’ì€ í’ˆì§ˆì„ ê°€ì§€ëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸ A:
{text_a}

í…ìŠ¤íŠ¸ B:
{text_b}

í‰ê°€ ê¸°ì¤€:
1. ë¬¸ë²•ì˜ ì •í™•ì„±
2. í‘œí˜„ì˜ ìì—°ìŠ¤ëŸ¬ì›€
3. ë‚´ìš©ì˜ ì¼ê´€ì„±ê³¼ ë…¼ë¦¬ì„±
4. ì–´íœ˜ ì„ íƒì˜ ì ì ˆì„±

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:
ì„ íƒ: A ë˜ëŠ” B
ì´ìœ : [ê°„ë‹¨í•œ ì´ìœ ]
"""
        
        results = []
        correct_predictions = 0
        
        for i, pair in enumerate(pairwise_data, 1):
            print(f"   ğŸ“ {i}/{len(pairwise_data)} í‰ê°€ ì¤‘...")
            
            # 50% í™•ë¥ ë¡œ ìˆœì„œ ë°”ê¾¸ê¸° (bias ë°©ì§€)
            import random
            if random.random() < 0.5:
                text_a, text_b = pair["chosen"], pair["rejected"]
                correct_choice = "A"
            else:
                text_a, text_b = pair["rejected"], pair["chosen"]
                correct_choice = "B"
            
            prompt = evaluation_prompt.format(text_a=text_a, text_b=text_b)
            
            try:
                response = client.generate_content(prompt)
                
                # GPT-4o ì‘ë‹µì—ì„œ ì„ íƒ ì¶”ì¶œ
                gpt_choice = None
                if "ì„ íƒ: A" in response or "ì„ íƒ:A" in response:
                    gpt_choice = "A"
                elif "ì„ íƒ: B" in response or "ì„ íƒ:B" in response:
                    gpt_choice = "B"
                
                is_correct = (gpt_choice == correct_choice)
                if is_correct:
                    correct_predictions += 1
                
                results.append({
                    "pair_id": pair["pair_id"],
                    "topic": pair["topic"],
                    "gpt_choice": gpt_choice,
                    "correct_choice": correct_choice,
                    "is_correct": is_correct,
                    "gpt_response": response,
                    "text_order": {"A": text_a[:50] + "...", "B": text_b[:50] + "..."}
                })
                
            except Exception as e:
                print(f"   âš ï¸ GPT-4o í‰ê°€ ì‹¤íŒ¨: {e}")
                results.append({
                    "pair_id": pair["pair_id"],
                    "topic": pair["topic"],
                    "error": str(e)
                })
        
        accuracy = correct_predictions / len(pairwise_data) if pairwise_data else 0
        
        print(f"âœ… GPT-4o í‰ê°€ ì™„ë£Œ: {correct_predictions}/{len(pairwise_data)} ì •í™•ë„ = {accuracy:.2%}")
        
        return {
            "model": openai_model,
            "total_pairs": len(pairwise_data),
            "correct_predictions": correct_predictions,
            "accuracy": accuracy,
            "detailed_results": results
        }
        
    except Exception as e:
        print(f"âŒ GPT-4o í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": str(e)}


def compare_reward_model_vs_gpt4(
    reward_model_results: Dict[str, Any],
    gpt4_results: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Reward Modelê³¼ GPT-4oì˜ í‰ê°€ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    
    Args:
        reward_model_results: Reward Model í‰ê°€ ê²°ê³¼
        gpt4_results: GPT-4o í‰ê°€ ê²°ê³¼
        
    Returns:
        Dict[str, Any]: ë¹„êµ ë¶„ì„ ê²°ê³¼
    """
    if "error" in reward_model_results or "error" in gpt4_results:
        return {"error": "í‰ê°€ ê²°ê³¼ì— ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤."}
    
    print(f"ğŸ” Reward Model vs GPT-4o ë¹„êµ ë¶„ì„...")
    
    # ê²°ê³¼ ë§¤ì¹­
    rm_details = {result["pair_id"]: result for result in reward_model_results.get("detailed_results", [])}
    gpt_details = {result["pair_id"]: result for result in gpt4_results.get("detailed_results", [])}
    
    # ê³µí†µ pair_id ì°¾ê¸°
    common_pairs = set(rm_details.keys()) & set(gpt_details.keys())
    
    if not common_pairs:
        return {"error": "ë¹„êµí•  ê³µí†µ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    # ìƒì„¸ ë¹„êµ
    detailed_comparisons = []
    agreements = 0
    
    for pair_id in common_pairs:
        rm_result = rm_details[pair_id]
        gpt_result = gpt_details[pair_id]
        
        # Reward Modelì´ chosenì„ ì„ í˜¸í•˜ëŠ”ì§€ í™•ì¸
        rm_prefers_chosen = rm_result.get("prefers_chosen", False)
        
        # GPT-4oê°€ chosenì„ ì„ í˜¸í•˜ëŠ”ì§€ í™•ì¸
        gpt_prefers_chosen = gpt_result.get("is_correct", False)
        
        # ë‘ ëª¨ë¸ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        agreement = (rm_prefers_chosen == gpt_prefers_chosen)
        if agreement:
            agreements += 1
        
        detailed_comparisons.append({
            "pair_id": pair_id,
            "topic": rm_result.get("topic", "Unknown"),
            "rm_prefers_chosen": rm_prefers_chosen,
            "gpt_prefers_chosen": gpt_prefers_chosen,
            "agreement": agreement,
            "rm_score_diff": rm_result.get("score_difference", 0),
            "gpt_choice": gpt_result.get("gpt_choice", "Unknown")
        })
    
    agreement_rate = agreements / len(common_pairs) if common_pairs else 0
    
    result = {
        "total_comparisons": len(common_pairs),
        "agreements": agreements,
        "disagreements": len(common_pairs) - agreements,
        "agreement_rate": agreement_rate,
        "reward_model_accuracy": reward_model_results.get("accuracy", 0),
        "gpt4_accuracy": gpt4_results.get("accuracy", 0),
        "detailed_comparisons": detailed_comparisons
    }
    
    print(f"âœ… ë¹„êµ ì™„ë£Œ: {len(common_pairs)}ê°œ ìŒ, ì¼ì¹˜ë„ {agreement_rate:.2%}")
    
    return result


def run_preference_ranking_evaluation(
    reward_model_path: str,
    benchmark_file: str,
    openai_model: str = "gpt-4o",
    max_length: int = 512,
    device: str = "auto"
) -> Dict[str, Any]:
    """
    Preference Ranking ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Reward Modelê³¼ GPT-4oì˜ alignmentë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
    
    Args:
        reward_model_path (str): Reward Model ê²½ë¡œ
        benchmark_file (str): ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ëª…
        openai_model (str): OpenAI ëª¨ë¸ëª…
        max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
        device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        
    Returns:
        Dict[str, Any]: ì „ì²´ í‰ê°€ ê²°ê³¼
    """
    print(f"ğŸš€ Preference Ranking í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
    
    try:
        # 1. ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë“œ
        print(f"\n1ï¸âƒ£ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë“œ:")
        preference_data = load_preference_ranking_benchmark(benchmark_file)
        
        if not preference_data:
            return {"error": "ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        # 2. Pairwise í˜•íƒœë¡œ ë³€í™˜
        print(f"\n2ï¸âƒ£ Pairwise í˜•íƒœë¡œ ë³€í™˜:")
        pairwise_data = convert_to_pairwise_format(preference_data)
        
        if not pairwise_data:
            return {"error": "Pairwise ë°ì´í„° ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}
        
        # 3. Reward Model í‰ê°€
        print(f"\n3ï¸âƒ£ Reward Model í‰ê°€:")
        reward_model = RewardModel(
            model_path=reward_model_path,
            device=device,
            max_length=max_length
        )
        
        rm_results = compare_passage_pairs_with_reward_model(
            reward_model=reward_model,
            pairwise_data=pairwise_data
        )
        
        if "error" in rm_results:
            return {"error": f"Reward Model í‰ê°€ ì‹¤íŒ¨: {rm_results['error']}"}
        
        # 4. GPT-4o í‰ê°€
        print(f"\n4ï¸âƒ£ GPT-4o í‰ê°€:")
        gpt4_results = evaluate_gpt4_preference_alignment(
            pairwise_data=pairwise_data,
            openai_model=openai_model
        )
        
        if "error" in gpt4_results:
            return {"error": f"GPT-4o í‰ê°€ ì‹¤íŒ¨: {gpt4_results['error']}"}
        
        # 5. ê²°ê³¼ ë¹„êµ
        print(f"\n5ï¸âƒ£ ê²°ê³¼ ë¹„êµ:")
        comparison_result = compare_reward_model_vs_gpt4(
            reward_model_results=rm_results,
            gpt4_results=gpt4_results
        )
        
        if "error" in comparison_result:
            return {"error": f"ê²°ê³¼ ë¹„êµ ì‹¤íŒ¨: {comparison_result['error']}"}
        
        print(f"\nğŸ‰ Preference Ranking í‰ê°€ ì™„ë£Œ!")
        
        return {
            "benchmark_file": benchmark_file,
            "reward_model_path": reward_model_path,
            "openai_model": openai_model,
            "total_pairs": len(pairwise_data),
            "reward_model_result": rm_results,
            "gpt4_result": gpt4_results,
            "comparison_result": comparison_result,
            "evaluation_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"error": str(e)}


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    result = run_preference_ranking_evaluation(
        reward_model_path="~/models/train_2025-07-26-12-04-23",
        benchmark_file="v1/iSKA-Gen_Benchmark_v1.0.0_20250726_PreferenceRanking.json",
        openai_model="gpt-4o",
        max_length=256
    )
    
    if "error" not in result:
        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"   ì´ í‰ê°€ ìŒ: {result['total_pairs']}ê°œ")
        comparison = result["comparison_result"]
        print(f"   Reward Model ì •í™•ë„: {comparison.get('reward_model_accuracy', 0):.2%}")
        print(f"   GPT-4o ì •í™•ë„: {comparison.get('gpt4_accuracy', 0):.2%}")
        print(f"   ë‘ ëª¨ë¸ ì¼ì¹˜ë„: {comparison.get('agreement_rate', 0):.2%}")
    else:
        print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {result['error']}")
