import sys
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path.cwd().parent))
sys.path.append(str(Path.cwd().parent / 'modules'))
sys.path.append(str(Path.cwd().parent / 'utils'))

from modules.iska.reward_model import RewardModel, compare_passage_pairs_with_reward_model
from modules.model_client import OpenAIModelClient
from utils.output_loader import load_passages
from utils.output_saver import save_model_output, DEFAULT_EVALUATION_DIR
from utils.benchmark_loader import load_benchmarks, get_benchmark_by_id


def evaluate_passage_preferences(
    reward_model_path: str,
    model_name: str,
    benchmark_file: str,
    benchmark_version: str = "v1.0.0",
    BENCH_ID_LIST: List[int] = [1, 2, 3, 4, 5],
    chosen_template: str = "passage_agent.create_passage",
    rejected_template: str = "passage_agent.create_passage_with_korean_errors",
    device: str = "auto",
    max_length: int = 512,
    seed: int = 42
) -> Dict[str, Any]:
    """
    Reward Modelì„ ì‚¬ìš©í•˜ì—¬ passage preferenceë¥¼ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        reward_model_path (str): Reward Model ê²½ë¡œ
        model_name (str): í‰ê°€í•  ëª¨ë¸ ì´ë¦„
        benchmark_file (str): ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ëª…
        benchmark_version (str): ë²¤ì¹˜ë§ˆí¬ ë²„ì „
        BENCH_ID_LIST (List[int]): í‰ê°€í•  ë²¤ì¹˜ë§ˆí¬ ID ë¦¬ìŠ¤íŠ¸
        chosen_template (str): ì„ í˜¸ë˜ëŠ” í…œí”Œë¦¿ í‚¤
        rejected_template (str): ê±°ë¶€ë˜ëŠ” í…œí”Œë¦¿ í‚¤
        device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
        seed (int): ëœë¤ ì‹œë“œ
        
    Returns:
        Dict[str, Any]: í‰ê°€ ê²°ê³¼
    """
    print(f"ğŸ¯ Passage Preference í‰ê°€ ì‹œì‘...")
    print(f"   ëª¨ë¸: {model_name}")
    print(f"   Reward Model: {reward_model_path}")
    print(f"   Chosen í…œí”Œë¦¿: {chosen_template}")
    print(f"   Rejected í…œí”Œë¦¿: {rejected_template}")
    
    # ì „ì²´ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    evaluation_results = {
        "model_name": model_name,
        "reward_model_path": reward_model_path,
        "chosen_template": chosen_template,
        "rejected_template": rejected_template,
        "benchmark_version": benchmark_version,
        "evaluation_timestamp": datetime.now().isoformat(),
        "benchmark_results": [],
        "overall_statistics": {}
    }
    
    all_comparisons = []
    total_pairs = 0
    total_correct = 0
    
    # ê° ë²¤ì¹˜ë§ˆí¬ë³„ë¡œ í‰ê°€ ìˆ˜í–‰
    for benchmark_id in BENCH_ID_LIST:
        print(f"\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ ID {benchmark_id} í‰ê°€ ì¤‘...")
        
        # 1. ë²¤ì¹˜ë§ˆí¬ ì •ë³´ ë¡œë“œ
        benchmark = get_benchmark_by_id(benchmark_file, benchmark_id)
        if not benchmark:
            print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ID {benchmark_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        # 2. ë‘ ê°€ì§€ í…œí”Œë¦¿ìœ¼ë¡œ ìƒì„±ëœ passage ë¡œë“œ
        chosen_passages = load_passages(
            model_name=model_name,
            benchmark_id=benchmark_id,
            benchmark_version=benchmark_version,
            template_key=chosen_template
        )
        
        rejected_passages = load_passages(
            model_name=model_name,
            benchmark_id=benchmark_id,
            benchmark_version=benchmark_version,
            template_key=rejected_template
        )
        
        if not chosen_passages:
            print(f"âŒ '{chosen_template}' í…œí”Œë¦¿ì˜ passageë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
            
        if not rejected_passages:
            print(f"âŒ '{rejected_template}' í…œí”Œë¦¿ì˜ passageë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        print(f"âœ… Chosen passages: {len(chosen_passages)}ê°œ")
        print(f"âœ… Rejected passages: {len(rejected_passages)}ê°œ")
        
        # 3. Passage pair ìƒì„± ë° í‰ê°€
        benchmark_pairs = create_passage_pairs_for_evaluation(
            chosen_passages, 
            rejected_passages, 
            benchmark_id
        )
        
        if not benchmark_pairs:
            print(f"âš ï¸ ë²¤ì¹˜ë§ˆí¬ ID {benchmark_id}: í‰ê°€í•  pairê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        # 4. Reward Modelë¡œ í‰ê°€
        comparison_result = compare_passage_pairs_with_reward_model(
            model_path=reward_model_path,
            passage_pairs=benchmark_pairs,
            device=device,
            max_length=max_length,
            seed=seed
        )
        
        # 5. ê²°ê³¼ ì§‘ê³„
        benchmark_result = {
            "benchmark_id": benchmark_id,
            "benchmark_info": {
                "problem_types": benchmark.get("problem_types", []),
                "eval_goals": benchmark.get("eval_goals", [])
            },
            "total_pairs": comparison_result["total_pairs"],
            "correct_preferences": comparison_result["correct_preferences"],
            "accuracy": comparison_result["accuracy"],
            "comparisons": comparison_result["comparisons"]
        }
        
        evaluation_results["benchmark_results"].append(benchmark_result)
        all_comparisons.extend(comparison_result["comparisons"])
        total_pairs += comparison_result["total_pairs"]
        total_correct += comparison_result["correct_preferences"]
        
        print(f"âœ… ë²¤ì¹˜ë§ˆí¬ ID {benchmark_id}: {comparison_result['accuracy']:.2%} ì •í™•ë„")
    
    # 6. ì „ì²´ í†µê³„ ê³„ì‚°
    overall_accuracy = total_correct / total_pairs if total_pairs > 0 else 0
    
    evaluation_results["overall_statistics"] = {
        "total_benchmarks": len(evaluation_results["benchmark_results"]),
        "total_pairs": total_pairs,
        "total_correct_preferences": total_correct,
        "overall_accuracy": overall_accuracy,
        "accuracy_by_benchmark": {
            result["benchmark_id"]: result["accuracy"] 
            for result in evaluation_results["benchmark_results"]
        }
    }
    
    print(f"\nğŸ‰ ì „ì²´ í‰ê°€ ì™„ë£Œ!")
    print(f"   ì „ì²´ ì •í™•ë„: {overall_accuracy:.2%}")
    print(f"   ì´ í‰ê°€ pair: {total_pairs}ê°œ")
    print(f"   ì •í™•í•œ ì„ í˜¸ë„: {total_correct}ê°œ")
    
    return evaluation_results


def create_passage_pairs_for_evaluation(
    chosen_passages: List[Dict[str, Any]], 
    rejected_passages: List[Dict[str, Any]], 
    benchmark_id: int
) -> List[Dict[str, Any]]:
    """
    í‰ê°€ìš© passage pair ìƒì„±
    
    Args:
        chosen_passages (List[Dict[str, Any]]): ì„ í˜¸ë˜ëŠ” passage ë¦¬ìŠ¤íŠ¸
        rejected_passages (List[Dict[str, Any]]): ê±°ë¶€ë˜ëŠ” passage ë¦¬ìŠ¤íŠ¸
        benchmark_id (int): ë²¤ì¹˜ë§ˆí¬ ID
        
    Returns:
        List[Dict[str, Any]]: í‰ê°€ìš© passage pair ë¦¬ìŠ¤íŠ¸
    """
    pairs = []
    
    min_length = min(len(chosen_passages), len(rejected_passages))
    
    for i in range(min_length):
        chosen_data = chosen_passages[i]
        rejected_data = rejected_passages[i]
        
        # source_itemì´ ë™ì¼í•œì§€ í™•ì¸
        chosen_source = chosen_data.get("source_item", {})
        rejected_source = rejected_data.get("source_item", {})
        
        if chosen_source == rejected_source:
            pair = {
                "pair_id": f"benchmark_{benchmark_id}_item_{i}",
                "chosen": chosen_data["generated_passage"],
                "rejected": rejected_data["generated_passage"],
                "source_item": chosen_source,
                "benchmark_id": benchmark_id
            }
            pairs.append(pair)
        else:
            print(f"âš ï¸ Item {i}: source_itemì´ ë‹¤ë¦…ë‹ˆë‹¤. ê±´ë„ˆëœ€")
    
    return pairs


def evaluate_single_benchmark_preference(
    reward_model_path: str,
    model_name: str,
    benchmark_file: str,
    benchmark_id: int,
    benchmark_version: str = "v1.0.0",
    chosen_template: str = "passage_agent.create_passage",
    rejected_template: str = "passage_agent.create_passage_with_korean_errors",
    device: str = "auto",
    max_length: int = 512,
    seed: int = 42
) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ passage preference í‰ê°€
    
    Args:
        reward_model_path (str): Reward Model ê²½ë¡œ
        model_name (str): í‰ê°€í•  ëª¨ë¸ ì´ë¦„
        benchmark_file (str): ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ëª…
        benchmark_id (int): í‰ê°€í•  ë²¤ì¹˜ë§ˆí¬ ID
        benchmark_version (str): ë²¤ì¹˜ë§ˆí¬ ë²„ì „
        chosen_template (str): ì„ í˜¸ë˜ëŠ” í…œí”Œë¦¿ í‚¤
        rejected_template (str): ê±°ë¶€ë˜ëŠ” í…œí”Œë¦¿ í‚¤
        device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
        seed (int): ëœë¤ ì‹œë“œ
        
    Returns:
        Dict[str, Any]: í‰ê°€ ê²°ê³¼
    """
    print(f"ğŸ¯ ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ Preference í‰ê°€ ì‹œì‘...")
    print(f"   ë²¤ì¹˜ë§ˆí¬ ID: {benchmark_id}")
    
    # ì „ì²´ í‰ê°€ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë˜ ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ë§Œ í‰ê°€
    result = evaluate_passage_preferences(
        reward_model_path=reward_model_path,
        model_name=model_name,
        benchmark_file=benchmark_file,
        benchmark_version=benchmark_version,
        BENCH_ID_LIST=[benchmark_id],
        chosen_template=chosen_template,
        rejected_template=rejected_template,
        device=device,
        max_length=max_length,
        seed=seed
    )
    
    # ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë§Œ ë°˜í™˜
    if result["benchmark_results"]:
        return result["benchmark_results"][0]
    else:
        return {"error": f"ë²¤ì¹˜ë§ˆí¬ ID {benchmark_id} í‰ê°€ ì‹¤íŒ¨"}


def save_preference_evaluation_results(
    evaluation_results: Dict[str, Any],
    reward_model_name: str,
    benchmark_version: str = "v1.0.0"
) -> Path:
    """
    Preference í‰ê°€ ê²°ê³¼ë¥¼ ì €ì¥
    
    Args:
        evaluation_results (Dict[str, Any]): í‰ê°€ ê²°ê³¼
        reward_model_name (str): Reward Model ì´ë¦„ (íŒŒì¼ëª…ìš©)
        benchmark_version (str): ë²¤ì¹˜ë§ˆí¬ ë²„ì „
        
    Returns:
        Path: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    model_name = evaluation_results.get("model_name", "unknown")
    chosen_template = evaluation_results.get("chosen_template", "unknown")
    rejected_template = evaluation_results.get("rejected_template", "unknown")
    
    # íŒŒì¼ëª…ì— ì‚¬ìš©í•  í…œí”Œë¦¿ ì •ë³´ (ê°„ë‹¨í•˜ê²Œ)
    template_info = f"{chosen_template.split('.')[-1]}_vs_{rejected_template.split('.')[-1]}"
    
    saved_file = save_model_output(
        model_name=f"{model_name}_{reward_model_name}_preference",
        benchmark_id=0,  # ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ë¥¼ í¬í•¨í•˜ë¯€ë¡œ 0
        benchmark_version=benchmark_version,
        template_key=f"preference_eval_{template_info}",
        data=evaluation_results,
        base_dir=DEFAULT_EVALUATION_DIR
    )
    
    return saved_file


def analyze_preference_patterns(
    evaluation_results: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Preference í‰ê°€ ê²°ê³¼ì—ì„œ íŒ¨í„´ ë¶„ì„
    
    Args:
        evaluation_results (Dict[str, Any]): í‰ê°€ ê²°ê³¼
        
    Returns:
        Dict[str, Any]: ë¶„ì„ ê²°ê³¼
    """
    analysis = {
        "score_distribution": {
            "chosen_scores": [],
            "rejected_scores": [],
            "score_differences": []
        },
        "performance_by_benchmark": {},
        "error_patterns": []
    }
    
    for benchmark_result in evaluation_results.get("benchmark_results", []):
        benchmark_id = benchmark_result["benchmark_id"]
        accuracy = benchmark_result["accuracy"]
        
        analysis["performance_by_benchmark"][benchmark_id] = {
            "accuracy": accuracy,
            "total_pairs": benchmark_result["total_pairs"],
            "correct_preferences": benchmark_result["correct_preferences"]
        }
        
        # ì ìˆ˜ ë¶„í¬ ë¶„ì„
        for comparison in benchmark_result.get("comparisons", []):
            chosen_score = comparison.get("chosen_score", 0)
            rejected_score = comparison.get("rejected_score", 0)
            score_diff = comparison.get("score_difference", 0)
            
            analysis["score_distribution"]["chosen_scores"].append(chosen_score)
            analysis["score_distribution"]["rejected_scores"].append(rejected_score)
            analysis["score_distribution"]["score_differences"].append(score_diff)
            
            # ì˜¤ë¥˜ íŒ¨í„´ (chosenì´ rejectedë³´ë‹¤ ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ì€ ê²½ìš°)
            if not comparison.get("correct_preference", True):
                analysis["error_patterns"].append({
                    "benchmark_id": benchmark_id,
                    "chosen_score": chosen_score,
                    "rejected_score": rejected_score,
                    "score_difference": score_diff
                })
    
    # í†µê³„ ê³„ì‚°
    if analysis["score_distribution"]["chosen_scores"]:
        import numpy as np
        
        chosen_scores = analysis["score_distribution"]["chosen_scores"]
        rejected_scores = analysis["score_distribution"]["rejected_scores"]
        score_diffs = analysis["score_distribution"]["score_differences"]
        
        analysis["statistics"] = {
            "chosen_score_stats": {
                "mean": np.mean(chosen_scores),
                "std": np.std(chosen_scores),
                "min": np.min(chosen_scores),
                "max": np.max(chosen_scores)
            },
            "rejected_score_stats": {
                "mean": np.mean(rejected_scores),
                "std": np.std(rejected_scores),
                "min": np.min(rejected_scores),
                "max": np.max(rejected_scores)
            },
            "score_difference_stats": {
                "mean": np.mean(score_diffs),
                "std": np.std(score_diffs),
                "min": np.min(score_diffs),
                "max": np.max(score_diffs)
            }
        }
    
    return analysis


def load_preference_ranking_benchmark(
    benchmark_file: str = "v1/iSKA-Gen_Benchmark_v1.0.0_20250726_PreferenceRanking.json"
) -> List[Dict[str, Any]]:
    """
    Preference Ranking ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        benchmark_file (str): ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ëª…
        
    Returns:
        List[Dict[str, Any]]: ë¡œë“œëœ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°
    """
    try:
        # ê¸°ì¡´ benchmark_loader ìœ í‹¸ í•¨ìˆ˜ ì‚¬ìš©
        raw_data = load_benchmarks(benchmark_file)
        
        # ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° (ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš°) ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(raw_data, dict):
            raw_data = [raw_data]
        elif not isinstance(raw_data, list):
            raise ValueError(f"ì˜ˆìƒì¹˜ ëª»í•œ ë°ì´í„° í˜•ì‹: {type(raw_data)}")
        
        print(f"âœ… Preference Ranking ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ ì™„ë£Œ: {len(raw_data)}ê°œ í•­ëª©")
        return raw_data
        
    except Exception as e:
        print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì§ì ‘ íŒŒì¼ ë¡œë“œë¡œ í´ë°±
        benchmark_path = Path(__file__).resolve().parents[2] / "src" / "data" / "benchmarks" / benchmark_file
        if not benchmark_path.exists():
            raise FileNotFoundError(f"ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {benchmark_path}")
        
        with open(benchmark_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        if isinstance(raw_data, dict):
            raw_data = [raw_data]
        
        print(f"âœ… ì§ì ‘ ë¡œë“œë¡œ Preference Ranking ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ ì™„ë£Œ: {len(raw_data)}ê°œ í•­ëª©")
        return raw_data


def convert_to_pairwise_format(
    preference_data: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Preference Ranking ë°ì´í„°ë¥¼ pairwise ë¹„êµ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        preference_data (List[Dict[str, Any]]): ì›ë³¸ preference ë°ì´í„°
        
    Returns:
        List[Dict[str, Any]]: pairwise í˜•íƒœë¡œ ë³€í™˜ëœ ë°ì´í„°
    """
    # ê³ í’ˆì§ˆê³¼ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ ë¶„ë¦¬
    high_quality = [item for item in preference_data if item.get("quality") == "high"]
    low_quality = [item for item in preference_data if item.get("quality") == "low"]
    
    print(f"ğŸ“Š ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸: {len(high_quality)}ê°œ, ì €í’ˆì§ˆ í…ìŠ¤íŠ¸: {len(low_quality)}ê°œ")
    
    # ì£¼ì œë³„ë¡œ ë§¤ì¹­
    pairs = []
    matched_topics = set()
    
    for high_item in high_quality:
        # ì£¼ì œ ì¶”ì¶œ - "ê³ í’ˆì§ˆ_ìˆ«ì_" ë¶€ë¶„ ì œê±°
        high_name = high_item["name"]
        if high_name.startswith("ê³ í’ˆì§ˆ_"):
            # "ê³ í’ˆì§ˆ_01_íšŒì‹_ë¬¸í™”" -> "íšŒì‹_ë¬¸í™”"
            topic = "_".join(high_name.split("_")[2:])
            
            # ê°™ì€ ì£¼ì œì˜ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ ì°¾ê¸°
            matching_low = None
            for low_item in low_quality:
                low_name = low_item["name"]
                if low_name.startswith("ì €í’ˆì§ˆ_"):
                    low_topic = "_".join(low_name.split("_")[2:])
                    if low_topic == topic:
                        matching_low = low_item
                        break
        
        if matching_low:
            pair = {
                "pair_id": f"preference_pair_{topic}",
                "topic": topic,
                "chosen": high_item["text"],
                "rejected": matching_low["text"],
                "chosen_quality": "high",
                "rejected_quality": "low",
                "metadata": {
                    "chosen_name": high_item["name"],
                    "rejected_name": matching_low["name"]
                }
            }
            pairs.append(pair)
            matched_topics.add(topic)
        else:
            print(f"âš ï¸ ì£¼ì œ '{topic}'ì— ëŒ€í•œ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"âœ… {len(pairs)}ê°œì˜ pairwise ë¹„êµ ìŒ ìƒì„± ì™„ë£Œ")
    return pairs


def evaluate_gpt4_preference_alignment(
    pairs: List[Dict[str, Any]],
    openai_model: str = "gpt-4o",
    max_tokens: int = 50
) -> Dict[str, Any]:
    """
    GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ preference alignmentë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
    
    Args:
        pairs (List[Dict[str, Any]]): í‰ê°€í•  pair ë°ì´í„°
        openai_model (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸
        max_tokens (int): ìµœëŒ€ í† í° ìˆ˜
        
    Returns:
        Dict[str, Any]: GPT-4o í‰ê°€ ê²°ê³¼
    """
    print(f"ğŸ¯ GPT-4o Preference Alignment í‰ê°€ ì‹œì‘...")
    print(f"   ëª¨ë¸: {openai_model}")
    print(f"   í‰ê°€í•  ìŒ: {len(pairs)}ê°œ")
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    try:
        client = OpenAIModelClient()
    except Exception as e:
        print(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"error": "OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨"}
    
    # í‰ê°€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    evaluation_prompt = """ë‹¤ìŒ ë‘ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì¤‘ ì–´ëŠ ê²ƒì´ ë” ë†’ì€ í’ˆì§ˆì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸ A:
{text_a}

í…ìŠ¤íŠ¸ B:
{text_b}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
- ë¬¸ë²•ì˜ ì •í™•ì„±
- ì–´íœ˜ ì„ íƒì˜ ì ì ˆì„±
- ë¬¸ì²´ì˜ ìì—°ìŠ¤ëŸ¬ì›€
- ë‚´ìš©ì˜ ë…¼ë¦¬ì„±

ë” í’ˆì§ˆì´ ë†’ë‹¤ê³  íŒë‹¨ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ê³ , ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ "A" ë˜ëŠ” "B"ë¡œ ì‹œì‘í•˜ê³ , ê·¸ ë‹¤ìŒì— ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
    
    results = []
    correct_count = 0
    
    for i, pair in enumerate(pairs):
        print(f"ğŸ“Š Pair {i+1}/{len(pairs)} í‰ê°€ ì¤‘...")
        
        # ëœë¤í•˜ê²Œ chosen/rejected ìˆœì„œ ê²°ì • (bias ë°©ì§€)
        import random
        if random.choice([True, False]):
            text_a = pair["chosen"]
            text_b = pair["rejected"]
            correct_answer = "A"
        else:
            text_a = pair["rejected"]
            text_b = pair["chosen"]
            correct_answer = "B"
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = evaluation_prompt.format(text_a=text_a, text_b=text_b)
        
        try:
            # GPT-4o í˜¸ì¶œ
            response = client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model=openai_model,
                max_tokens=max_tokens,
                temperature=0.1  # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ ë‚®ì€ temperature
            )
            
            gpt_response = response.strip()
            gpt_choice = gpt_response[0].upper() if gpt_response else "?"
            
            # ì •ë‹µ ì—¬ë¶€ í™•ì¸
            is_correct = gpt_choice == correct_answer
            if is_correct:
                correct_count += 1
            
            result = {
                "pair_id": pair["pair_id"],
                "topic": pair["topic"],
                "text_a": text_a,
                "text_b": text_b,
                "correct_answer": correct_answer,
                "gpt_response": gpt_response,
                "gpt_choice": gpt_choice,
                "is_correct": is_correct
            }
            results.append(result)
            
        except Exception as e:
            print(f"âŒ GPT-4o í‰ê°€ ì‹¤íŒ¨ (Pair {i+1}): {e}")
            result = {
                "pair_id": pair["pair_id"],
                "topic": pair["topic"],
                "error": str(e)
            }
            results.append(result)
    
    # ì „ì²´ ê²°ê³¼ ê³„ì‚°
    total_evaluated = len([r for r in results if "error" not in r])
    accuracy = correct_count / total_evaluated if total_evaluated > 0 else 0
    
    evaluation_result = {
        "model": openai_model,
        "total_pairs": len(pairs),
        "total_evaluated": total_evaluated,
        "correct_preferences": correct_count,
        "accuracy": accuracy,
        "evaluation_timestamp": datetime.now().isoformat(),
        "results": results
    }
    
    print(f"âœ… GPT-4o í‰ê°€ ì™„ë£Œ!")
    print(f"   í‰ê°€ëœ ìŒ: {total_evaluated}/{len(pairs)}")
    print(f"   ì •í™•ë„: {accuracy:.2%}")
    
    return evaluation_result


def compare_reward_model_vs_gpt4(
    reward_model_path: str,
    pairs: List[Dict[str, Any]],
    openai_model: str = "gpt-4o",
    device: str = "auto",
    max_length: int = 512,
    seed: int = 42
) -> Dict[str, Any]:
    """
    Reward Modelê³¼ GPT-4oì˜ preference alignmentë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    
    Args:
        reward_model_path (str): Reward Model ê²½ë¡œ
        pairs (List[Dict[str, Any]]): í‰ê°€í•  pair ë°ì´í„°
        openai_model (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸
        device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
        seed (int): ëœë¤ ì‹œë“œ
        
    Returns:
        Dict[str, Any]: ë¹„êµ ê²°ê³¼
    """
    print(f"ğŸ”„ Reward Model vs GPT-4o ë¹„êµ ì‹œì‘...")
    
    # 1. Reward Model í‰ê°€
    print("\n1ï¸âƒ£ Reward Model í‰ê°€:")
    rm_comparison_pairs = [{"chosen": p["chosen"], "rejected": p["rejected"]} for p in pairs]
    rm_result = compare_passage_pairs_with_reward_model(
        model_path=reward_model_path,
        passage_pairs=rm_comparison_pairs,
        device=device,
        max_length=max_length,
        seed=seed
    )
    
    # 2. GPT-4o í‰ê°€
    print("\n2ï¸âƒ£ GPT-4o í‰ê°€:")
    gpt_result = evaluate_gpt4_preference_alignment(pairs, openai_model)
    
    # 3. ê²°ê³¼ ë¹„êµ
    print("\n3ï¸âƒ£ ê²°ê³¼ ë¹„êµ:")
    
    # ê³µí†µìœ¼ë¡œ í‰ê°€ëœ pairë§Œ ë¹„êµ
    rm_comparisons = rm_result.get("comparisons", [])
    gpt_results = gpt_result.get("results", [])
    
    agreement_count = 0
    total_comparable = min(len(rm_comparisons), len(gpt_results))
    
    detailed_comparisons = []
    
    for i in range(total_comparable):
        rm_comp = rm_comparisons[i]
        gpt_res = gpt_results[i]
        
        if "error" in gpt_res:
            continue
        
        # Reward Modelì˜ ì„ í˜¸ë„
        rm_prefers_chosen = rm_comp.get("correct_preference", False)
        
        # GPT-4oì˜ ì„ í˜¸ë„ (ì •ë‹µê³¼ ì¼ì¹˜ ì—¬ë¶€)
        gpt_prefers_chosen = gpt_res.get("is_correct", False)
        
        # ë‘ ëª¨ë¸ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        is_agreement = rm_prefers_chosen == gpt_prefers_chosen
        if is_agreement:
            agreement_count += 1
        
        detailed_comparison = {
            "pair_index": i,
            "topic": pairs[i]["topic"] if i < len(pairs) else f"pair_{i}",
            "rm_chosen_score": rm_comp.get("chosen_score", 0),
            "rm_rejected_score": rm_comp.get("rejected_score", 0),
            "rm_prefers_chosen": rm_prefers_chosen,
            "gpt_response": gpt_res.get("gpt_response", ""),
            "gpt_prefers_chosen": gpt_prefers_chosen,
            "agreement": is_agreement
        }
        detailed_comparisons.append(detailed_comparison)
    
    # ì „ì²´ ë¹„êµ ê²°ê³¼
    agreement_rate = agreement_count / total_comparable if total_comparable > 0 else 0
    
    comparison_result = {
        "comparison_timestamp": datetime.now().isoformat(),
        "reward_model_path": reward_model_path,
        "gpt_model": openai_model,
        "total_pairs": len(pairs),
        "comparable_pairs": total_comparable,
        "agreement_count": agreement_count,
        "agreement_rate": agreement_rate,
        "reward_model_accuracy": rm_result.get("accuracy", 0),
        "gpt4_accuracy": gpt_result.get("accuracy", 0),
        "detailed_comparisons": detailed_comparisons,
        "summary": {
            "rm_correct": rm_result.get("correct_preferences", 0),
            "rm_total": rm_result.get("total_pairs", 0),
            "gpt_correct": gpt_result.get("correct_preferences", 0),
            "gpt_total": gpt_result.get("total_evaluated", 0)
        }
    }
    
    print(f"âœ… ë¹„êµ ì™„ë£Œ!")
    print(f"   ë¹„êµ ê°€ëŠ¥í•œ ìŒ: {total_comparable}ê°œ")
    print(f"   ì¼ì¹˜ë„: {agreement_rate:.2%}")
    print(f"   Reward Model ì •í™•ë„: {rm_result.get('accuracy', 0):.2%}")
    print(f"   GPT-4o ì •í™•ë„: {gpt_result.get('accuracy', 0):.2%}")
    
    return comparison_result


def run_preference_ranking_evaluation(
    reward_model_path: str,
    benchmark_file: str = "v1/iSKA-Gen_Benchmark_v1.0.0_20250726_PreferenceRanking.json",
    openai_model: str = "gpt-4o",
    device: str = "auto",
    max_length: int = 512,
    seed: int = 42
) -> Dict[str, Any]:
    """
    Preference Ranking ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‚¬ìš©í•œ ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸
    
    Args:
        reward_model_path (str): Reward Model ê²½ë¡œ
        benchmark_file (str): Preference Ranking ë²¤ì¹˜ë§ˆí¬ íŒŒì¼
        openai_model (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸
        device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
        seed (int): ëœë¤ ì‹œë“œ
        
    Returns:
        Dict[str, Any]: ì „ì²´ í‰ê°€ ê²°ê³¼
    """
    print("ğŸš€ Preference Ranking í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
    
    try:
        # 1. ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë“œ
        print("\n1ï¸âƒ£ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë“œ:")
        preference_data = load_preference_ranking_benchmark(benchmark_file)
        
        # 2. Pairwise í˜•íƒœë¡œ ë³€í™˜
        print("\n2ï¸âƒ£ Pairwise í˜•íƒœë¡œ ë³€í™˜:")
        pairs = convert_to_pairwise_format(preference_data)
        
        if not pairs:
            return {"error": "ë³€í™˜ëœ pairê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # 3. Reward Model vs GPT-4o ë¹„êµ
        print("\n3ï¸âƒ£ Reward Model vs GPT-4o ë¹„êµ:")
        comparison_result = compare_reward_model_vs_gpt4(
            reward_model_path=reward_model_path,
            pairs=pairs,
            openai_model=openai_model,
            device=device,
            max_length=max_length,
            seed=seed
        )
        
        # 4. ê²°ê³¼ ì €ì¥
        print("\n4ï¸âƒ£ ê²°ê³¼ ì €ì¥:")
        saved_file = save_model_output(
            model_name="PreferenceRanking_Comparison",
            benchmark_id=0,
            benchmark_version="v1.0.0",
            template_key="rm_vs_gpt4_alignment",
            data=comparison_result,
            base_dir=DEFAULT_EVALUATION_DIR
        )
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {saved_file}")
        
        # 5. ìµœì¢… ê²°ê³¼ ìš”ì•½
        final_result = {
            "evaluation_type": "preference_ranking_alignment",
            "benchmark_file": benchmark_file,
            "total_pairs": len(pairs),
            "comparison_result": comparison_result,
            "saved_file": str(saved_file)
        }
        
        print("\nğŸ‰ Preference Ranking í‰ê°€ ì™„ë£Œ!")
        print(f"   ì´ {len(pairs)}ê°œ ìŒ í‰ê°€")
        print(f"   Reward Model vs GPT-4o ì¼ì¹˜ë„: {comparison_result.get('agreement_rate', 0):.2%}")
        
        return final_result
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}


# --- ì‹¤í–‰ ì˜ˆì‹œ ---
if __name__ == "__main__":
    # ì„¤ì • ê°’ë“¤
    REWARD_MODEL_PATH = "./saves/Qwen3-4B-Instruct/lora/train_2025-07-26-12-04-23"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½
    PREFERENCE_BENCHMARK_FILE = "v1/iSKA-Gen_Benchmark_v1.0.0_20250726_PreferenceRanking.json"
    OPENAI_MODEL = "gpt-4o"
    
    try:
        print("ğŸš€ Preference Ranking vs GPT-4o Alignment í‰ê°€ ì‹œì‘...")
        
        # ìƒˆë¡œìš´ Preference Ranking í‰ê°€ ì‹¤í–‰
        print("\nğŸ“Š Preference Ranking ë²¤ì¹˜ë§ˆí¬ í‰ê°€:")
        preference_result = run_preference_ranking_evaluation(
            reward_model_path=REWARD_MODEL_PATH,
            benchmark_file=PREFERENCE_BENCHMARK_FILE,
            openai_model=OPENAI_MODEL,
            max_length=256  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì§§ê²Œ ì„¤ì •
        )
        
        if "error" not in preference_result:
            comparison_data = preference_result["comparison_result"]
            print(f"\nğŸ“ˆ ìµœì¢… ê²°ê³¼:")
            print(f"   ì´ í‰ê°€ ìŒ: {preference_result['total_pairs']}ê°œ")
            print(f"   Reward Model ì •í™•ë„: {comparison_data.get('reward_model_accuracy', 0):.2%}")
            print(f"   GPT-4o ì •í™•ë„: {comparison_data.get('gpt4_accuracy', 0):.2%}")
            print(f"   ë‘ ëª¨ë¸ ì¼ì¹˜ë„: {comparison_data.get('agreement_rate', 0):.2%}")
            
            # ìƒì„¸ ë¶„ì„
            if comparison_data.get('detailed_comparisons'):
                agreements = [c for c in comparison_data['detailed_comparisons'] if c['agreement']]
                disagreements = [c for c in comparison_data['detailed_comparisons'] if not c['agreement']]
                
                print(f"\nğŸ“‹ ìƒì„¸ ë¶„ì„:")
                print(f"   ì¼ì¹˜í•˜ëŠ” ê²½ìš°: {len(agreements)}ê°œ")
                print(f"   ë¶ˆì¼ì¹˜í•˜ëŠ” ê²½ìš°: {len(disagreements)}ê°œ")
                
                if disagreements:
                    print(f"\nâš ï¸ ë¶ˆì¼ì¹˜ ì‚¬ë¡€ (ì²˜ìŒ 3ê°œ):")
                    for i, disagreement in enumerate(disagreements[:3]):
                        print(f"   {i+1}. ì£¼ì œ: {disagreement.get('topic', 'Unknown')}")
                        print(f"      RM ì„ í˜¸: {'Chosen' if disagreement['rm_prefers_chosen'] else 'Rejected'}")
                        print(f"      GPT-4o ì„ í˜¸: {'Chosen' if disagreement['gpt_prefers_chosen'] else 'Rejected'}")
        
        # ê¸°ì¡´ ë°©ì‹ë„ í…ŒìŠ¤íŠ¸ (í˜¸í™˜ì„± í™•ì¸)
        print("\n\nğŸ”„ ê¸°ì¡´ ë°©ì‹ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸:")
        OLD_MODEL_NAME = "A.X-4.0-Light"
        OLD_BENCHMARK_FILE = "v1/iSKA-Gen_Benchmark_v1.0.0_20250725_Initial.json"
        BENCH_ID_LIST = [1]  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ í•˜ë‚˜ë§Œ
        
        try:
            evaluation_results = evaluate_passage_preferences(
                reward_model_path=REWARD_MODEL_PATH,
                model_name=OLD_MODEL_NAME,
                benchmark_file=OLD_BENCHMARK_FILE,
                benchmark_version="v1.0.0",
                BENCH_ID_LIST=BENCH_ID_LIST,
                max_length=256
            )
            
            print(f"âœ… ê¸°ì¡´ ë°©ì‹ í˜¸í™˜ì„± í™•ì¸ ì™„ë£Œ")
            if evaluation_results.get("overall_statistics"):
                stats = evaluation_results["overall_statistics"]
                print(f"   ê¸°ì¡´ ë°©ì‹ ì •í™•ë„: {stats.get('overall_accuracy', 0):.2%}")
            
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ë°©ì‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ì •ìƒì ì„): {e}")
        
        print("\nğŸ‰ ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
