"""
RM í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ì„¸ ê°€ì§€ ìœ í˜•ì˜ ì„ í˜¸ë„ ìŒ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤:
1. SPF (Supervised Preference Dataset by Filtering)
2. IMP (Inter-Model Performance Preference Dataset)  
3. ICP (Intra-Model Contrastive Preference Dataset)
"""

import sys
import json
import random
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / 'modules'))
sys.path.append(str(project_root / 'utils'))

from modules.model_client import OpenAIModelClient
from utils.prompt_loader import get_prompt


class RMDatasetGenerator:
    """RM í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„±ê¸°"""
    
    def __init__(self, openai_model: str = "gpt-4o"):
        """
        ì´ˆê¸°í™”
        
        Args:
            openai_model (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸ëª…
        """
        self.openai_model = openai_model
        self.client = OpenAIModelClient(model_name=openai_model)
        self.data_dir = Path(__file__).parent.parent / "data"
        self.rm_training_dir = self.data_dir / "rm_training"
        
        # ìµœì‹  6ê°œ ë£¨ë¸Œë¦­ ê¸°ì¤€ ì •ì˜ (RM_Experiment_v1.0.0.md)
        self.rubrics = [
            "completeness_for_guidelines",      # í‰ê°€ ì§€ì¹¨ ì™„ì „ì„±
            "core_theme_clarity",               # í•µì‹¬ ì£¼ì œ ëª…í™•ì„±
            "reference_groundedness",           # ì°¸ê³  ìë£Œ ê¸°ë°˜ì„±
            "logical_flow_and_structure",       # ë…¼ë¦¬ì  íë¦„ ë° êµ¬ì¡°
            "korean_quality",                   # í•œêµ­ì–´ í’ˆì§ˆ
            "l2_learner_suitability"            # L2 í•™ìŠµì ì í•©ì„±
        ]
        
        print(f"ğŸ¯ RM ë°ì´í„°ì…‹ ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   OpenAI ëª¨ë¸: {openai_model}")
        print(f"   ë°ì´í„° ë””ë ‰í† ë¦¬: {self.rm_training_dir}")
        print(f"   ë£¨ë¸Œë¦­ ìˆ˜: {len(self.rubrics)}")

    def load_base_passages(self, file_path: str) -> List[Dict[str, Any]]:
        """
        ê¸°ë³¸ ì§€ë¬¸ ë°ì´í„° ë¡œë“œ
        
        Args:
            file_path (str): ì§€ë¬¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            List[Dict[str, Any]]: ë¡œë“œëœ ì§€ë¬¸ ë°ì´í„°
        """
        full_path = self.data_dir / "base_passages" / file_path
        
        if not full_path.exists():
            raise FileNotFoundError(f"ê¸°ë³¸ ì§€ë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")
        
        with open(full_path, 'r', encoding='utf-8') as f:
            passages = json.load(f)
        
        print(f"âœ… ê¸°ë³¸ ì§€ë¬¸ ë¡œë“œ ì™„ë£Œ: {len(passages)}ê°œ")
        return passages

    def evaluate_passage_with_gpt4o(self, passage: str, rubric: str) -> Dict[str, Any]:
        """
        GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë£¨ë¸Œë¦­ ê¸°ì¤€ìœ¼ë¡œ ì§€ë¬¸ í‰ê°€
        
        Args:
            passage (str): í‰ê°€í•  ì§€ë¬¸
            rubric (str): í‰ê°€ ê¸°ì¤€ (ë£¨ë¸Œë¦­)
            
        Returns:
            Dict[str, Any]: í‰ê°€ ê²°ê³¼ (pass/fail, score, reason)
        """
        try:
            # ë£¨ë¸Œë¦­ë³„ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = get_prompt(
                f"rubric_evaluation.{rubric}", 
                agent="iska",
                passage=passage
            )
            
            response = self.client.call([{"role": "user", "content": prompt}])
            
            # ì‘ë‹µ íŒŒì‹± (ì˜ˆ: "PASS" ë˜ëŠ” "FAIL"ë¡œ ì‹œì‘í•˜ëŠ” ì‘ë‹µ)
            if response.strip().upper().startswith("PASS"):
                result = {
                    "rubric": rubric,
                    "result": "pass",
                    "score": 1.0,
                    "reason": response.strip()
                }
            elif response.strip().upper().startswith("FAIL"):
                result = {
                    "rubric": rubric,
                    "result": "fail", 
                    "score": 0.0,
                    "reason": response.strip()
                }
            else:
                # ì• ë§¤í•œ ê²½ìš° ê¸°ë³¸ê°’
                result = {
                    "rubric": rubric,
                    "result": "uncertain",
                    "score": 0.5,
                    "reason": response.strip()
                }
            
            return result
            
        except Exception as e:
            print(f"âš ï¸ GPT-4o í‰ê°€ ì‹¤íŒ¨ ({rubric}): {e}")
            return {
                "rubric": rubric,
                "result": "error",
                "score": 0.0,
                "reason": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}"
            }

    def generate_spf_dataset(
        self, 
        passages: List[Dict[str, Any]], 
        target_pairs_per_rubric: int = 1000
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        SPF (Supervised Preference Dataset by Filtering) ìƒì„±
        
        Args:
            passages (List[Dict[str, Any]]): ê¸°ë³¸ ì§€ë¬¸ ë°ì´í„°
            target_pairs_per_rubric (int): ë£¨ë¸Œë¦­ë‹¹ ëª©í‘œ ìŒ ê°œìˆ˜
            
        Returns:
            Dict[str, List[Dict[str, Any]]]: ë£¨ë¸Œë¦­ë³„ ì„ í˜¸ë„ ìŒ ë°ì´í„°
        """
        print(f"ğŸ”„ SPF ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
        print(f"   ì…ë ¥ ì§€ë¬¸ ìˆ˜: {len(passages)}")
        print(f"   ë£¨ë¸Œë¦­ë‹¹ ëª©í‘œ ìŒ ìˆ˜: {target_pairs_per_rubric}")
        
        spf_dataset = {}
        
        for rubric in self.rubrics:
            print(f"\nğŸ“Š {rubric} ë£¨ë¸Œë¦­ ì²˜ë¦¬ ì¤‘...")
            
            positive_passages = []
            negative_passages = []
            
            # ê° ì§€ë¬¸ì„ í•´ë‹¹ ë£¨ë¸Œë¦­ìœ¼ë¡œ í‰ê°€
            for i, passage_data in enumerate(passages):
                if i % 100 == 0:
                    print(f"   ì§„í–‰ë¥ : {i}/{len(passages)}")
                
                passage_text = passage_data.get("text", passage_data.get("generated_passage", ""))
                
                evaluation = self.evaluate_passage_with_gpt4o(passage_text, rubric)
                
                passage_with_eval = passage_data.copy()
                passage_with_eval["evaluation"] = evaluation
                
                if evaluation["result"] == "pass":
                    positive_passages.append(passage_with_eval)
                elif evaluation["result"] == "fail":
                    negative_passages.append(passage_with_eval)
            
            print(f"   âœ… {rubric}: Positive {len(positive_passages)}ê°œ, Negative {len(negative_passages)}ê°œ")
            
            # ì„ í˜¸ë„ ìŒ ìƒì„±
            pairs = []
            min_count = min(len(positive_passages), len(negative_passages), target_pairs_per_rubric)
            
            # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ìŒ ìƒì„±
            positive_sample = random.sample(positive_passages, min_count)
            negative_sample = random.sample(negative_passages, min_count)
            
            for pos, neg in zip(positive_sample, negative_sample):
                pair = {
                    "pair_id": f"spf_{rubric}_{len(pairs)+1:04d}",
                    "rubric": rubric,
                    "chosen": pos.get("text", pos.get("generated_passage", "")),
                    "rejected": neg.get("text", neg.get("generated_passage", "")),
                    "chosen_metadata": pos,
                    "rejected_metadata": neg,
                    "dataset_type": "SPF",
                    "created_at": datetime.now().isoformat()
                }
                pairs.append(pair)
            
            spf_dataset[rubric] = pairs
            print(f"   ğŸ“ {rubric}: {len(pairs)}ê°œ ìŒ ìƒì„± ì™„ë£Œ")
        
        return spf_dataset

    def generate_imp_dataset(
        self,
        high_performance_passages: List[Dict[str, Any]],
        low_performance_passages: List[Dict[str, Any]],
        target_pairs_per_rubric: int = 1000
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        IMP (Inter-Model Performance Preference Dataset) ìƒì„±
        
        Args:
            high_performance_passages: ê³ ì„±ëŠ¥ ëª¨ë¸ ìƒì„± ì§€ë¬¸ (98% ë‹¬ì„±ë¥ )
            low_performance_passages: ì €ì„±ëŠ¥ ëª¨ë¸ ìƒì„± ì§€ë¬¸ (40% ë‹¬ì„±ë¥ )
            target_pairs_per_rubric: ë£¨ë¸Œë¦­ë‹¹ ëª©í‘œ ìŒ ê°œìˆ˜
            
        Returns:
            Dict[str, List[Dict[str, Any]]]: ë£¨ë¸Œë¦­ë³„ ì„ í˜¸ë„ ìŒ ë°ì´í„°
        """
        print(f"ğŸ”„ IMP ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
        print(f"   ê³ ì„±ëŠ¥ ì§€ë¬¸ ìˆ˜: {len(high_performance_passages)}")
        print(f"   ì €ì„±ëŠ¥ ì§€ë¬¸ ìˆ˜: {len(low_performance_passages)}")
        
        imp_dataset = {}
        
        for rubric in self.rubrics:
            print(f"\nğŸ“Š {rubric} ë£¨ë¸Œë¦­ ì²˜ë¦¬ ì¤‘...")
            
            pairs = []
            min_count = min(
                len(high_performance_passages), 
                len(low_performance_passages), 
                target_pairs_per_rubric
            )
            
            # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ìŒ ìƒì„±
            high_sample = random.sample(high_performance_passages, min_count)
            low_sample = random.sample(low_performance_passages, min_count)
            
            for high, low in zip(high_sample, low_sample):
                pair = {
                    "pair_id": f"imp_{rubric}_{len(pairs)+1:04d}",
                    "rubric": rubric,
                    "chosen": high.get("text", high.get("generated_passage", "")),
                    "rejected": low.get("text", low.get("generated_passage", "")),
                    "chosen_metadata": high,
                    "rejected_metadata": low,
                    "dataset_type": "IMP",
                    "created_at": datetime.now().isoformat()
                }
                pairs.append(pair)
            
            imp_dataset[rubric] = pairs
            print(f"   ğŸ“ {rubric}: {len(pairs)}ê°œ ìŒ ìƒì„± ì™„ë£Œ")
        
        return imp_dataset

    def generate_contrasted_passage(
        self, 
        base_passage: str, 
        rubric: str, 
        violation_type: str = "negative"
    ) -> str:
        """
        ê¸°ë³¸ ì§€ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • ë£¨ë¸Œë¦­ì„ ìœ„ë°˜í•˜ëŠ” ëŒ€ì¡° ì§€ë¬¸ ìƒì„±
        
        Args:
            base_passage (str): ê¸°ë³¸ ì§€ë¬¸
            rubric (str): ìœ„ë°˜í•  ë£¨ë¸Œë¦­
            violation_type (str): ìœ„ë°˜ ìœ í˜•
            
        Returns:
            str: ëŒ€ì¡° ì§€ë¬¸
        """
        try:
            prompt = get_prompt(
                f"contrastive_generation.{rubric}_{violation_type}",
                agent="iska", 
                base_passage=base_passage
            )
            
            contrasted_passage = self.client.call([{"role": "user", "content": prompt}])
            return contrasted_passage.strip()
            
        except Exception as e:
            print(f"âš ï¸ ëŒ€ì¡° ì§€ë¬¸ ìƒì„± ì‹¤íŒ¨ ({rubric}): {e}")
            return base_passage  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜

    def generate_icp_dataset(
        self,
        base_passages: List[Dict[str, Any]],
        target_pairs_per_rubric: int = 1000
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        ICP (Intra-Model Contrastive Preference Dataset) ìƒì„±
        
        Args:
            base_passages: ê¸°ë³¸ ì§€ë¬¸ ë°ì´í„°
            target_pairs_per_rubric: ë£¨ë¸Œë¦­ë‹¹ ëª©í‘œ ìŒ ê°œìˆ˜
            
        Returns:
            Dict[str, List[Dict[str, Any]]]: ë£¨ë¸Œë¦­ë³„ ì„ í˜¸ë„ ìŒ ë°ì´í„°
        """
        print(f"ğŸ”„ ICP ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
        print(f"   ê¸°ë³¸ ì§€ë¬¸ ìˆ˜: {len(base_passages)}")
        
        icp_dataset = {}
        
        for rubric in self.rubrics:
            print(f"\nğŸ“Š {rubric} ë£¨ë¸Œë¦­ ì²˜ë¦¬ ì¤‘...")
            
            pairs = []
            sample_passages = random.sample(
                base_passages, 
                min(len(base_passages), target_pairs_per_rubric)
            )
            
            for i, passage_data in enumerate(sample_passages):
                if i % 100 == 0:
                    print(f"   ì§„í–‰ë¥ : {i}/{len(sample_passages)}")
                
                base_text = passage_data.get("text", passage_data.get("generated_passage", ""))
                
                # ë£¨ë¸Œë¦­ ìœ„ë°˜ ì§€ë¬¸ ìƒì„±
                violated_text = self.generate_contrasted_passage(base_text, rubric)
                
                pair = {
                    "pair_id": f"icp_{rubric}_{len(pairs)+1:04d}",
                    "rubric": rubric,
                    "chosen": base_text,  # ì›ë³¸ (í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±)
                    "rejected": violated_text,  # ìœ„ë°˜ ì§€ë¬¸
                    "chosen_metadata": passage_data,
                    "rejected_metadata": {
                        "violation_type": rubric,
                        "base_passage_id": passage_data.get("id", f"base_{i}")
                    },
                    "dataset_type": "ICP",
                    "created_at": datetime.now().isoformat()
                }
                pairs.append(pair)
            
            icp_dataset[rubric] = pairs
            print(f"   ğŸ“ {rubric}: {len(pairs)}ê°œ ìŒ ìƒì„± ì™„ë£Œ")
        
        return icp_dataset

    def save_dataset(
        self, 
        dataset: Dict[str, List[Dict[str, Any]]], 
        dataset_type: str
    ) -> Dict[str, str]:
        """
        ìƒì„±ëœ ë°ì´í„°ì…‹ì„ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            dataset: ì €ì¥í•  ë°ì´í„°ì…‹
            dataset_type: ë°ì´í„°ì…‹ ìœ í˜• (spf, imp, icp)
            
        Returns:
            Dict[str, str]: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        dataset_dir = self.rm_training_dir / dataset_type.lower()
        dataset_dir.mkdir(exist_ok=True)
        
        saved_files = {}
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for rubric, pairs in dataset.items():
            filename = f"{dataset_type.upper()}_{rubric}_{timestamp}.json"
            file_path = dataset_dir / filename
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(pairs, f, ensure_ascii=False, indent=2)
            
            saved_files[rubric] = str(file_path)
            print(f"ğŸ’¾ {rubric}: {filename} ì €ì¥ë¨ ({len(pairs)}ê°œ ìŒ)")
        
        # ì „ì²´ ë°ì´í„°ì…‹ë„ ì €ì¥
        full_filename = f"{dataset_type.upper()}_complete_{timestamp}.json"
        full_path = dataset_dir / full_filename
        
        with open(full_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        saved_files["complete"] = str(full_path)
        print(f"ğŸ’¾ ì „ì²´ ë°ì´í„°ì…‹: {full_filename} ì €ì¥ë¨")
        
        return saved_files

    def load_dataset(self, dataset_type: str, rubric: Optional[str] = None) -> Dict[str, Any]:
        """
        ì €ì¥ëœ ë°ì´í„°ì…‹ ë¡œë“œ
        
        Args:
            dataset_type: ë°ì´í„°ì…‹ ìœ í˜• (spf, imp, icp)
            rubric: íŠ¹ì • ë£¨ë¸Œë¦­ (Noneì´ë©´ ì „ì²´ ë¡œë“œ)
            
        Returns:
            Dict[str, Any]: ë¡œë“œëœ ë°ì´í„°ì…‹
        """
        dataset_dir = self.rm_training_dir / dataset_type.lower()
        
        if not dataset_dir.exists():
            raise FileNotFoundError(f"ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {dataset_dir}")
        
        if rubric:
            # íŠ¹ì • ë£¨ë¸Œë¦­ íŒŒì¼ ì°¾ê¸°
            pattern = f"{dataset_type.upper()}_{rubric}_*.json"
            files = list(dataset_dir.glob(pattern))
            if not files:
                raise FileNotFoundError(f"ë£¨ë¸Œë¦­ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern}")
            
            latest_file = max(files, key=lambda p: p.stat().st_mtime)
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # ì „ì²´ ë°ì´í„°ì…‹ íŒŒì¼ ì°¾ê¸°
            pattern = f"{dataset_type.upper()}_complete_*.json"
            files = list(dataset_dir.glob(pattern))
            if not files:
                raise FileNotFoundError(f"ì „ì²´ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern}")
            
            latest_file = max(files, key=lambda p: p.stat().st_mtime)
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)

    def get_dataset_stats(self, dataset: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """
        ë°ì´í„°ì…‹ í†µê³„ ì •ë³´ ë°˜í™˜
        
        Args:
            dataset: ë¶„ì„í•  ë°ì´í„°ì…‹
            
        Returns:
            Dict[str, Any]: í†µê³„ ì •ë³´
        """
        stats = {
            "rubrics": list(dataset.keys()),
            "total_rubrics": len(dataset),
            "pairs_per_rubric": {},
            "total_pairs": 0
        }
        
        for rubric, pairs in dataset.items():
            stats["pairs_per_rubric"][rubric] = len(pairs)
            stats["total_pairs"] += len(pairs)
        
        stats["average_pairs_per_rubric"] = stats["total_pairs"] / stats["total_rubrics"] if stats["total_rubrics"] > 0 else 0
        
        return stats


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_spf_dataset(
    passages_file: str,
    target_pairs_per_rubric: int = 1000,
    openai_model: str = "gpt-4o"
) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, str]]:
    """
    SPF ë°ì´í„°ì…‹ ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Returns:
        Tuple[dataset, saved_files]: ìƒì„±ëœ ë°ì´í„°ì…‹ê³¼ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
    """
    generator = RMDatasetGenerator(openai_model)
    passages = generator.load_base_passages(passages_file)
    dataset = generator.generate_spf_dataset(passages, target_pairs_per_rubric)
    saved_files = generator.save_dataset(dataset, "SPF")
    
    stats = generator.get_dataset_stats(dataset)
    print(f"\nğŸ“Š SPF ë°ì´í„°ì…‹ í†µê³„:")
    print(f"   ì´ ë£¨ë¸Œë¦­ ìˆ˜: {stats['total_rubrics']}")
    print(f"   ì´ ìŒ ìˆ˜: {stats['total_pairs']}")
    print(f"   ë£¨ë¸Œë¦­ë‹¹ í‰ê·  ìŒ ìˆ˜: {stats['average_pairs_per_rubric']:.1f}")
    
    return dataset, saved_files


def create_imp_dataset(
    high_perf_file: str,
    low_perf_file: str,
    target_pairs_per_rubric: int = 1000,
    openai_model: str = "gpt-4o"
) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, str]]:
    """
    IMP ë°ì´í„°ì…‹ ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Returns:
        Tuple[dataset, saved_files]: ìƒì„±ëœ ë°ì´í„°ì…‹ê³¼ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
    """
    generator = RMDatasetGenerator(openai_model)
    high_passages = generator.load_base_passages(high_perf_file)
    low_passages = generator.load_base_passages(low_perf_file)
    dataset = generator.generate_imp_dataset(high_passages, low_passages, target_pairs_per_rubric)
    saved_files = generator.save_dataset(dataset, "IMP")
    
    stats = generator.get_dataset_stats(dataset)
    print(f"\nğŸ“Š IMP ë°ì´í„°ì…‹ í†µê³„:")
    print(f"   ì´ ë£¨ë¸Œë¦­ ìˆ˜: {stats['total_rubrics']}")
    print(f"   ì´ ìŒ ìˆ˜: {stats['total_pairs']}")
    print(f"   ë£¨ë¸Œë¦­ë‹¹ í‰ê·  ìŒ ìˆ˜: {stats['average_pairs_per_rubric']:.1f}")
    
    return dataset, saved_files


def create_icp_dataset(
    base_passages_file: str,
    target_pairs_per_rubric: int = 1000,
    openai_model: str = "gpt-4o"
) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, str]]:
    """
    ICP ë°ì´í„°ì…‹ ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Returns:
        Tuple[dataset, saved_files]: ìƒì„±ëœ ë°ì´í„°ì…‹ê³¼ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
    """
    generator = RMDatasetGenerator(openai_model)
    passages = generator.load_base_passages(base_passages_file)
    dataset = generator.generate_icp_dataset(passages, target_pairs_per_rubric)
    saved_files = generator.save_dataset(dataset, "ICP")
    
    stats = generator.get_dataset_stats(dataset)
    print(f"\nğŸ“Š ICP ë°ì´í„°ì…‹ í†µê³„:")
    print(f"   ì´ ë£¨ë¸Œë¦­ ìˆ˜: {stats['total_rubrics']}")
    print(f"   ì´ ìŒ ìˆ˜: {stats['total_pairs']}")
    print(f"   ë£¨ë¸Œë¦­ë‹¹ í‰ê·  ìŒ ìˆ˜: {stats['average_pairs_per_rubric']:.1f}")
    
    return dataset, saved_files


if __name__ == "__main__":
    print("ğŸš€ RM ë°ì´í„°ì…‹ ìƒì„±ê¸° ëª¨ë“ˆ ë¡œë“œë¨")
    print("ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜:")
    print("   - create_spf_dataset(): SPF ë°ì´í„°ì…‹ ìƒì„±")
    print("   - create_imp_dataset(): IMP ë°ì´í„°ì…‹ ìƒì„±") 
    print("   - create_icp_dataset(): ICP ë°ì´í„°ì…‹ ìƒì„±")
    print("   - RMDatasetGenerator: ë©”ì¸ ìƒì„±ê¸° í´ë˜ìŠ¤")
