class RMDatasetGenerator:
    """
    ëª¨ë“  ë°ì´í„°ì…‹(pair) ì¶œë ¥ì€ ì•„ë˜ì™€ ê°™ì€ í†µì¼ëœ í¬ë§·ì„ ë”°ë¦…ë‹ˆë‹¤:

    {
      "pair_id": "imp_completeness_for_guidelines_1ì¸ ê°€êµ¬ ì¦ê°€ í˜„ìƒ ë¶„ì„ ë° ì‚¬íšŒì  ì‹œì‚¬ì _0001",
      "rubric": "completeness_for_guidelines",
      "source_item": {
        "korean_topic": "...",
        "korean_context": "...",
        "foreign_topic": "...",
        "foreign_context": "...",
        "problem_types": [...],
        "eval_goals": [...]
      },
      "chosen": "...",
      "rejected": "...",
      "dataset_type": "IMP",
      "created_at": "2025-07-29T18:43:12.159730"
    }
    """
"""
RM í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ì„¸ ê°€ì§€ ìœ í˜•ì˜ ì„ í˜¸ë„ ìŒ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤:
1. SPF (Supervised Preference Dataset by Filtering)
2. IMP (Inter-Model Performance Preference Dataset)  
3. ICP (Intra-Model Contrastive Preference Dataset)
"""

import sys
import json
import random
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / 'modules'))
sys.path.append(str(project_root / 'utils'))

from modules.model_client import OpenAIModelClient
from utils.prompt_loader import get_prompt


class RMDatasetGenerator:
    """RM í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„±ê¸°"""
    
    def __init__(self, openai_model: str = "gpt-4o"):
        """
        ì´ˆê¸°í™”
        
        Args:
            openai_model (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸ëª…
        """
        self.openai_model = openai_model
        self.client = OpenAIModelClient(model_name=openai_model)
        self.data_dir = Path(__file__).parent.parent / "data"
        self.rm_training_dir = self.data_dir / "rm_training"
        
        # ìµœì‹  6ê°œ ë£¨ë¸Œë¦­ ê¸°ì¤€ ì •ì˜ (RM_Experiment_v1.0.0.md)
        self.rubrics = [
            "completeness_for_guidelines",      # í‰ê°€ ì§€ì¹¨ ì™„ì „ì„±
            "clarity_of_core_theme",               # í•µì‹¬ ì£¼ì œ ëª…í™•ì„±
            "reference_groundedness",           # ì°¸ê³  ìë£Œ ê¸°ë°˜ì„±
            "logical_flow",       # ë…¼ë¦¬ì  íë¦„ ë° êµ¬ì¡°
            "korean_quality",                   # í•œêµ­ì–´ í’ˆì§ˆ
            "l2_learner_suitability"            # L2 í•™ìŠµì ì í•©ì„±
        ]
        
        print(f"ğŸ¯ RM ë°ì´í„°ì…‹ ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   OpenAI ëª¨ë¸: {openai_model}")
        print(f"   ë°ì´í„° ë””ë ‰í† ë¦¬: {self.rm_training_dir}")
        print(f"   ë£¨ë¸Œë¦­ ìˆ˜: {len(self.rubrics)}")

    def evaluate_passage_with_gpt4o(self, passage: str, rubric: str) -> Dict[str, Any]:
        """
        GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë£¨ë¸Œë¦­ ê¸°ì¤€ìœ¼ë¡œ ì§€ë¬¸ í‰ê°€
        
        Args:
            passage (str): í‰ê°€í•  ì§€ë¬¸
            rubric (str): í‰ê°€ ê¸°ì¤€ (ë£¨ë¸Œë¦­)
            
        Returns:
            Dict[str, Any]: í‰ê°€ ê²°ê³¼ (pass/fail, score, reason)
        """
        try:
            # ë£¨ë¸Œë¦­ë³„ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = get_prompt(
                f"rubric_evaluation.{rubric}", 
                agent="iska",
                passage=passage
            )
            
            response = self.client.call([{"role": "user", "content": prompt}])
            
            # ì‘ë‹µ íŒŒì‹± (ì˜ˆ: "PASS" ë˜ëŠ” "FAIL"ë¡œ ì‹œì‘í•˜ëŠ” ì‘ë‹µ)
            if response.strip().upper().startswith("PASS"):
                result = {
                    "rubric": rubric,
                    "result": "pass",
                    "score": 1.0,
                    "reason": response.strip()
                }
            elif response.strip().upper().startswith("FAIL"):
                result = {
                    "rubric": rubric,
                    "result": "fail", 
                    "score": 0.0,
                    "reason": response.strip()
                }
            else:
                # ì• ë§¤í•œ ê²½ìš° ê¸°ë³¸ê°’
                result = {
                    "rubric": rubric,
                    "result": "uncertain",
                    "score": 0.5,
                    "reason": response.strip()
                }
            
            return result
            
        except Exception as e:
            print(f"âš ï¸ GPT-4o í‰ê°€ ì‹¤íŒ¨ ({rubric}): {e}")
            return {
                "rubric": rubric,
                "result": "error",
                "score": 0.0,
                "reason": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}"
            }

    def generate_spf_dataset(
        self, 
        passages: List[Dict[str, Any]], 
        target_pairs_per_rubric: int = 1000
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        SPF (Supervised Preference Dataset by Filtering) ìƒì„±
        
        Args:
            passages (List[Dict[str, Any]]): ê¸°ë³¸ ì§€ë¬¸ ë°ì´í„°
            target_pairs_per_rubric (int): ë£¨ë¸Œë¦­ë‹¹ ëª©í‘œ ìŒ ê°œìˆ˜
            
        Returns:
            Dict[str, List[Dict[str, Any]]]: ë£¨ë¸Œë¦­ë³„ ì„ í˜¸ë„ ìŒ ë°ì´í„°
        """
        print(f"ğŸ”„ SPF ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
        print(f"   ì…ë ¥ ì§€ë¬¸ ìˆ˜: {len(passages)}")
        print(f"   ë£¨ë¸Œë¦­ë‹¹ ëª©í‘œ ìŒ ìˆ˜: {target_pairs_per_rubric}")
        
        spf_dataset = {}
        
        for rubric in self.rubrics:
            print(f"\nğŸ“Š {rubric} ë£¨ë¸Œë¦­ ì²˜ë¦¬ ì¤‘...")
            
            positive_passages = []
            negative_passages = []
            
            # ê° ì§€ë¬¸ì„ í•´ë‹¹ ë£¨ë¸Œë¦­ìœ¼ë¡œ í‰ê°€
            for i, passage_data in enumerate(passages):
                if i % 100 == 0:
                    print(f"   ì§„í–‰ë¥ : {i}/{len(passages)}")
                
                passage_text = passage_data.get("text", passage_data.get("generated_passage", ""))
                
                evaluation = self.evaluate_passage_with_gpt4o(passage_text, rubric)
                
                passage_with_eval = passage_data.copy()
                passage_with_eval["evaluation"] = evaluation
                
                if evaluation["result"] == "pass":
                    positive_passages.append(passage_with_eval)
                elif evaluation["result"] == "fail":
                    negative_passages.append(passage_with_eval)
            
            print(f"   âœ… {rubric}: Positive {len(positive_passages)}ê°œ, Negative {len(negative_passages)}ê°œ")
            
            # ì„ í˜¸ë„ ìŒ ìƒì„±
            pairs = []
            min_count = min(len(positive_passages), len(negative_passages), target_pairs_per_rubric)
            
            # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ìŒ ìƒì„±
            positive_sample = random.sample(positive_passages, min_count)
            negative_sample = random.sample(negative_passages, min_count)
            
            for pos, neg in zip(positive_sample, negative_sample):
                # source_item í†µì¼: posì˜ source_itemì— problem_types/eval_goalsê°€ ìˆìœ¼ë©´ í¬í•¨
                source_item = pos.get("source_item", {}).copy()
                if "problem_types" in pos:
                    source_item["problem_types"] = pos["problem_types"]
                if "eval_goals" in pos:
                    source_item["eval_goals"] = pos["eval_goals"]
                pair = {
                    "pair_id": f"spf_{rubric}_{source_item.get('korean_topic', 'unknown')}_{len(pairs)+1:04d}",
                    "rubric": rubric,
                    "source_item": source_item,
                    "chosen": pos.get("text", pos.get("generated_passage", "")),
                    "rejected": neg.get("text", neg.get("generated_passage", "")),
                    "dataset_type": "SPF",
                    "created_at": datetime.now().isoformat()
                }
                pairs.append(pair)
            
            spf_dataset[rubric] = pairs
            print(f"   ğŸ“ {rubric}: {len(pairs)}ê°œ ìŒ ìƒì„± ì™„ë£Œ")
        
        return spf_dataset

    def generate_imp_dataset(
        self,
        high_performance_passages: List[Dict[str, Any]],
        low_performance_passages: List[Dict[str, Any]],
        target_pairs_per_rubric: int = 1000
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        IMP (Inter-Model Performance Preference Dataset) ìƒì„±
        
        Args:
            high_performance_passages: ê³ ì„±ëŠ¥ ëª¨ë¸ ìƒì„± ì§€ë¬¸ (98% ë‹¬ì„±ë¥ )
            low_performance_passages: ì €ì„±ëŠ¥ ëª¨ë¸ ìƒì„± ì§€ë¬¸ (40% ë‹¬ì„±ë¥ )
            target_pairs_per_rubric: ë£¨ë¸Œë¦­ë‹¹ ëª©í‘œ ìŒ ê°œìˆ˜
        Returns:
            Dict[str, List[Dict[str, Any]]]: ë£¨ë¸Œë¦­ë³„ ì„ í˜¸ë„ ìŒ ë°ì´í„°
        """
        print(f"ğŸ”„ IMP ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
        print(f"   ê³ ì„±ëŠ¥ ì§€ë¬¸ ìˆ˜: {len(high_performance_passages)}")
        print(f"   ì €ì„±ëŠ¥ ì§€ë¬¸ ìˆ˜: {len(low_performance_passages)}")

        imp_dataset = {}

        for rubric in self.rubrics:
            print(f"\nğŸ“Š {rubric} ë£¨ë¸Œë¦­ ì²˜ë¦¬ ì¤‘...")

            def get_benchmark_key(p):
                if "source_item" in p:
                    si = p["source_item"]
                    return si.get("korean_topic") or si.get("topic") or si.get("item_id") or si.get("benchmark_id")
                return None

            high_groups = {}
            for p in high_performance_passages:
                key = get_benchmark_key(p)
                if key is not None:
                    high_groups.setdefault(key, []).append(p)
            low_groups = {}
            for p in low_performance_passages:
                key = get_benchmark_key(p)
                if key is not None:
                    low_groups.setdefault(key, []).append(p)

            common_keys = set(high_groups.keys()) & set(low_groups.keys())
            pairs = []

            for idx, key in enumerate(sorted(common_keys)):
                high_list = high_groups[key]
                low_list = low_groups[key]
                min_count = min(len(high_list), len(low_list), target_pairs_per_rubric)

                for i in range(min_count):
                    high = high_list[i]
                    low = low_list[i]
                    si_high = high.get("source_item", {}).copy()
                    si_low = low.get("source_item", {}).copy()
                    # source_item: ê³µí†µ í•„ë“œ + problem_types/eval_goals í¬í•¨
                    source_item = {k: si_high[k] for k in si_high if k in si_low and si_high[k] == si_low[k]}
                    for meta_key in ["problem_types", "eval_goals"]:
                        if meta_key in si_high:
                            source_item[meta_key] = si_high[meta_key]
                    pair = {
                        "pair_id": f"imp_{rubric}_{source_item.get('korean_topic', key)}_{i+1:04d}",
                        "rubric": rubric,
                        "source_item": source_item,
                        "chosen": high.get("text", high.get("generated_passage", "")),
                        "rejected": low.get("text", low.get("generated_passage", "")),
                        "dataset_type": "IMP",
                        "created_at": datetime.now().isoformat()
                    }
                    pairs.append(pair)

            imp_dataset[rubric] = pairs
            print(f"   ğŸ“ {rubric}: {len(pairs)}ê°œ ìŒ ìƒì„± ì™„ë£Œ")

        return imp_dataset

    def generate_icp_dataset(
        self,
        high_performance_passages: List[Dict[str, Any]],
        low_performance_passages: List[Dict[str, Any]],
        rubric: str,
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        ICP (Intra-Model Contrastive Preference Dataset) ìƒì„±
        
        Args:
            high_performance_passages: ê³ ì„±ëŠ¥ ëª¨ë¸ì˜ ì§€ë¬¸ ë°ì´í„°
            low_performance_passages: ì €ì„±ëŠ¥ ëª¨ë¸ì˜ ì§€ë¬¸ ë°ì´í„°
            rubric: ì²˜ë¦¬í•  ë£¨ë¸Œë¦­ ì´ë¦„
        Returns:
            Dict[str, List[Dict[str, Any]]]: ë£¨ë¸Œë¦­ë³„ ì„ í˜¸ë„ ìŒ ë°ì´í„°
        """
        if rubric not in self.rubrics:
            raise ValueError(f"Invalid rubric: {rubric}. Must be one of {self.rubrics}")

        print(f"ğŸ”„ ICP ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘... (Rubric: {rubric})")
        print(f"   ê³ ì„±ëŠ¥ ì§€ë¬¸ ìˆ˜: {len(high_performance_passages)}")
        print(f"   ì €ì„±ëŠ¥ ì§€ë¬¸ ìˆ˜: {len(low_performance_passages)}")

        icp_dataset = {}

        def get_benchmark_key(p):
            if "source_item" in p:
                si = p["source_item"]
                return si.get("korean_topic") or si.get("topic") or si.get("item_id") or si.get("benchmark_id")
            return None

        high_groups = {}
        for p in high_performance_passages:
            key = get_benchmark_key(p)
            if key is not None:
                high_groups.setdefault(key, []).append(p)

        low_groups = {}
        for p in low_performance_passages:
            key = get_benchmark_key(p)
            if key is not None:
                low_groups.setdefault(key, []).append(p)

        common_keys = set(high_groups.keys()) & set(low_groups.keys())
        pairs = []

        for idx, key in enumerate(sorted(common_keys)):
            high_list = high_groups[key]
            low_list = low_groups[key]
            min_count = min(len(high_list), len(low_list))

            for i in range(min_count):
                high = high_list[i]
                low = low_list[i]
                high_text = high.get("text", high.get("generated_passage", ""))
                low_text = low.get("text", low.get("generated_passage", ""))

                # ë‘ ì§€ë¬¸ì˜ source_itemì—ì„œ ê³µí†µ í•„ë“œë§Œ ì¶”ì¶œ
                si_high = high.get("source_item", {}).copy()
                si_low = low.get("source_item", {}).copy()
                source_item = {k: si_high[k] for k in si_high if k in si_low and si_high[k] == si_low[k]}
                
                # problem_types/eval_goals í¬í•¨
                for meta_key in ["problem_types", "eval_goals"]:
                    if meta_key in high:
                        source_item[meta_key] = high[meta_key]

                pair = {
                    "pair_id": f"icp_{rubric}_{source_item.get('korean_topic', key)}_{i+1:04d}",
                    "rubric": rubric,
                    "source_item": source_item,
                    "chosen": high_text,
                    "rejected": low_text,
                    "dataset_type": "ICP",
                    "created_at": datetime.now().isoformat()
                }
                pairs.append(pair)

        icp_dataset[rubric] = pairs
        print(f"   ğŸ“ {rubric}: {len(pairs)}ê°œ ìŒ ìƒì„± ì™„ë£Œ")

        if len(pairs) == 0:
            print(f"âš ï¸ ê²½ê³ : {rubric} ë£¨ë¸Œë¦­ì— ëŒ€í•´ ìƒì„±ëœ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")

        return icp_dataset

    def save_dataset(
        self, 
        dataset: Dict[str, List[Dict[str, Any]]], 
        dataset_type: str
    ) -> Dict[str, str]:
        """
        ìƒì„±ëœ ë°ì´í„°ì…‹ì„ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            dataset: ì €ì¥í•  ë°ì´í„°ì…‹
            dataset_type: ë°ì´í„°ì…‹ ìœ í˜• (spf, imp, icp)
            
        Returns:
            Dict[str, str]: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        dataset_dir = self.rm_training_dir / dataset_type.lower()
        dataset_dir.mkdir(exist_ok=True)
        
        saved_files = {}
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for rubric, pairs in dataset.items():
            filename = f"{dataset_type.upper()}_{rubric}_{timestamp}.json"
            file_path = dataset_dir / filename
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(pairs, f, ensure_ascii=False, indent=2)
            
            saved_files[rubric] = str(file_path)
            print(f"ğŸ’¾ {rubric}: {filename} ì €ì¥ë¨ ({len(pairs)}ê°œ ìŒ)")
        
        # ì „ì²´ ë°ì´í„°ì…‹ë„ ì €ì¥
        full_filename = f"{dataset_type.upper()}_complete_{timestamp}.json"
        full_path = dataset_dir / full_filename
        
        with open(full_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        saved_files["complete"] = str(full_path)
        print(f"ğŸ’¾ ì „ì²´ ë°ì´í„°ì…‹: {full_filename} ì €ì¥ë¨")
        
        return saved_files

    def load_dataset(self, dataset_type: str, rubric: Optional[str] = None) -> Dict[str, Any]:
        """
        ì €ì¥ëœ ë°ì´í„°ì…‹ ë¡œë“œ
        
        Args:
            dataset_type: ë°ì´í„°ì…‹ ìœ í˜• (spf, imp, icp)
            rubric: íŠ¹ì • ë£¨ë¸Œë¦­ (Noneì´ë©´ ì „ì²´ ë¡œë“œ)
            
        Returns:
            Dict[str, Any]: ë¡œë“œëœ ë°ì´í„°ì…‹
        """
        dataset_dir = self.rm_training_dir / dataset_type.lower()
        
        if not dataset_dir.exists():
            raise FileNotFoundError(f"ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {dataset_dir}")
        
        if rubric:
            # íŠ¹ì • ë£¨ë¸Œë¦­ íŒŒì¼ ì°¾ê¸°
            pattern = f"{dataset_type.upper()}_{rubric}_*.json"
            files = list(dataset_dir.glob(pattern))
            if not files:
                raise FileNotFoundError(f"ë£¨ë¸Œë¦­ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern}")
            
            latest_file = max(files, key=lambda p: p.stat().st_mtime)
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # ì „ì²´ ë°ì´í„°ì…‹ íŒŒì¼ ì°¾ê¸°
            pattern = f"{dataset_type.upper()}_complete_*.json"
            files = list(dataset_dir.glob(pattern))
            if not files:
                raise FileNotFoundError(f"ì „ì²´ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern}")
            
            latest_file = max(files, key=lambda p: p.stat().st_mtime)
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)

    def get_dataset_stats(self, dataset: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """
        ë°ì´í„°ì…‹ í†µê³„ ì •ë³´ ë°˜í™˜
        
        Args:
            dataset: ë¶„ì„í•  ë°ì´í„°ì…‹
            
        Returns:
            Dict[str, Any]: í†µê³„ ì •ë³´
        """
        stats = {
            "rubrics": list(dataset.keys()),
            "total_rubrics": len(dataset),
            "pairs_per_rubric": {},
            "total_pairs": 0
        }
        
        for rubric, pairs in dataset.items():
            stats["pairs_per_rubric"][rubric] = len(pairs)
            stats["total_pairs"] += len(pairs)
        
        stats["average_pairs_per_rubric"] = stats["total_pairs"] / stats["total_rubrics"] if stats["total_rubrics"] > 0 else 0
        
        return stats


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_spf_dataset(
    passages: Any,
    target_pairs_per_rubric: int = 1000,
    openai_model: str = "gpt-4o"
) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, str]]:
    """
    SPF ë°ì´í„°ì…‹ ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Returns:
        Tuple[dataset, saved_files]: ìƒì„±ëœ ë°ì´í„°ì…‹ê³¼ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
    """
    generator = RMDatasetGenerator(openai_model)
    # íŒŒì¼ëª…(str)ì´ë©´ ë¦¬ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    dataset = generator.generate_spf_dataset(passages, target_pairs_per_rubric)
    saved_files = generator.save_dataset(dataset, "SPF")
    
    stats = generator.get_dataset_stats(dataset)
    print(f"\nğŸ“Š SPF ë°ì´í„°ì…‹ í†µê³„:")
    print(f"   ì´ ë£¨ë¸Œë¦­ ìˆ˜: {stats['total_rubrics']}")
    print(f"   ì´ ìŒ ìˆ˜: {stats['total_pairs']}")
    print(f"   ë£¨ë¸Œë¦­ë‹¹ í‰ê·  ìŒ ìˆ˜: {stats['average_pairs_per_rubric']:.1f}")
    
    return dataset, saved_files


def create_imp_dataset(
    high_perf: Any,
    low_perf: Any,
    openai_model: str = "gpt-4o"
) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, str]]:
    """
    IMP ë°ì´í„°ì…‹ ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Returns:
        Tuple[dataset, saved_files]: ìƒì„±ëœ ë°ì´í„°ì…‹ê³¼ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
    """
    generator = RMDatasetGenerator(openai_model)
    # íŒŒì¼ëª…(str)ì´ë©´ ë¡œë“œ, ë¦¬ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    high_passages = high_perf
    low_passages = low_perf
    dataset = generator.generate_imp_dataset(high_passages, low_passages)
    saved_files = generator.save_dataset(dataset, "IMP")
    
    stats = generator.get_dataset_stats(dataset)
    print(f"\nğŸ“Š IMP ë°ì´í„°ì…‹ í†µê³„:")
    print(f"   ì´ ë£¨ë¸Œë¦­ ìˆ˜: {stats['total_rubrics']}")
    print(f"   ì´ ìŒ ìˆ˜: {stats['total_pairs']}")
    
    return dataset, saved_files


def create_icp_dataset(
    high_perf: Any,
    low_perf: Any,
    rubric: str,
    openai_model: str = "gpt-4o"
) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, str]]:
    """
    ICP ë°ì´í„°ì…‹ ìƒì„± í¸ì˜ í•¨ìˆ˜

    Args:
        high_perf: ê³ ì„±ëŠ¥ ëª¨ë¸ì˜ ì§€ë¬¸ ë°ì´í„°
        low_perf: ì €ì„±ëŠ¥ ëª¨ë¸ì˜ ì§€ë¬¸ ë°ì´í„°
        rubric: ì²˜ë¦¬í•  ë£¨ë¸Œë¦­ ì´ë¦„
        target_pairs_per_rubric: ë£¨ë¸Œë¦­ë‹¹ ëª©í‘œ ìŒ ê°œìˆ˜
        openai_model: ì‚¬ìš©í•  OpenAI ëª¨ë¸ëª…
    
    Returns:
        Tuple[dataset, saved_files]: ìƒì„±ëœ ë°ì´í„°ì…‹ê³¼ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
    """
    generator = RMDatasetGenerator(openai_model)
    # íŒŒì¼ëª…(str)ì´ë©´ ë¡œë“œ, ë¦¬ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    high_passages = high_perf
    low_passages = low_perf

    dataset = generator.generate_icp_dataset(
        high_passages, 
        low_passages, 
        rubric,
    )
    saved_files = generator.save_dataset(dataset, "ICP")
    
    stats = generator.get_dataset_stats(dataset)
    print(f"\nğŸ“Š ICP ë°ì´í„°ì…‹ í†µê³„:")
    print(f"   ì´ ë£¨ë¸Œë¦­ ìˆ˜: {stats['total_rubrics']}")
    print(f"   ì´ ìŒ ìˆ˜: {stats['total_pairs']}")
    print(f"   ë£¨ë¸Œë¦­ë‹¹ í‰ê·  ìŒ ìˆ˜: {stats['average_pairs_per_rubric']:.1f}")
    
    return dataset, saved_files


if __name__ == "__main__":
    print("ğŸš€ RM ë°ì´í„°ì…‹ ìƒì„±ê¸° ëª¨ë“ˆ ë¡œë“œë¨")
    print("ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜:")
    print("   - create_spf_dataset(): SPF ë°ì´í„°ì…‹ ìƒì„±")
    print("   - create_imp_dataset(): IMP ë°ì´í„°ì…‹ ìƒì„±") 
    print("   - create_icp_dataset(): ICP ë°ì´í„°ì…‹ ìƒì„±")
    print("   - RMDatasetGenerator: ë©”ì¸ ìƒì„±ê¸° í´ë˜ìŠ¤")
