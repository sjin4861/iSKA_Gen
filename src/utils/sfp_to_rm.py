#!/usr/bin/env python
# coding: utf-8

"""
Score-based Pairwise Data Generator for Reward Model Training

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì§€ë¬¸ ì±„ì  ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ Reward Model í›ˆë ¨ìš© pairwise ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ë‹¨ê³„ë³„ ê³¼ì •:
1. ê¸°ì¤€ë³„ë¡œ ë°ì´í„° ë¶„ë¥˜ ë° ë™ì¼í•œ source itemì— ëŒ€í•´ì„œ ì ìˆ˜ë³„ë¡œ ì •ë ¬
2. ì ìˆ˜ ì°¨ì´ê°€ ë‚˜ëŠ” ì§€ë¬¸ë¼ë¦¬ ìŒì„ ìƒì„± (5>4, 5>3, 4>3 ë“±)
3. ì ìˆ˜ ì°¨ì´ë§Œí¼ marginì„ ì„¤ì •
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Any
from datetime import datetime
from collections import defaultdict, Counter
import itertools

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.append(str(PROJECT_ROOT))
sys.path.append(str(PROJECT_ROOT / 'src'))

# ë£¨ë¸Œë¦­ ì •ì˜
RUBRICS = [
    "completeness_for_guidelines",
    "clarity_of_core_theme", 
    "reference_groundedness",
    "logical_flow",
    "korean_quality",
    "l2_learner_suitability"
]

def load_evaluation_data(evaluation_dir: str) -> Dict[str, List[Dict]]:
    """
    í‰ê°€ ê²°ê³¼ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        evaluation_dir: í‰ê°€ ê²°ê³¼ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        Dict[str, List[Dict]]: ëª¨ë¸ë³„ í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    evaluation_data = {}
    eval_path = Path(evaluation_dir)
    
    if not eval_path.exists():
        print(f"âŒ í‰ê°€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {evaluation_dir}")
        return evaluation_data
    
    print(f"ğŸ” ë””ë²„ê¹…: ê²€ìƒ‰ ì‹œì‘ ë””ë ‰í† ë¦¬ = {eval_path}")
    
    # í‰ê°€ íŒŒì¼ ê²€ìƒ‰ (eval_rubric íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰)
    for eval_file in eval_path.rglob("*.json"):
        print(f"ğŸ” ë””ë²„ê¹…: ì°¾ì€ íŒŒì¼ = {eval_file}")
        if "eval_rubric" in eval_file.name:
            try:
                with open(eval_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # íŒŒì¼ ê²½ë¡œì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ
                parts = eval_file.parts
                model_name = "unknown"
                for part in parts:
                    if "_evaluation" in part:
                        model_name = part.replace("_evaluation", "")
                        break
                
                # íŒŒì¼ëª…ì—ì„œ ëª¨ë¸ëª…ê³¼ ë²¤ì¹˜ë§ˆí¬ ì •ë³´ ì¶”ì¶œ
                filename = eval_file.name
                # benchmark_1_v1.0.0_eval_rubric.json í˜•ì‹
                if "eval_rubric" in filename:
                    key = f"{model_name}_{filename}"
                    evaluation_data[key] = data
                    print(f"âœ… ë¡œë“œë¨: {key} ({len(data)}ê°œ í•­ëª©)")
                    print(f"ğŸ” ë””ë²„ê¹…: ì „ì²´ ê²½ë¡œ = {eval_file}")
                    
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {eval_file.name} - {e}")
    
    print(f"ğŸ” ë””ë²„ê¹…: ì´ ë¡œë“œëœ íŒŒì¼ ìˆ˜ = {len(evaluation_data)}")
    print(f"ğŸ” ë””ë²„ê¹…: ë¡œë“œëœ í‚¤ ëª©ë¡ = {list(evaluation_data.keys())}")
    
    return evaluation_data

def extract_source_key(source_item: Dict) -> str:
    """
    source_itemì—ì„œ ê³ ìœ  í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    korean_topic = source_item.get('korean_topic', 'N/A')
    foreign_topic = source_item.get('foreign_topic', 'N/A')
    return f"{korean_topic}||{foreign_topic}"

def group_by_source_and_score(evaluation_data: Dict[str, List[Dict]], rubric: str) -> Dict[str, Dict[int, List[Dict]]]:
    """
    1ë‹¨ê³„: ë™ì¼í•œ source itemì— ëŒ€í•´ ì ìˆ˜ë³„ë¡œ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    
    Args:
        evaluation_data: í‰ê°€ ê²°ê³¼ ë°ì´í„°
        rubric: í‰ê°€ ê¸°ì¤€ (ì˜ˆ: "completeness_for_guidelines")
        
    Returns:
        Dict[source_key, Dict[score, List[passage_data]]]
    """
    grouped_data = defaultdict(lambda: defaultdict(list))
    
    print(f"ğŸ” ë””ë²„ê¹…: group_by_source_and_score ì‹œì‘")
    print(f"ğŸ” ë””ë²„ê¹…: ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜ = {len(evaluation_data)}")
    print(f"ğŸ” ë””ë²„ê¹…: ì°¾ì„ ë£¨ë¸Œë¦­ = {rubric}")
    
    for file_path, data in evaluation_data.items():
        print(f"ğŸ” ë””ë²„ê¹…: ì²˜ë¦¬ ì¤‘ì¸ íŒŒì¼ = {file_path}")
        print(f"ğŸ” ë””ë²„ê¹…: í•´ë‹¹ íŒŒì¼ì˜ ë°ì´í„° ìˆ˜ = {len(data)}")
        
        valid_items = 0
        for item in data:
            if 'evaluation' not in item or 'source_item' not in item:
                continue
                
            evaluation = item['evaluation']
            score_key = f"{rubric}_score"
            
            if score_key not in evaluation:
                continue
                
            score = evaluation[score_key]
            source_key = extract_source_key(item['source_item'])
            
            print(f"ğŸ” ë””ë²„ê¹…: source_key = {source_key[:50]}...")
            print(f"ğŸ” ë””ë²„ê¹…: score = {score}")
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            passage_data = {
                'source_item': item['source_item'],
                'generated_passage': item['generated_passage'],
                'evaluation': evaluation,
                'score': score,
                'file_path': file_path,
                'rubric': rubric
            }
            
            grouped_data[source_key][score].append(passage_data)
            valid_items += 1
        
        print(f"ğŸ” ë””ë²„ê¹…: í•´ë‹¹ íŒŒì¼ì—ì„œ ìœ íš¨í•œ ì•„ì´í…œ ìˆ˜ = {valid_items}")
    
    print(f"ğŸ” ë””ë²„ê¹…: ê·¸ë£¹í™”ëœ source_key ìˆ˜ = {len(grouped_data)}")
    
    # ê° source_keyë³„ ì ìˆ˜ ë¶„í¬ ì¶œë ¥
    for source_key, score_groups in list(grouped_data.items())[:3]:  # ì²« 3ê°œë§Œ ì¶œë ¥
        print(f"ğŸ” ë””ë²„ê¹…: source_key = {source_key[:50]}...")
        print(f"ğŸ” ë””ë²„ê¹…:   ì ìˆ˜ë³„ ë¶„í¬ = {dict(score_groups.keys())}")
        for score, passages in score_groups.items():
            print(f"ğŸ” ë””ë²„ê¹…:     ì ìˆ˜ {score}: {len(passages)}ê°œ ì§€ë¬¸")

    return grouped_data

def create_pairs(grouped_data: Dict[str, Dict[int, List[Dict]]], min_score_diff: int = 1) -> List[Dict]:
    """
    2ë‹¨ê³„: ì ìˆ˜ ì°¨ì´ê°€ ë‚˜ëŠ” ì§€ë¬¸ë¼ë¦¬ ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        grouped_data: ê·¸ë£¹í™”ëœ ë°ì´í„°
        min_score_diff: ìµœì†Œ ì ìˆ˜ ì°¨ì´ (ê¸°ë³¸ê°’: 1)
        
    Returns:
        List[Dict]: ìƒì„±ëœ ìŒ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    pairs = []
    
    print(f"ğŸ” ë””ë²„ê¹…: create_pairs ì‹œì‘")
    print(f"ğŸ” ë””ë²„ê¹…: ì²˜ë¦¬í•  source_key ìˆ˜ = {len(grouped_data)}")
    
    for source_key, score_groups in grouped_data.items():
        scores = sorted(score_groups.keys())
        print(f"ğŸ” ë””ë²„ê¹…: source_key = {source_key[:50]}...")
        print(f"ğŸ” ë””ë²„ê¹…:   ì‚¬ìš© ê°€ëŠ¥í•œ ì ìˆ˜ë“¤ = {scores}")
        
        # ëª¨ë“  ì ìˆ˜ ì¡°í•©ì— ëŒ€í•´ ìŒ ìƒì„±
        for high_score, low_score in itertools.combinations(scores, 2):
            score_diff = high_score - low_score
            print(f"ğŸ” ë””ë²„ê¹…:   ì ìˆ˜ ì¡°í•© ì‹œë„: {high_score} vs {low_score} (ì°¨ì´: {score_diff})")
            
            if score_diff < min_score_diff:
                print(f"ğŸ” ë””ë²„ê¹…:     ì ìˆ˜ ì°¨ì´ê°€ ìµœì†Œê°’({min_score_diff})ë³´ë‹¤ ì‘ìŒ - ê±´ë„ˆëœ€")
                continue
                
            high_passages = score_groups[high_score]
            low_passages = score_groups[low_score]
            
            print(f"ğŸ” ë””ë²„ê¹…:     ë†’ì€ ì ìˆ˜({high_score}) ì§€ë¬¸ ìˆ˜: {len(high_passages)}")
            print(f"ğŸ” ë””ë²„ê¹…:     ë‚®ì€ ì ìˆ˜({low_score}) ì§€ë¬¸ ìˆ˜: {len(low_passages)}")
            
            # ë†’ì€ ì ìˆ˜ì™€ ë‚®ì€ ì ìˆ˜ ì§€ë¬¸ë“¤ ê°„ì˜ ëª¨ë“  ì¡°í•©
            pair_count = 0
            for high_passage in high_passages:
                for low_passage in low_passages:
                    # 3ë‹¨ê³„: margin ê³„ì‚°
                    margin = high_score - low_score
                    
                    pair = {
                        'source_key': source_key,
                        'high_score': high_score,
                        'low_score': low_score,
                        'margin': margin,
                        'chosen_passage': high_passage,
                        'rejected_passage': low_passage,
                        'rubric': high_passage['rubric']
                    }
                    
                    pairs.append(pair)
                    pair_count += 1
            
            print(f"ğŸ” ë””ë²„ê¹…:     ìƒì„±ëœ ìŒ ìˆ˜: {pair_count}")
    
    print(f"ğŸ” ë””ë²„ê¹…: ì´ ìƒì„±ëœ ìŒ ìˆ˜ = {len(pairs)}")
    return pairs

def format_for_reward_model(pairs: List[Dict], rubric: str) -> List[Dict]:
    """
    Reward Model í›ˆë ¨ìš© í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    IMP_...json íŒŒì¼ í˜•ì‹ì„ ì°¸ê³ í•˜ì—¬ ìƒì„±í•©ë‹ˆë‹¤.
    """
    formatted_data = []
    
    for pair in pairs:
        # ê³µí†µ ì¡°ê±´ ìƒì„± (source_itemì—ì„œ ì¶”ì¶œ)
        source_item = pair['chosen_passage']['source_item']
        
        # í‰ê°€ ê¸°ì¤€ ì„¤ëª… ìƒì„±
        rubric_descriptions = {
            "completeness_for_guidelines": "ì§€ë¬¸ì´ ì£¼ì–´ì§„ 3ê°œì˜ `ê³µí†µ ì¡°ê±´` ê°ê°ì— ëŒ€í•œ ì§ˆë¬¸ì„ ëª¨ë‘ ë§Œë“¤ ìˆ˜ ìˆì„ ë§Œí¼, ì¶©ë¶„í•˜ê³  ê· í˜• ì¡íŒ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ê°€?",
            "clarity_of_core_theme": "ì§€ë¬¸ì´ í•œêµ­ ë¬¸í™”ì™€ ì™¸êµ­ ë¬¸í™”ì˜ í•µì‹¬ ì£¼ì œë¥¼ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì œì‹œí•˜ê³  ìˆëŠ”ê°€?",
            "reference_groundedness": "ì§€ë¬¸ì´ ì œê³µëœ ì°¸ê³  ìë£Œì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê³  ê·¼ê±°ë¡œ í™œìš©í•˜ê³  ìˆëŠ”ê°€?",
            "logical_flow": "ì§€ë¬¸ì˜ ë‚´ìš©ì´ ë…¼ë¦¬ì  ìˆœì„œì™€ ì—°ê²°ì„±ì„ ê°–ì¶”ì–´ ì¼ê´€ì„± ìˆê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ê°€?",
            "korean_quality": "ì§€ë¬¸ì´ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±ë˜ì–´ ìˆëŠ”ê°€?",
            "l2_learner_suitability": "ì§€ë¬¸ì´ L2 í•™ìŠµì(í•œêµ­ì–´ë¥¼ ì™¸êµ­ì–´ë¡œ í•™ìŠµí•˜ëŠ” ì‚¬ëŒ)ì—ê²Œ ì í•©í•œ ìˆ˜ì¤€ê³¼ í‘œí˜„ìœ¼ë¡œ ì‘ì„±ë˜ì–´ ìˆëŠ”ê°€?"
        }
        
        rubric_names = {
            "completeness_for_guidelines": "í‰ê°€ ì§€ì¹¨ ì™„ì „ì„±",
            "clarity_of_core_theme": "í•µì‹¬ ì£¼ì œ ëª…í™•ì„±", 
            "reference_groundedness": "ì°¸ê³ ìë£Œ ê¸°ë°˜ì„±",
            "logical_flow": "ë…¼ë¦¬ì  íë¦„",
            "korean_quality": "í•œêµ­ì–´ í’ˆì§ˆ",
            "l2_learner_suitability": "L2 í•™ìŠµì ì í•©ì„±"
        }
        
        # ë¬¸ì œ ìœ í˜•ê³¼ í‰ê°€ ëª©í‘œëŠ” source_itemì´ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
        problem_types = "ë¬¸í™” ë¹„êµí•˜ê¸°, ë‚´ìš© ë¶„ì„í•˜ê¸°, ì˜ê²¬ ì œì‹œí•˜ê¸°"
        eval_goals = "ë¬¸í™”ì  ì°¨ì´ì ê³¼ ê³µí†µì ì„ íŒŒì•…í•˜ì—¬ ë¹„êµ ë¶„ì„í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•œë‹¤., ì£¼ì–´ì§„ ë‚´ìš©ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ê³  ë¶„ì„í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•œë‹¤., ë¬¸í™”ì  í˜„ìƒì— ëŒ€í•œ ìì‹ ì˜ ì˜ê²¬ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì œì‹œí•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•œë‹¤."
        
        # ëŒ€í™” í˜•ì‹ ìƒì„±
        conversation = {
            "conversations": [
                {
                    "from": "human",
                    "value": f"ë‘ ê°œì˜ í•œêµ­ì–´ ë…í•´ ì§€ë¬¸ Aì™€ Bê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.\në‘ ì§€ë¬¸ì€ ëª¨ë‘ ì•„ë˜ [ê³µí†µ ì¡°ê±´]ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n[ê³µí†µ ì¡°ê±´]:\n- ë¬¸ì œ ìœ í˜•: {problem_types}\n- í‰ê°€ ëª©í‘œ: {eval_goals}\n\në‹¤ìŒì€ ìƒì„±ëœ ì§€ë¬¸ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ê¸°ì¤€ì…ë‹ˆë‹¤:\n\n**í‰ê°€ ê¸°ì¤€:**\n- {rubric_descriptions[rubric]}\n\nìœ„ ê¸°ì¤€ì— ë”°ë¼, ë‘ ì§€ë¬¸ ì¤‘ [{rubric_names[rubric]}] ì¸¡ë©´ì—ì„œ ë” ìš°ìˆ˜í•œ ê²ƒì„ ì„ íƒí•˜ì„¸ìš”.\n"
                }
            ],
            "chosen": {
                "from": "gpt",
                "value": pair['chosen_passage']['generated_passage']
            },
            "rejected": {
                "from": "gpt", 
                "value": pair['rejected_passage']['generated_passage']
            },
            "metadata": {
                "source_key": pair['source_key'],
                "rubric": rubric,
                "high_score": pair['high_score'],
                "low_score": pair['low_score'],
                "margin": pair['margin'],
                "korean_topic": source_item.get('korean_topic', 'N/A'),
                "foreign_topic": source_item.get('foreign_topic', 'N/A'),
                "korean_context": source_item.get('korean_context', 'N/A'),
                "foreign_context": source_item.get('foreign_context', 'N/A'),
                "guidelines": {
                    "problem_types": problem_types,
                    "eval_goals": eval_goals
                }
            }
        }
        
        formatted_data.append(conversation)
    
    return formatted_data

def save_pairwise_data(data: List[Dict], rubric: str, output_dir: str, date_str: str = None):
    """
    ìƒì„±ëœ pairwise ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    if date_str is None:
        date_str = datetime.now().strftime("%Y%m%d")
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    filename = f"IMP_{rubric}_{date_str}.json"
    filepath = output_path / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì €ì¥ë¨: {filepath} ({len(data)}ê°œ ìŒ)")
    return filepath

def analyze_pair_statistics(pairs: List[Dict], rubric: str):
    """
    ìƒì„±ëœ ìŒ ë°ì´í„°ì˜ í†µê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    print(f"\nğŸ“Š {rubric} ë£¨ë¸Œë¦­ ìŒ ìƒì„± í†µê³„:")
    print(f"   ì´ ìŒ ê°œìˆ˜: {len(pairs)}")
    
    # ì ìˆ˜ ì°¨ì´ë³„ ë¶„í¬
    margin_counts = Counter([pair['margin'] for pair in pairs])
    print(f"   ì ìˆ˜ ì°¨ì´ë³„ ë¶„í¬:")
    for margin in sorted(margin_counts.keys()):
        print(f"     ì°¨ì´ {margin}ì : {margin_counts[margin]}ê°œ")
    
    # ì ìˆ˜ë³„ ë¶„í¬
    high_score_counts = Counter([pair['high_score'] for pair in pairs])
    low_score_counts = Counter([pair['low_score'] for pair in pairs])
    print(f"   ë†’ì€ ì ìˆ˜ ë¶„í¬: {dict(sorted(high_score_counts.items()))}")
    print(f"   ë‚®ì€ ì ìˆ˜ ë¶„í¬: {dict(sorted(low_score_counts.items()))}")
    
    # ì†ŒìŠ¤ë³„ ë¶„í¬
    source_counts = Counter([pair['source_key'] for pair in pairs])
    print(f"   ì†ŒìŠ¤ ì•„ì´í…œë³„ ìŒ ê°œìˆ˜ (ìƒìœ„ 5ê°œ):")
    for source_key, count in source_counts.most_common(5):
        topics = source_key.split('||')
        korean_topic = topics[0][:20] + "..." if len(topics[0]) > 20 else topics[0]
        foreign_topic = topics[1][:20] + "..." if len(topics[1]) > 20 else topics[1]
        print(f"     {korean_topic} vs {foreign_topic}: {count}ê°œ")

def generate_pairwise_data_for_rubric(evaluation_dir: str, rubric: str, output_dir: str, 
                                     min_score_diff: int = 1, date_str: str = None) -> str:
    """
    íŠ¹ì • ë£¨ë¸Œë¦­ì— ëŒ€í•œ pairwise ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        evaluation_dir: í‰ê°€ ê²°ê³¼ ë””ë ‰í† ë¦¬
        rubric: í‰ê°€ ê¸°ì¤€
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        min_score_diff: ìµœì†Œ ì ìˆ˜ ì°¨ì´
        date_str: ë‚ ì§œ ë¬¸ìì—´
        
    Returns:
        str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    print(f"\nğŸ”„ {rubric} ë£¨ë¸Œë¦­ pairwise ë°ì´í„° ìƒì„± ì‹œì‘...")
    
    # 1ë‹¨ê³„: í‰ê°€ ë°ì´í„° ë¡œë“œ
    print("1ï¸âƒ£ í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘...")
    evaluation_data = load_evaluation_data(evaluation_dir)
    
    if not evaluation_data:
        print(f"âŒ {rubric}: í‰ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # 2ë‹¨ê³„: ì†ŒìŠ¤ë³„/ì ìˆ˜ë³„ ê·¸ë£¹í™”
    print("2ï¸âƒ£ ë°ì´í„° ê·¸ë£¹í™” ì¤‘...")
    grouped_data = group_by_source_and_score(evaluation_data, rubric)
    
    if not grouped_data:
        print(f"âŒ {rubric}: ê·¸ë£¹í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # 3ë‹¨ê³„: ìŒ ìƒì„±
    print("3ï¸âƒ£ ìŒ ìƒì„± ì¤‘...")
    pairs = create_pairs(grouped_data, min_score_diff)
    
    if not pairs:
        print(f"âŒ {rubric}: ìƒì„±í•  ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # 4ë‹¨ê³„: í˜•ì‹ ë³€í™˜
    print("4ï¸âƒ£ í˜•ì‹ ë³€í™˜ ì¤‘...")
    formatted_data = format_for_reward_model(pairs, rubric)
    
    # 5ë‹¨ê³„: í†µê³„ ë¶„ì„
    analyze_pair_statistics(pairs, rubric)
    
    # 6ë‹¨ê³„: ì €ì¥
    print("5ï¸âƒ£ ë°ì´í„° ì €ì¥ ì¤‘...")
    filepath = save_pairwise_data(formatted_data, rubric, output_dir, date_str)
    
    return filepath

def generate_all_pairwise_data(evaluation_dir: str, output_dir: str, 
                              min_score_diff: int = 1, date_str: str = None):
    """
    ëª¨ë“  ë£¨ë¸Œë¦­ì— ëŒ€í•´ pairwise ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("ğŸš€ ëª¨ë“  ë£¨ë¸Œë¦­ì— ëŒ€í•œ pairwise ë°ì´í„° ìƒì„± ì‹œì‘...")
    print(f"   í‰ê°€ ë°ì´í„° ë””ë ‰í† ë¦¬: {evaluation_dir}")
    print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"   ìµœì†Œ ì ìˆ˜ ì°¨ì´: {min_score_diff}")
    print(f"   ë‚ ì§œ: {date_str or 'auto'}")
    
    generated_files = []
    
    for rubric in RUBRICS:
        try:
            filepath = generate_pairwise_data_for_rubric(
                evaluation_dir, rubric, output_dir, min_score_diff, date_str
            )
            if filepath:
                generated_files.append(filepath)
        except Exception as e:
            print(f"âŒ {rubric} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    print(f"\nğŸ‰ ì™„ë£Œ! {len(generated_files)}ê°œ íŒŒì¼ ìƒì„±:")
    for filepath in generated_files:
        print(f"   âœ… {filepath}")

# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    # ì„¤ì •
    # ì‹¤ì œ í‰ê°€ ê²°ê³¼ êµ¬ì¡°: /home/sjin4861/25-1/HCLT/iSKA_Gen/src/data/evaluations/2025-08-05/misc/{MODEL_NAME}_evaluation/eval_rubric/benchmark_{ID}_v1.0.0_eval_rubric.json
    EVALUATION_DIR = "src/data/evaluations/2025-08-05/misc"  # ëª¨ë“  ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ í¬í•¨í•˜ëŠ” ìƒìœ„ ê²½ë¡œ
    OUTPUT_DIR = "src/data/pairwise_data/train/v4/generated"  # ìƒì„±ëœ pairwise ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
    MIN_SCORE_DIFF = 1  # ìµœì†Œ ì ìˆ˜ ì°¨ì´
    DATE_STR = "2025-08-08"  # ë‚ ì§œ ë¬¸ìì—´
    
    # ë‹¨ì¼ ë£¨ë¸Œë¦­ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª ë‹¨ì¼ ë£¨ë¸Œë¦­ í…ŒìŠ¤íŠ¸:")
    test_rubric = "completeness_for_guidelines"
    generate_pairwise_data_for_rubric(
        EVALUATION_DIR, test_rubric, OUTPUT_DIR, MIN_SCORE_DIFF, DATE_STR
    )
    
    # ì „ì²´ ë£¨ë¸Œë¦­ ìƒì„± (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
    # print("\n" + "="*80)
    # generate_all_pairwise_data(EVALUATION_DIR, OUTPUT_DIR, MIN_SCORE_DIFF, DATE_STR)
