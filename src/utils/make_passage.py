import os
import sys
import json
from pathlib import Path
import gc  # <-- í•´ê²°ì±… 2: ê°€ë¹„ì§€ ì»¬ë ‰í„° ëª¨ë“ˆ ì„í¬íŠ¸
import torch
import pandas as pd
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))
sys.path.append(str(Path(__file__).parent.parent / 'modules'))

from modules.iska.passage_agent import PassageAgent
from modules.model_client import LocalModelClient
from utils.output_saver import save_model_output
from utils.benchmark_loader import load_benchmarks
import re

def clean_passage_text(text: str) -> str:
    """
    ì§€ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš©ì„ ì œê±°í•˜ëŠ” í›„ì²˜ë¦¬ í•¨ìˆ˜
    
    Args:
        text (str): ì›ë³¸ ì§€ë¬¸ í…ìŠ¤íŠ¸
        
    Returns:
        str: ê´„í˜¸ ë‚´ìš©ì´ ì œê±°ëœ ì •ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return text
        
    # ëª¨ë“  ì¢…ë¥˜ì˜ ê´„í˜¸ ì œê±°: (), [], {}, ã€ã€‘, ã€ã€ ë“±
    # ì¤‘ê´„í˜¸, ëŒ€ê´„í˜¸, ì†Œê´„í˜¸, í•œê¸€ ê´„í˜¸ ë“± ëª¨ë“  ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±°
    patterns = [
        r'\([^)]*\)',      # (ë‚´ìš©)
        r'\[[^\]]*\]',     # [ë‚´ìš©]
        r'\{[^}]*\}',      # {ë‚´ìš©}
        r'ã€[^ã€‘]*ã€‘',       # ã€ë‚´ìš©ã€‘
        r'ã€[^ã€]*ã€',       # ã€ë‚´ìš©ã€
        r'ã€Œ[^ã€]*ã€',       # ã€Œë‚´ìš©ã€
        r'ã€ˆ[^ã€‰]*ã€‰',       # ã€ˆë‚´ìš©ã€‰
        r'ã€Š[^ã€‹]*ã€‹',       # ã€Šë‚´ìš©ã€‹
    ]
    
    cleaned_text = text
    for pattern in patterns:
        cleaned_text = re.sub(pattern, '', cleaned_text)
    
    # ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    cleaned_text = cleaned_text.strip()
    
    return cleaned_text

def generate_passage(benchmark_file : str, model_name : str,  template_key : str, benchmark_version: str = "v1.0.0", gpus : list = [2,3], BENCH_ID_LIST : list =  [1, 2, 3, 4, 5], date_str: str = None):
    benchmarks = load_benchmarks(benchmark_file)

    llm_client = LocalModelClient(model_name=model_name, gpus = gpus)
    passage_agent = PassageAgent(llm_client=llm_client)
    
    for id in BENCH_ID_LIST:
        benchmark = benchmarks[id - 1]  # idëŠ” 1ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ -1ì„ í•´ì¤Œ
        problem_types = benchmark['problem_types']
        eval_goals = benchmark['eval_goals']
        passage_datas = []
        for item in benchmark['items']:
            korean_topic = item['korean_topic']
            korean_context = item['korean_context']
            foreign_topic = item['foreign_topic']
            foreign_context = item['foreign_context']
        
            source_item = {
                "korean_topic": korean_topic,
                "korean_context": korean_context,
                "foreign_topic": foreign_topic,
                "foreign_context": foreign_context
            }
            # ìµœëŒ€ 10ë²ˆê¹Œì§€ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ ì¶”ê°€
            max_retries = 10
            retry_count = 0
            generated_passage = None
            
            while generated_passage is None and retry_count < max_retries:
                temp_passage = passage_agent.generate_passage(korean_topic=korean_topic, korean_context=korean_context, foreign_topic=foreign_topic, foreign_context=foreign_context, problem_types=problem_types, eval_goals=eval_goals, template_key=template_key)
                
                if temp_passage is None:
                    retry_count += 1
                    print(f"Passage generation returned None. Retrying... ({retry_count}/{max_retries})")
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    torch.cuda.empty_cache()
                    gc.collect()
                else:
                    # í›„ì²˜ë¦¬: ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±°
                    # temp_passage = clean_passage_text(temp_passage)
                    print(f"Passage cleaned (removed brackets and their contents)")
                    
                    # ê¸¸ì´ ê²€ì¦ (ê³µë°± í¬í•¨)
                    passage_length = len(temp_passage)
                    if passage_length < 300:
                        retry_count += 1
                        print(f"Passage too short ({passage_length} chars < 300). Retrying... ({retry_count}/{max_retries})")
                        # ë©”ëª¨ë¦¬ ì •ë¦¬
                        torch.cuda.empty_cache()
                        gc.collect()
                    elif passage_length > 500:
                        retry_count += 1
                        print(f"Passage too long ({passage_length} chars > 500). Retrying... ({retry_count}/{max_retries})")
                        # ë©”ëª¨ë¦¬ ì •ë¦¬
                        torch.cuda.empty_cache()
                        gc.collect()
                    else:
                        # ê¸¸ì´ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ í†µê³¼
                        generated_passage = temp_passage
                        print(f"Passage length validated ({passage_length} chars). Generation successful.")
                        break  # while ë£¨í”„ íƒˆì¶œ
            
            # ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ passage ì‚¬ìš©
            passage_data = {
                "source_item": source_item,
                "generated_passage": generated_passage,
            }
            passage_datas.append(passage_data)
        
        # output_saverë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ ì €ì¥
        saved_file = save_model_output(
            model_name=model_name,
            benchmark_id=id,
            benchmark_version=benchmark_version,
            template_key=template_key,
            data=passage_datas,
            date_str=date_str
        )
        print(f"Generated passage for benchmark ID {id} and saved to {saved_file}")


def generate_single_passage(
    korean_topic: str,
    korean_context: str,
    foreign_topic: str,
    foreign_context: str,
    problem_types: list,
    eval_goals: list,
    model_name: str,
    template_key: str,
    gpus: list = [2, 3],
    max_retries: int = 10,
    min_length: int = 300,
    max_length: int = 500
) -> dict:
    """
    íŠ¹ì •í•œ ë²¤ì¹˜ë§ˆí¬ ì •ë³´ë¥¼ ì…ë ¥ë°›ì•„ ì§€ë¬¸ í•˜ë‚˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        korean_topic (str): í•œêµ­ ì£¼ì œ
        korean_context (str): í•œêµ­ ì»¨í…ìŠ¤íŠ¸
        foreign_topic (str): ì™¸êµ­ ì£¼ì œ
        foreign_context (str): ì™¸êµ­ ì»¨í…ìŠ¤íŠ¸
        problem_types (list): ë¬¸ì œ ìœ í˜• ë¦¬ìŠ¤íŠ¸ (3ê°œ)
        eval_goals (list): í‰ê°€ ëª©í‘œ ë¦¬ìŠ¤íŠ¸ (3ê°œ)
        model_name (str): ì‚¬ìš©í•  ëª¨ë¸ëª…
        template_key (str): í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
        gpus (list): ì‚¬ìš©í•  GPU ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: [2, 3])
        max_retries (int): ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 10)
        min_length (int): ìµœì†Œ ì§€ë¬¸ ê¸¸ì´ (ê¸°ë³¸ê°’: 300)
        max_length (int): ìµœëŒ€ ì§€ë¬¸ ê¸¸ì´ (ê¸°ë³¸ê°’: 500)
        
    Returns:
        dict: ìƒì„±ëœ ì§€ë¬¸ ë°ì´í„° ë˜ëŠ” ì˜¤ë¥˜ ì •ë³´
    """
    print(f"\nğŸ”§ ë‹¨ì¼ ì§€ë¬¸ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"   ğŸ“š í•œêµ­ ì£¼ì œ: {korean_topic}")
    print(f"   ğŸŒ ì™¸êµ­ ì£¼ì œ: {foreign_topic}")
    print(f"   ğŸ¤– ëª¨ë¸: {model_name}")
    print(f"   ğŸ“ í…œí”Œë¦¿: {template_key}")
    
    try:
        # LLM í´ë¼ì´ì–¸íŠ¸ ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        llm_client = LocalModelClient(model_name=model_name, gpus=gpus)
        passage_agent = PassageAgent(llm_client=llm_client)
        
        source_item = {
            "korean_topic": korean_topic,
            "korean_context": korean_context,
            "foreign_topic": foreign_topic,
            "foreign_context": foreign_context
        }
        
        # ì§€ë¬¸ ìƒì„± ì¬ì‹œë„ ë¡œì§
        retry_count = 0
        generated_passage = None
        
        while generated_passage is None and retry_count < max_retries:
            print(f"   ğŸ”„ ì‹œë„ {retry_count + 1}/{max_retries}...")
            
            temp_passage = passage_agent.generate_passage(
                korean_topic=korean_topic,
                korean_context=korean_context,
                foreign_topic=foreign_topic,
                foreign_context=foreign_context,
                problem_types=problem_types,
                eval_goals=eval_goals,
                template_key=template_key
            )
            
            if temp_passage is None:
                retry_count += 1
                print(f"   âš ï¸ ì§€ë¬¸ ìƒì„± ì‹¤íŒ¨. ì¬ì‹œë„ ì¤‘... ({retry_count}/{max_retries})")
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                torch.cuda.empty_cache()
                gc.collect()
            else:
                # í›„ì²˜ë¦¬: ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±° (í•„ìš”ì‹œ)
                # temp_passage = clean_passage_text(temp_passage)
                
                # ê¸¸ì´ ê²€ì¦
                passage_length = len(temp_passage)
                print(f"   ğŸ“ ìƒì„±ëœ ì§€ë¬¸ ê¸¸ì´: {passage_length}ì")
                
                if passage_length < min_length:
                    retry_count += 1
                    print(f"   âš ï¸ ì§€ë¬¸ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({passage_length}ì < {min_length}ì). ì¬ì‹œë„...")
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    torch.cuda.empty_cache()
                    gc.collect()
                elif passage_length > max_length:
                    retry_count += 1
                    print(f"   âš ï¸ ì§€ë¬¸ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({passage_length}ì > {max_length}ì). ì¬ì‹œë„... ({retry_count}/{max_retries})")
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    torch.cuda.empty_cache()
                    gc.collect()
                else:
                    # ê¸¸ì´ ì¡°ê±´ ë§Œì¡±
                    generated_passage = temp_passage
                    print(f"   âœ… ì§€ë¬¸ ê¸¸ì´ ê²€ì¦ í†µê³¼: {passage_length}ì")
                    break
        
        # ê²°ê³¼ ë°˜í™˜
        if generated_passage is not None:
            result = {
                "success": True,
                "source_item": source_item,
                "generated_passage": generated_passage,
                "passage_length": len(generated_passage),
                "retries_used": retry_count,
                "generation_info": {
                    "model_name": model_name,
                    "template_key": template_key,
                    "gpus": gpus
                }
            }
            print(f"   ğŸ‰ ì§€ë¬¸ ìƒì„± ì„±ê³µ! (ì¬ì‹œë„ {retry_count}íšŒ)")
            print(f"   ğŸ“ ìƒì„±ëœ ì§€ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {generated_passage[:100]}...")
            return result
        else:
            result = {
                "success": False,
                "error": f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries})ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.",
                "source_item": source_item,
                "retries_used": retry_count,
                "generation_info": {
                    "model_name": model_name,
                    "template_key": template_key,
                    "gpus": gpus
                }
            }
            print(f"   âŒ ì§€ë¬¸ ìƒì„± ì‹¤íŒ¨: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
            return result
            
    except Exception as e:
        result = {
            "success": False,
            "error": f"ì§€ë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "source_item": {
                "korean_topic": korean_topic,
                "korean_context": korean_context,
                "foreign_topic": foreign_topic,
                "foreign_context": foreign_context
            },
            "generation_info": {
                "model_name": model_name,
                "template_key": template_key,
                "gpus": gpus
            }
        }
        print(f"   âŒ ì§€ë¬¸ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return result


def generate_single_passage_from_benchmark(
    benchmark_file: str,
    benchmark_id: int,
    item_index: int,
    model_name: str,
    template_key: str,
    benchmark_version: str = "v1.0.0",
    gpus: list = [2, 3],
    max_retries: int = 10,
    min_length: int = 300,
    max_length: int = 500
) -> dict:
    """
    ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ì—ì„œ íŠ¹ì • ì•„ì´í…œì„ ì„ íƒí•˜ì—¬ ì§€ë¬¸ í•˜ë‚˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        benchmark_file (str): ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ëª…
        benchmark_id (int): ë²¤ì¹˜ë§ˆí¬ ID (1-5)
        item_index (int): ë²¤ì¹˜ë§ˆí¬ ë‚´ ì•„ì´í…œ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)
        model_name (str): ì‚¬ìš©í•  ëª¨ë¸ëª…
        template_key (str): í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
        benchmark_version (str): ë²¤ì¹˜ë§ˆí¬ ë²„ì „ (ê¸°ë³¸ê°’: "v1.0.0")
        gpus (list): ì‚¬ìš©í•  GPU ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: [2, 3])
        max_retries (int): ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 10)
        min_length (int): ìµœì†Œ ì§€ë¬¸ ê¸¸ì´ (ê¸°ë³¸ê°’: 300)
        max_length (int): ìµœëŒ€ ì§€ë¬¸ ê¸¸ì´ (ê¸°ë³¸ê°’: 500)
        
    Returns:
        dict: ìƒì„±ëœ ì§€ë¬¸ ë°ì´í„° ë˜ëŠ” ì˜¤ë¥˜ ì •ë³´
    """
    print(f"\nğŸ”§ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë‹¨ì¼ ì§€ë¬¸ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"   ğŸ“„ ë²¤ì¹˜ë§ˆí¬ íŒŒì¼: {benchmark_file}")
    print(f"   ğŸ†” ë²¤ì¹˜ë§ˆí¬ ID: {benchmark_id}")
    print(f"   ğŸ“ ì•„ì´í…œ ì¸ë±ìŠ¤: {item_index}")
    
    try:
        # ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ
        benchmarks = load_benchmarks(benchmark_file)
        
        if benchmark_id < 1 or benchmark_id > len(benchmarks):
            return {
                "success": False,
                "error": f"ì˜ëª»ëœ ë²¤ì¹˜ë§ˆí¬ ID: {benchmark_id} (ìœ íš¨ ë²”ìœ„: 1-{len(benchmarks)})"
            }
        
        benchmark = benchmarks[benchmark_id - 1]  # IDëŠ” 1ë¶€í„° ì‹œì‘
        problem_types = benchmark['problem_types']
        eval_goals = benchmark['eval_goals']
        items = benchmark['items']
        
        if item_index < 0 or item_index >= len(items):
            return {
                "success": False,
                "error": f"ì˜ëª»ëœ ì•„ì´í…œ ì¸ë±ìŠ¤: {item_index} (ìœ íš¨ ë²”ìœ„: 0-{len(items)-1})"
            }
        
        item = items[item_index]
        
        print(f"   ğŸ“š ì„ íƒëœ ì•„ì´í…œ: {item['korean_topic']} vs {item['foreign_topic']}")
        
        # ë‹¨ì¼ ì§€ë¬¸ ìƒì„± í˜¸ì¶œ
        result = generate_single_passage(
            korean_topic=item['korean_topic'],
            korean_context=item['korean_context'],
            foreign_topic=item['foreign_topic'],
            foreign_context=item['foreign_context'],
            problem_types=problem_types,
            eval_goals=eval_goals,
            model_name=model_name,
            template_key=template_key,
            gpus=gpus,
            max_retries=max_retries,
            min_length=min_length,
            max_length=max_length
        )
        
        # ë²¤ì¹˜ë§ˆí¬ ì •ë³´ ì¶”ê°€
        if result["success"]:
            result["benchmark_info"] = {
                "benchmark_file": benchmark_file,
                "benchmark_id": benchmark_id,
                "item_index": item_index,
                "benchmark_version": benchmark_version,
                "problem_types": problem_types,
                "eval_goals": eval_goals
            }
        
        return result
        
    except Exception as e:
        return {
            "success": False,
            "error": f"ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "benchmark_info": {
                "benchmark_file": benchmark_file,
                "benchmark_id": benchmark_id,
                "item_index": item_index
            }
        }

 