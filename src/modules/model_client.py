import os
import sys
import time
from typing import List, Dict, Optional
from openai import OpenAI, RateLimitError, APITimeoutError
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import json
from dotenv import load_dotenv
from pathlib import Path

# ê²½ë¡œ ì„¤ì •
sys.path.append(str(Path.cwd().parent.parent))

# ëŸ°íƒ€ì„ì— import
try:
    from src.utils.settings_loader import get_settings
except ImportError:
    from utils.settings_loader import get_settings

load_dotenv()
_CFG = get_settings()
_LLM_CFG = _CFG.get('llm', {})
_CHATGPT_CFG = _CFG.get('chatgpt', {})

# === Configuration Constants ===
# LLM (ë¡œì»¬ ëª¨ë¸) ì„¤ì •
_LLM_MAX_TOKENS = int(_LLM_CFG.get('max_tokens', 512))
_LLM_TEMPERATURE = float(_LLM_CFG.get('temperature', 0.7))
_LLM_REPETITION_PENALTY = float(_LLM_CFG.get('repetition_penalty', 1.1))
_LLM_TOP_P = float(_LLM_CFG.get('top_p', 0.9))
_LLM_TOP_K = int(_LLM_CFG.get('top_k', 50))
_LLM_NO_REPEAT_NGRAM_SIZE = int(_LLM_CFG.get('no_repeat_ngram_size', 0))

# ChatGPT (OpenAI) ì„¤ì •
_CHATGPT_MAX_TOKENS = int(_CHATGPT_CFG.get('max_tokens', 1024))
_CHATGPT_TEMPERATURE = float(_CHATGPT_CFG.get('temperature', 0))
_CHATGPT_TOP_P = float(_CHATGPT_CFG.get('top_p', 1.0))
_CHATGPT_FREQUENCY_PENALTY = float(_CHATGPT_CFG.get('frequency_penalty', 0.0))
_CHATGPT_PRESENCE_PENALTY = float(_CHATGPT_CFG.get('presence_penalty', 0.0))

# === Local Models Configuration ===
_LOCAL_MODELS_DIR = os.getenv('LOCAL_MODELS_PATH') or os.path.expanduser(_LLM_CFG.get('local_models_dir', '~/models'))
_DEFAULT_TORCH_DTYPE = torch.bfloat16
_FALLBACK_TORCH_DTYPE = torch.float16

# === Default Values ===
_DEFAULT_OPENAI_MODEL = 'gpt-4o-mini'
_DEFAULT_LOCAL_MODEL = 'default'
_DEFAULT_DEVICE = 'auto'

# === Batch API Configuration (í•„ìš”í•œ ê²½ìš°ë§Œ) ===
_BATCH_COMPLETION_WINDOW = "24h"
_BATCH_POLL_INTERVAL = 10

# === File Names ===
_TEMP_BATCH_FILE = "temp_batch_input.jsonl"

# === Status Constants ===
_BATCH_COMPLETED_STATUSES = ["completed", "failed", "cancelled"]
_API_ERROR_KEYWORDS = {
    "invalid_key": ["invalid_api_key", "401"],
    "rate_limit": ["rate_limit", "429"]
}

# === Error Messages ===
_ERROR_NO_RESPONSE = "ì£„ì†¡í•©ë‹ˆë‹¤. AIë¡œë¶€í„° ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
_ERROR_INVALID_API_KEY = "âŒ OpenAI API í‚¤ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
_ERROR_RATE_LIMIT = "â³ API ì‚¬ìš©ëŸ‰ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

class BaseModelClient:
    """ëª¨ë“  ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ì˜ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤"""
    def call(self, messages: List[Dict], **kwargs) -> str:
        raise NotImplementedError

class OpenAIModelClient(BaseModelClient):
    """ìµœì‹  openai ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í˜¸í™˜ë˜ëŠ” í´ë¼ì´ì–¸íŠ¸"""
    def __init__(self, model_name: str, api_key: Optional[str] = None, **kwargs):
        self.model_name = model_name
        self.client = OpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))
        self.default_params = {
            "temperature": _CHATGPT_TEMPERATURE,
            "max_tokens": _CHATGPT_MAX_TOKENS,
            "top_p": _CHATGPT_TOP_P,
            "frequency_penalty": _CHATGPT_FREQUENCY_PENALTY,
            "presence_penalty": _CHATGPT_PRESENCE_PENALTY,
            **kwargs
        }
        print(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ '{self.model_name}' ëª¨ë¸ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def call(self, messages: List[Dict], **kwargs) -> str:
        params = self.default_params.copy()
        params.update(kwargs)
        
        request_payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": params.get('temperature'),
            "max_tokens": params.get('max_tokens'),
            "top_p": params.get('top_p'),
            "frequency_penalty": params.get('frequency_penalty'),
            "presence_penalty": params.get('presence_penalty')
        }

        # JSON ëª¨ë“œ ê°•ì œ ì—¬ë¶€ í™•ì¸
        if params.get('force_json', False):
            request_payload["response_format"] = {"type": "json_object"}

        try:
            response = self.client.chat.completions.create(**request_payload)
            content = response.choices[0].message.content
            return content or _ERROR_NO_RESPONSE
        except Exception as e:
            print(f"âŒ OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # API í‚¤ ì—ëŸ¬ì¸ì§€ í™•ì¸
            error_str = str(e)
            if any(keyword in error_str for keyword in _API_ERROR_KEYWORDS["invalid_key"]):
                return _ERROR_INVALID_API_KEY
            elif any(keyword in error_str for keyword in _API_ERROR_KEYWORDS["rate_limit"]):
                return _ERROR_RATE_LIMIT
            else:
                return f"âŒ AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)[:100]}"

     # --- âœ¨ ì£¼ìš” ë³€ê²½ ì‚¬í•­ ì‹œì‘ âœ¨ ---

    def _prepare_batch_file(self, batch_of_messages: List[List[Dict]], **kwargs) -> str:
        """ë°°ì¹˜ APIì— ì—…ë¡œë“œí•  .jsonl íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        batch_requests = []
        for i, messages in enumerate(batch_of_messages):
            request_body = {
                "model": self.model_name,
                "messages": messages,
                "temperature": self.default_params.get('temperature'),
                "max_tokens": self.default_params.get('max_tokens')
            }
            # ì¶”ê°€ì ì¸ kwargs íŒŒë¼ë¯¸í„°ë¥¼ bodyì— ì—…ë°ì´íŠ¸
            request_body.update(kwargs)

            batch_requests.append({
                "custom_id": f"request_{i+1}",
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": request_body
            })

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        file_path = _TEMP_BATCH_FILE
        with open(file_path, "w", encoding="utf-8") as f:
            for req in batch_requests:
                f.write(json.dumps(req, ensure_ascii=False) + "\n")
        
        return file_path

    def call_batch(self, batch_of_messages: List[List[Dict]], **kwargs) -> List[str]:
        """
        OpenAI ë°°ì¹˜ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ìš”ì²­ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ë¹„ìš©ì„ ì ˆê°í•©ë‹ˆë‹¤.

        Args:
            batch_of_messages: ê° ìš”ì†Œê°€ message ë¦¬ìŠ¤íŠ¸ì¸ ë°°ì¹˜ ìš”ì²­.
            **kwargs: ìƒì„± íŒŒë¼ë¯¸í„° (temperature, max_tokens ë“±).

        Returns:
            List[str]: ê° ìš”ì²­ì— ëŒ€í•œ ì‘ë‹µ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸.
        """
        print(f"ğŸš€ {len(batch_of_messages)}ê°œì˜ ìš”ì²­ì— ëŒ€í•œ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        # 1. ë°°ì¹˜ ì…ë ¥ íŒŒì¼ ìƒì„±
        batch_file_path = self._prepare_batch_file(batch_of_messages, **kwargs)

        try:
            # 2. íŒŒì¼ ì—…ë¡œë“œ
            print(f"  - 1/4: ë°°ì¹˜ íŒŒì¼ '{batch_file_path}'ì„(ë¥¼) ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
            batch_input_file = self.client.files.create(
                file=open(batch_file_path, "rb"),
                purpose="batch"
            )

            # 3. ë°°ì¹˜ ì‘ì—… ìƒì„±
            print(f"  - 2/4: ë°°ì¹˜ ì‘ì—…ì„ ìƒì„±í•˜ê³  APIì— ì œì¶œí•©ë‹ˆë‹¤...")
            batch_job = self.client.batches.create(
                input_file_id=batch_input_file.id,
                endpoint="/v1/chat/completions",
                completion_window=_BATCH_COMPLETION_WINDOW # 24ì‹œê°„ ë‚´ì— ì™„ë£Œë˜ë„ë¡ ì„¤ì •
            )

            # 4. ë°°ì¹˜ ì‘ì—… ì™„ë£Œ ëŒ€ê¸° (í´ë§)
            print(f"  - 3/4: ë°°ì¹˜ ì‘ì—…(ID: {batch_job.id})ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
            while batch_job.status not in _BATCH_COMPLETED_STATUSES:
                time.sleep(_BATCH_POLL_INTERVAL) # 10ì´ˆë§ˆë‹¤ ìƒíƒœ í™•ì¸
                batch_job = self.client.batches.retrieve(batch_job.id)
                print(f"    - í˜„ì¬ ìƒíƒœ: {batch_job.status}...")

            if batch_job.status != "completed":
                raise RuntimeError(f"ë°°ì¹˜ ì‘ì—…ì´ ì‹¤íŒ¨ ë˜ëŠ” ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì¢… ìƒíƒœ: {batch_job.status}")

            # 5. ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±
            print(f"  - 4/4: ì‘ì—… ì™„ë£Œ! ê²°ê³¼ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤...")
            result_file_id = batch_job.output_file_id
            result_content = self.client.files.content(result_file_id).read()
            
            responses = []
            for line in result_content.decode("utf-8").strip().split("\n"):
                data = json.loads(line)
                # ì‘ë‹µ ë³¸ë¬¸ì—ì„œ contentë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                content = data["response"]["body"]["choices"][0]["message"]["content"]
                responses.append(content)

            print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ! {len(responses)}ê°œì˜ ì‘ë‹µì„ ì„±ê³µì ìœ¼ë¡œ ë°›ì•˜ìŠµë‹ˆë‹¤.")
            return responses

        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return [f"ì˜¤ë¥˜: {e}" for _ in batch_of_messages]
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(batch_file_path):
                os.remove(batch_file_path)

class LocalModelClient(BaseModelClient):
    """
    ë¡œì»¬ HuggingFace ëª¨ë¸ ì¶”ë¡ ìš© í´ë¼ì´ì–¸íŠ¸ (ê°•í™”ëœ GPU ì œì–´ ê¸°ëŠ¥)
    """
    def __init__(self, model_name: str, **kwargs):
        """
        í´ë˜ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³ , ì§€ì •ëœ GPUì— ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            model_name (str): '~/models/' ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë¸ í´ë” ì´ë¦„.
            **kwargs: 'gpus' (List[int])ì™€ ê°™ì€ ì¶”ê°€ ì¸ìë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        self.model_name = model_name
        self.model_path = os.path.join(_LOCAL_MODELS_DIR, model_name)
        
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"Model not found at {self.model_path}")

        # --- ê°•í™”ëœ GPU ì„¤ì • ë¡œì§ ---
        gpus = kwargs.pop('gpus', None)
        device_setting = kwargs.pop('device', _LLM_CFG.get('device', _DEFAULT_DEVICE))
        self.target_gpus = gpus
        
        # ì„¤ì • íŒŒì¼ì˜ device ê°’ì— ë”°ë¥¸ ì²˜ë¦¬
        if device_setting == "cpu":
            # CPU ê°•ì œ ì‚¬ìš©
            self.device_map = "cpu"
            self.target_device = "cpu"
            print("ğŸ¯ CPU ëª¨ë“œ: ì„¤ì •ì— ì˜í•œ CPU ì‚¬ìš©")
        elif gpus is not None and torch.cuda.is_available():
            if isinstance(gpus, list) and all(isinstance(i, int) for i in gpus):
                # 1. CUDA_VISIBLE_DEVICESë¥¼ ë¨¼ì € ì„¤ì • (ë§¤ìš° ì¤‘ìš”!)
                os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, gpus))
                print(f"ğŸ¯ CUDA_VISIBLE_DEVICES ì„¤ì •: {','.join(map(str, gpus))}")
                
                # 2. PyTorch CUDA ìºì‹œ ì •ë¦¬
                torch.cuda.empty_cache()
                print("ğŸ§¹ CUDA ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                
                # 3. GPU ì„¤ì • - CUDA_VISIBLE_DEVICES ì„¤ì • í›„ì—ëŠ” 0ë¶€í„° ì‹œì‘
                if len(gpus) == 1:
                    # ë‹¨ì¼ GPU ì‚¬ìš© - CUDA_VISIBLE_DEVICES ì„¤ì • í›„ì—ëŠ” 0ë²ˆ ì¸ë±ìŠ¤
                    self.device_map = {"": "cuda:0"}
                    self.target_device = "cuda:0"
                    print(f"ğŸ¯ ë‹¨ì¼ GPU ëª¨ë“œ: ì‹¤ì œ GPU {gpus[0]} â†’ ë…¼ë¦¬ì  GPU 0")
                else:
                    # ë‹¤ì¤‘ GPU ì‚¬ìš© - ë…¼ë¦¬ì  GPU ì¸ë±ìŠ¤ ë§¤í•‘
                    gpu_mapping = {f"cuda:{i}": f"cuda:{i}" for i in range(len(gpus))}
                    self.device_map = gpu_mapping
                    self.target_device = "cuda:0"  # ì²« ë²ˆì§¸ ë…¼ë¦¬ì  GPU
                    print(f"ğŸ¯ ë‹¤ì¤‘ GPU ëª¨ë“œ: ì‹¤ì œ GPU {gpus} â†’ ë…¼ë¦¬ì  GPU 0~{len(gpus)-1}")
                    print(f"ğŸ”— ë””ë°”ì´ìŠ¤ ë§¤í•‘: {gpu_mapping}")
            else:
                print("âš ï¸ 'gpus' ì¸ìëŠ” ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤ (ì˜ˆ: [2, 3]). autoë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                self.device_map = "auto"
                self.target_device = "cuda:0" if torch.cuda.is_available() else "cpu"
        else:
            # GPU ì§€ì • ì—†ìŒ - ì„¤ì • íŒŒì¼ ê¸°ë°˜ ì²˜ë¦¬
            if device_setting == "auto" and torch.cuda.is_available():
                self.device_map = "auto"
                self.target_device = "cuda:0"
                print("ğŸ¯ AUTO ëª¨ë“œ: ì„¤ì •ì— ì˜í•œ ìë™ ë””ë°”ì´ìŠ¤ ë§¤í•‘")
            elif device_setting == "cuda" and torch.cuda.is_available():
                self.device_map = "auto"
                self.target_device = "cuda:0"
                print("ğŸ¯ CUDA ëª¨ë“œ: ì„¤ì •ì— ì˜í•œ GPU ì‚¬ìš©")
            else:
                self.device_map = "cpu"
                self.target_device = "cpu"
                print("ğŸ¯ CPU ëª¨ë“œ: CUDA ì‚¬ìš© ë¶ˆê°€ ë˜ëŠ” ì„¤ì •ì— ì˜í•œ CPU ì‚¬ìš©")
        # ------------------------------------
            
        print(f"ğŸ”„ ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}...")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ (CUDA_VISIBLE_DEVICES ì„¤ì • í›„)
        if torch.cuda.is_available() and gpus:
            print("ğŸ“Š GPU ë©”ëª¨ë¦¬ ìƒíƒœ:")
            for logical_idx in range(len(gpus)):
                try:
                    memory_allocated = torch.cuda.memory_allocated(logical_idx) / 1024**3
                    memory_cached = torch.cuda.memory_reserved(logical_idx) / 1024**3
                    memory_total = torch.cuda.get_device_properties(logical_idx).total_memory / 1024**3
                    actual_gpu = gpus[logical_idx]
                    print(f"   ë…¼ë¦¬ì  GPU {logical_idx} (ì‹¤ì œ GPU {actual_gpu}): {memory_allocated:.1f}GB í• ë‹¹, {memory_cached:.1f}GB ìºì‹œ, {memory_total:.1f}GB ì´ìš©ëŸ‰")
                except Exception as e:
                    print(f"   GPU {logical_idx} ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¡œë”© ì˜µì…˜
            load_options = {
                "torch_dtype": _DEFAULT_TORCH_DTYPE,
                "device_map": self.device_map,
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,  # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
            }
            
            print(f"ğŸ”§ ëª¨ë¸ ë¡œë”© ì˜µì…˜: device_map={self.device_map}")
            
            self.model = AutoModelForCausalLM.from_pretrained(self.model_path, **load_options)
            
            # ëª¨ë¸ì´ ë¡œë“œëœ í›„ ë””ë°”ì´ìŠ¤ í™•ì¸
            if hasattr(self, 'target_device'):
                print(f"ğŸ”§ ëª¨ë¸ íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤: {self.target_device}")
                # ë‹¨ì¼ GPU ì‚¬ìš© ì‹œ ëª…ì‹œì  ì´ë™ (ì•ˆì „ì„± í™•ë³´)
                if gpus and len(gpus) == 1:
                    try:
                        self.model = self.model.to(self.target_device)
                        print(f"âœ… ëª¨ë¸ì„ {self.target_device}ë¡œ ëª…ì‹œì  ì´ë™ ì™„ë£Œ")
                    except Exception as e:
                        print(f"âš ï¸ ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì´ë™ ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # í´ë°±: ë” ë³´ìˆ˜ì ì¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
            print("ğŸ”„ ë³´ìˆ˜ì ì¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„...")
            try:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=_FALLBACK_TORCH_DTYPE,  # bfloat16 ëŒ€ì‹  float16
                    device_map="cpu",  # ì¼ë‹¨ CPUë¡œ ë¡œë“œ
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                # ë‚˜ì¤‘ì— ì²« ë²ˆì§¸ GPUë¡œ ì´ë™
                if torch.cuda.is_available() and gpus:
                    self.target_device = f"cuda:{gpus[0]}"
                    self.model = self.model.to(self.target_device)
                    print(f"ğŸ”§ ëª¨ë¸ì„ {self.target_device}ë¡œ ì´ë™ ì™„ë£Œ")
                else:
                    self.target_device = "cpu"
            except Exception as e2:
                raise RuntimeError(f"ëª¨ë¸ ë¡œë”© ì™„ì „ ì‹¤íŒ¨: {e2}")
        
        # pad_token ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # default_params ì„¤ì •
        self.default_params = {
            "temperature": _LLM_TEMPERATURE,
            "max_new_tokens": _LLM_MAX_TOKENS,
            "repetition_penalty": _LLM_REPETITION_PENALTY,
            "top_p": _LLM_TOP_P,
            "top_k": _LLM_TOP_K,
            "no_repeat_ngram_size": _LLM_NO_REPEAT_NGRAM_SIZE,
        }
            
        print(f"âœ… ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({self.target_device})")

    def call(self, messages: List[Dict], **kwargs) -> str:
        params = self.default_params.copy()
        params.update(kwargs)
        
        # --- âœ¨ ì¶”ë¡  ê¸°ëŠ¥ í† ê¸€ ì¶”ê°€ âœ¨ ---
        # 1. 'enable_thinking' ì¸ìë¥¼ kwargsì—ì„œ ì¶”ì¶œí•©ë‹ˆë‹¤. (ê¸°ë³¸ê°’: False)
        enable_thinking = params.pop('enable_thinking', False)
        
        # 2. apply_chat_templateì— ì „ë‹¬í•  ì¸ìë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
        chat_template_kwargs = {
            "tokenize": False,
            "add_generation_prompt": True
        }
        
        # 3. í† ê¸€ì´ Trueì¼ ê²½ìš°ì—ë§Œ 'enable_thinking' ì¸ìë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        if enable_thinking:
            chat_template_kwargs['enable_thinking'] = True
            print("ğŸ’¡ ì¶”ë¡ (Thinking) ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        prompt = self.tokenizer.apply_chat_template(messages, **chat_template_kwargs)
        # --- âœ¨ ì¶”ë¡  ê¸°ëŠ¥ í† ê¸€ ë âœ¨ ---
        
        # ì…ë ¥ì„ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ë³´ë‚´ê¸°
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.target_device)
        
        try:
            with torch.no_grad():
                # ğŸ”§ model.generateì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„° í•„í„°ë§
                generate_kwargs = {
                    "input_ids": inputs.input_ids,
                    "attention_mask": inputs.attention_mask,
                    "temperature": params['temperature'],
                    "max_new_tokens": params['max_new_tokens'],
                    "repetition_penalty": params['repetition_penalty'],
                    "top_p": params['top_p'],
                    "top_k": params['top_k'],
                    "no_repeat_ngram_size": params['no_repeat_ngram_size'],
                    "pad_token_id": self.tokenizer.eos_token_id,
                    "do_sample": True,  # temperature > 0ì¼ ë•Œ í•„ìš”
                }
                
                # token_type_idsê°€ ìˆìœ¼ë©´ì„œ ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
                if hasattr(inputs, 'token_type_ids') and inputs.token_type_ids is not None:
                    # ëª¨ë¸ì´ token_type_idsë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸
                    if 'token_type_ids' in self.model.forward.__code__.co_varnames:
                        generate_kwargs['token_type_ids'] = inputs.token_type_ids
                    # ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì˜ ê²½ìš° token_type_idsëŠ” ì œì™¸
                
                outputs = self.model.generate(**generate_kwargs)
            response_text = self.tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
            return response_text.strip()
        except Exception as e:
            print(f"âŒ ë¡œì»¬ ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

# Factory function to create appropriate client
def create_model_client(
    client_type: str,
    model_name: str,
    **kwargs
) -> BaseModelClient:
    """
    Factory function to create appropriate model client.
    
    Args:
        client_type: "openai" or "local"
        model_name: Name of the model
        **kwargs: Additional arguments for specific clients
    
    Returns:
        BaseModelClient instance
    """
    if client_type.lower() == "openai":
        return OpenAIModelClient(model_name=model_name, **kwargs)
    elif client_type.lower() == "local":
        return LocalModelClient(model_name=model_name, **kwargs)
    else:
        raise ValueError(f"Unknown client type: {client_type}. Supported: 'openai', 'local'")

# Utility function to list available local models
def list_local_models() -> List[str]:
    """
    List available models in ~/models/ directory.
    
    Returns:
        List of model names
    """
    models_dir = _LOCAL_MODELS_DIR
    
    if not os.path.exists(models_dir):
        return []
    
    try:
        models = [d for d in os.listdir(models_dir) 
                 if os.path.isdir(os.path.join(models_dir, d))]
        return sorted(models)
    except Exception as e:
        print(f"Error listing local models: {e}")
        return []

# Utility function to create client from settings
def create_client_from_settings() -> BaseModelClient:
    """
    Create model client based on settings.yaml configuration.
    
    Returns:
        BaseModelClient instance configured from settings
    """
    client_type = _LLM_CFG.get('client_type', 'openai').lower()
    
    if client_type == "openai":
        external_services = _CFG.get('external_services', {})
        openai_config = external_services.get('openai', {})
        model_name = openai_config.get('model', _DEFAULT_OPENAI_MODEL)
        return OpenAIModelClient(model_name=model_name)
    elif client_type == "local":
        # ë¡œì»¬ ëª¨ë¸ ì„¤ì • - ê¸°ë³¸ llm ì„¤ì •ì„ ì‚¬ìš©í•˜ë˜ local_modelì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
        local_config = _LLM_CFG.get('local_model', {})
        return LocalModelClient(
            model_name=local_config.get('name', _DEFAULT_LOCAL_MODEL),
            device=_LLM_CFG.get('device', _DEFAULT_DEVICE)  # llm.device ì„¤ì • ì‚¬ìš©
        )
    else:
        raise ValueError(f"Unknown client type in settings: {client_type}. Supported: 'openai', 'local'")

# Quick test function
def test_client_from_settings():
    """Test client creation from settings"""
    try:
        print("ğŸ”„ Creating client from settings...")
        client = create_client_from_settings()
        
        test_messages = [
            {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”."}
        ]
        
        print("ğŸ¤– Testing client...")
        response = client.call(test_messages)
        print(f"âœ… Response: {response[:100]}...")
        
    except Exception as e:
        print(f"âŒ Error testing client: {e}")
