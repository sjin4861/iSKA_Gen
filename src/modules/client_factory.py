# src/modules/client_factory.py
"""
λ¨λΈ ν΄λΌμ΄μ–ΈνΈ ν©ν† λ¦¬ - ν΄λΌμ΄μ–ΈνΈ μ ν•μ— λ”°λΌ μ μ ν• ν΄λΌμ΄μ–ΈνΈλ¥Ό μƒμ„±ν•©λ‹λ‹¤.
"""

from pathlib import Path
import sys
import os

# κ²½λ΅ μ„¤μ •
sys.path.append(str(Path.cwd().parent.parent))

from modules.model_client import BaseModelClient, OpenAIModelClient, LocalModelClient
from src.utils.settings_loader import get_settings
from typing import Optional, Dict, Any, List

class ModelClientFactory:
    """λ¨λΈ ν΄λΌμ΄μ–ΈνΈλ¥Ό μƒμ„±ν•κ³  κ΄€λ¦¬ν•λ” ν©ν† λ¦¬ ν΄λμ¤"""
    
    @staticmethod
    def create_model_client(client_type: str, model_name: str, **kwargs) -> BaseModelClient:
        """
        ν΄λΌμ΄μ–ΈνΈ μ ν•μ— λ”°λΌ μ μ ν• λ¨λΈ ν΄λΌμ΄μ–ΈνΈλ¥Ό μƒμ„±ν•©λ‹λ‹¤.
        
        Args:
            client_type: ν΄λΌμ΄μ–ΈνΈ μ ν• ("openai", "local")
            model_name: λ¨λΈ μ΄λ¦„
            **kwargs: μ¶”κ°€ μ„¤μ •
        
        Returns:
            BaseModelClient: μƒμ„±λ ν΄λΌμ΄μ–ΈνΈ
        
        Raises:
            ValueError: μ§€μ›ν•μ§€ μ•λ” ν΄λΌμ΄μ–ΈνΈ μ ν•
        """
        client_type = client_type.lower()
        
        if client_type == "openai":
            api_key = kwargs.get('api_key') or os.getenv('OPENAI_API_KEY')
            return OpenAIModelClient(
                model_name=model_name,
                api_key=api_key,
                **{k: v for k, v in kwargs.items() if k != 'api_key'}
            )
        
        elif client_type == "local":
            return LocalModelClient(
                model_name=model_name,
                **kwargs
            )
        
        else:
            raise ValueError(f"μ§€μ›ν•μ§€ μ•λ” ν΄λΌμ΄μ–ΈνΈ μ ν•: {client_type}. μ‚¬μ© κ°€λ¥ν• μ ν•: openai, local")

    @staticmethod
    def get_available_client_types() -> Dict[str, str]:
        """μ‚¬μ© κ°€λ¥ν• ν΄λΌμ΄μ–ΈνΈ μ ν•λ“¤μ„ λ°ν™ν•©λ‹λ‹¤."""
        return {
            "openai": "OpenAI API ν΄λΌμ΄μ–ΈνΈ",
            "local": "λ΅μ»¬ λ¨λΈ ν΄λΌμ΄μ–ΈνΈ"
        }

    @staticmethod
    def get_default_model_for_client(client_type: str) -> str:
        """ν΄λΌμ΄μ–ΈνΈ μ ν•λ³„ κΈ°λ³Έ λ¨λΈμ„ λ°ν™ν•©λ‹λ‹¤."""
        defaults = {
            "openai": "gpt-4o-mini",
            "local": "Qwen3-8B"
        }
        return defaults.get(client_type.lower(), "")

    @staticmethod
    def create_default_client() -> BaseModelClient:
        """μ„¤μ • νμΌ κΈ°λ°μΌλ΅ κΈ°λ³Έ ν΄λΌμ΄μ–ΈνΈλ¥Ό μƒμ„±ν•©λ‹λ‹¤."""
        cfg = get_settings()
        llm_cfg = cfg.get('llm', {})
        
        # κΈ°λ³Έ ν΄λΌμ΄μ–ΈνΈ μ ν• κ²°μ •
        default_client_type = llm_cfg.get('default_client', 'local')
        default_model = llm_cfg.get('model') or ModelClientFactory.get_default_model_for_client(default_client_type)
        
        return ModelClientFactory.create_model_client(
            client_type=default_client_type,
            model_name=default_model
        )

    @staticmethod
    def validate_client_config(client_type: str, model_name: str, **kwargs) -> Dict[str, Any]:
        """
        ν΄λΌμ΄μ–ΈνΈ μ„¤μ •μ„ κ²€μ¦ν•κ³  λ¬Έμ κ°€ μμΌλ©΄ μ¤λ¥ μ •λ³΄λ¥Ό λ°ν™ν•©λ‹λ‹¤.
        
        Returns:
            Dict[str, Any]: {"valid": bool, "errors": List[str], "warnings": List[str]}
        """
        result = {"valid": True, "errors": [], "warnings": []}
        
        client_type = client_type.lower()
        
        # ν΄λΌμ΄μ–ΈνΈ μ ν• κ²€μ¦
        if client_type not in ModelClientFactory.get_available_client_types():
            result["valid"] = False
            result["errors"].append(f"μ§€μ›ν•μ§€ μ•λ” ν΄λΌμ΄μ–ΈνΈ μ ν•: {client_type}")
            return result
        
        # λ¨λΈ μ΄λ¦„ κ²€μ¦
        if not model_name or not model_name.strip():
            result["valid"] = False
            result["errors"].append("λ¨λΈ μ΄λ¦„μ΄ ν•„μ”ν•©λ‹λ‹¤")
            return result
        
        # OpenAI νΉλ³„ κ²€μ¦
        if client_type == "openai":
            api_key = kwargs.get('api_key') or os.getenv('OPENAI_API_KEY')
            if not api_key:
                result["valid"] = False
                result["errors"].append("OpenAI API ν‚¤κ°€ ν•„μ”ν•©λ‹λ‹¤")
            
            # OpenAI λ¨λΈλ… κ²€μ¦
            valid_openai_models = [
                "gpt-4o", "gpt-4o-mini"
            ]
            if model_name not in valid_openai_models:
                result["warnings"].append(f"μΌλ°μ μ΄μ§€ μ•μ€ OpenAI λ¨λΈλ…: {model_name}")
        
        # Local νΉλ³„ κ²€μ¦
        elif client_type == "local":
            models_dir = os.getenv('LOCAL_MODELS_PATH')
            if not models_dir:
                result["warnings"].append("LOCAL_MODELS_PATH ν™κ²½λ³€μκ°€ μ„¤μ •λμ§€ μ•μ. κΈ°λ³Έκ°’ μ‚¬μ©: ~/models")
            
            # GPU μ„¤μ • κ²€μ¦
            gpus = kwargs.get('gpus')
            if gpus is not None:
                import torch
                if not torch.cuda.is_available():
                    result["warnings"].append("CUDAκ°€ μ‚¬μ© λ¶κ°€λ¥ν•μ§€λ§ GPU μ„¤μ •μ΄ μ§€μ •λ¨")
                elif isinstance(gpus, list):
                    available_gpus = torch.cuda.device_count()
                    invalid_gpus = [gpu for gpu in gpus if gpu >= available_gpus or gpu < 0]
                    if invalid_gpus:
                        result["errors"].append(f"μ ν¨ν•μ§€ μ•μ€ GPU μΈλ±μ¤: {invalid_gpus}. μ‚¬μ© κ°€λ¥ν• GPU: 0~{available_gpus-1}")
                else:
                    result["errors"].append("gpus νλΌλ―Έν„°λ” μ •μ λ¦¬μ¤νΈμ—¬μ•Ό ν•©λ‹λ‹¤ (μ: [0, 1])")
        
        return result

    # νΈμ ν•¨μλ“¤
    @staticmethod
    def create_openai_client(model_name: str = "gpt-4o-mini", api_key: Optional[str] = None, **kwargs) -> OpenAIModelClient:
        """OpenAI ν΄λΌμ΄μ–ΈνΈ μƒμ„± νΈμ ν•¨μ"""
        return ModelClientFactory.create_model_client("openai", model_name, api_key=api_key, **kwargs)

    @staticmethod
    def create_local_client(model_name: str = "Qwen3-8B", gpus: Optional[List[int]] = None, **kwargs) -> LocalModelClient:
        """
        λ΅μ»¬ ν΄λΌμ΄μ–ΈνΈ μƒμ„± νΈμ ν•¨μ
        
        Args:
            model_name: λ¨λΈ μ΄λ¦„
            gpus: μ‚¬μ©ν•  GPU λ¦¬μ¤νΈ (μ: [2, 3])
            **kwargs: μ¶”κ°€ μ„¤μ •
        """
        if gpus is not None:
            kwargs['gpus'] = gpus
        return ModelClientFactory.create_model_client("local", model_name, **kwargs)

    # GPU μ ν‹Έλ¦¬ν‹° ν•¨μλ“¤
    @staticmethod
    def get_available_gpus() -> List[int]:
        """μ‚¬μ© κ°€λ¥ν• GPU μΈλ±μ¤ λ¦¬μ¤νΈλ¥Ό λ°ν™ν•©λ‹λ‹¤."""
        try:
            import torch
            if torch.cuda.is_available():
                return list(range(torch.cuda.device_count()))
            return []
        except ImportError:
            return []

    @staticmethod
    def get_gpu_memory_info(gpu_id: int) -> Dict[str, float]:
        """νΉμ • GPUμ λ©”λ¨λ¦¬ μ •λ³΄λ¥Ό λ°ν™ν•©λ‹λ‹¤ (GB λ‹¨μ„)."""
        try:
            import torch
            if torch.cuda.is_available() and gpu_id < torch.cuda.device_count():
                allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3
                cached = torch.cuda.memory_reserved(gpu_id) / 1024**3
                total = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3
                return {
                    "allocated": allocated,
                    "cached": cached,
                    "total": total,
                    "free": total - allocated
                }
            return {}
        except ImportError:
            return {}

    @staticmethod
    def suggest_optimal_gpus(required_memory_gb: float = 8.0) -> List[int]:
        """
        ν•„μ”ν• λ©”λ¨λ¦¬ μ–‘μ„ κΈ°μ¤€μΌλ΅ μµμ μ GPUλ“¤μ„ μ¶”μ²ν•©λ‹λ‹¤.
        
        Args:
            required_memory_gb: ν•„μ”ν• λ©”λ¨λ¦¬ μ–‘ (GB)
        
        Returns:
            μ¶”μ² GPU μΈλ±μ¤ λ¦¬μ¤νΈ
        """
        available_gpus = ModelClientFactory.get_available_gpus()
        if not available_gpus:
            return []
        
        # κ° GPUμ μ‚¬μ© κ°€λ¥ν• λ©”λ¨λ¦¬ κ³„μ‚°
        gpu_scores = []
        for gpu_id in available_gpus:
            memory_info = ModelClientFactory.get_gpu_memory_info(gpu_id)
            if memory_info:
                free_memory = memory_info["free"]
                if free_memory >= required_memory_gb:
                    # μ—¬μ  λ©”λ¨λ¦¬κ°€ λ§μ„μλ΅ λ†’μ€ μ μ
                    score = free_memory
                    gpu_scores.append((gpu_id, score))
        
        # λ©”λ¨λ¦¬ μ—¬μ  κ³µκ°„ κΈ°μ¤€μΌλ΅ μ •λ ¬
        gpu_scores.sort(key=lambda x: x[1], reverse=True)
        
        # μƒμ„ GPUλ“¤ λ°ν™ (μµλ€ 2κ°)
        return [gpu_id for gpu_id, _ in gpu_scores[:2]]

    @staticmethod
    def create_local_client_auto_gpu(model_name: str, **kwargs) -> LocalModelClient:
        """
        GPUλ¥Ό μλ™μΌλ΅ μ„ νƒν•μ—¬ λ΅μ»¬ ν΄λΌμ΄μ–ΈνΈλ¥Ό μƒμ„±ν•©λ‹λ‹¤.
        
        Args:
            model_name: λ¨λΈ μ΄λ¦„
            **kwargs: μ¶”κ°€ μ„¤μ •
        """
        # λ¨λΈ ν¬κΈ°μ— λ”°λ¥Έ λ©”λ¨λ¦¬ μ”κµ¬λ‰ μ¶”μ •
        memory_requirements = {
            "3B": 6.0,
            "8B": 12.0,
            "32B": 48.0,
            "30B": 45.0
        }
        
        required_memory = 8.0  # κΈ°λ³Έκ°’
        for size, memory in memory_requirements.items():
            if size in model_name:
                required_memory = memory
                break
        
        # μµμ  GPU μ¶”μ²
        suggested_gpus = ModelClientFactory.suggest_optimal_gpus(required_memory)
        
        if suggested_gpus:
            print(f"π― λ¨λΈ {model_name}μ— λ€ν•΄ GPU {suggested_gpus} μ¶”μ² (μμƒ λ©”λ¨λ¦¬: {required_memory}GB)")
            kwargs['gpus'] = suggested_gpus
        else:
            print("β οΈ μ μ ν• GPUλ¥Ό μ°Ύμ„ μ μ—†μ–΄ auto λ¨λ“λ΅ μ„¤μ •ν•©λ‹λ‹¤.")
        
        return ModelClientFactory.create_model_client("local", model_name, **kwargs)


# ν•μ„ νΈν™μ„±μ„ μ„ν• ν•¨μλ“¤ (κΈ°μ΅΄ μ½”λ“μ—μ„ μ‚¬μ©ν•λ ν•¨μλ… μ μ§€)
def create_model_client(client_type: str, model_name: str, **kwargs) -> BaseModelClient:
    """ν•μ„ νΈν™μ„±μ„ μ„ν• λνΌ ν•¨μ"""
    return ModelClientFactory.create_model_client(client_type, model_name, **kwargs)

def get_available_client_types() -> Dict[str, str]:
    """ν•μ„ νΈν™μ„±μ„ μ„ν• λνΌ ν•¨μ"""
    return ModelClientFactory.get_available_client_types()

def get_default_model_for_client(client_type: str) -> str:
    """ν•μ„ νΈν™μ„±μ„ μ„ν• λνΌ ν•¨μ"""
    return ModelClientFactory.get_default_model_for_client(client_type)

def create_default_client() -> BaseModelClient:
    """ν•μ„ νΈν™μ„±μ„ μ„ν• λνΌ ν•¨μ"""
    return ModelClientFactory.create_default_client()

def validate_client_config(client_type: str, model_name: str, **kwargs) -> Dict[str, Any]:
    """ν•μ„ νΈν™μ„±μ„ μ„ν• λνΌ ν•¨μ"""
    return ModelClientFactory.validate_client_config(client_type, model_name, **kwargs)

def create_openai_client(model_name: str = "gpt-4o-mini", api_key: Optional[str] = None, **kwargs) -> OpenAIModelClient:
    """ν•μ„ νΈν™μ„±μ„ μ„ν• λνΌ ν•¨μ"""
    return ModelClientFactory.create_openai_client(model_name, api_key, **kwargs)

def create_local_client(model_name: str = "Qwen3-8B", gpus: Optional[List[int]] = None, **kwargs) -> LocalModelClient:
    """ν•μ„ νΈν™μ„±μ„ μ„ν• λνΌ ν•¨μ"""
    return ModelClientFactory.create_local_client(model_name, gpus, **kwargs)

def get_available_gpus() -> List[int]:
    """ν•μ„ νΈν™μ„±μ„ μ„ν• λνΌ ν•¨μ"""
    return ModelClientFactory.get_available_gpus()

def get_gpu_memory_info(gpu_id: int) -> Dict[str, float]:
    """ν•μ„ νΈν™μ„±μ„ μ„ν• λνΌ ν•¨μ"""
    return ModelClientFactory.get_gpu_memory_info(gpu_id)

def suggest_optimal_gpus(required_memory_gb: float = 8.0) -> List[int]:
    """ν•μ„ νΈν™μ„±μ„ μ„ν• λνΌ ν•¨μ"""
    return ModelClientFactory.suggest_optimal_gpus(required_memory_gb)

def create_local_client_auto_gpu(model_name: str, **kwargs) -> LocalModelClient:
    """ν•μ„ νΈν™μ„±μ„ μ„ν• λνΌ ν•¨μ"""
    return ModelClientFactory.create_local_client_auto_gpu(model_name, **kwargs)
