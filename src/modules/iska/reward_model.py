import sys
import torch
import random
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from transformers import AutoTokenizer, AutoModelForCausalLM

try:
    from trl import AutoModelForCausalLMWithValueHead
    TRL_AVAILABLE = True
except ImportError:
    TRL_AVAILABLE = False
    print("âš ï¸ TRL ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¼ë°˜ ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")

try:
    from peft import AutoPeftModelForCausalLM, PeftModel
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    print("âš ï¸ PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PEFT ëª¨ë¸ ì§€ì›ì´ ì œí•œë©ë‹ˆë‹¤.")


class RewardModel:
    """
    Reward Modelì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, model_path: str, device: str = "auto", seed: int = 42, gpus: Optional[List[int]] = None):
        """
        Reward Model ì´ˆê¸°í™”
        
        Args:
            model_path (str): ëª¨ë¸ ê²½ë¡œ
            device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ("auto", "cuda", "cpu")
            seed (int): ëœë¤ ì‹œë“œ
            gpus (Optional[List[int]]): ì‚¬ìš©í•  GPU ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [0, 1])
        """
        # ëª¨ë¸ ê²½ë¡œ í‹¸ë“œ í™•ì¥
        self.model_path = str(Path(model_path).expanduser())
        self.seed = seed
        self.gpus = gpus
        self.set_seed(seed)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ğŸ¯ Reward Model ì´ˆê¸°í™” ì¤‘...")
        print(f"   ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   GPU ë¦¬ìŠ¤íŠ¸: {self.gpus}")
        print(f"   ì‹œë“œ: {seed}")
        
        # GPU ì„¤ì •
        if self.gpus is not None and torch.cuda.is_available():
            if isinstance(self.gpus, list) and all(isinstance(i, int) for i in self.gpus):
                # GPU í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                import os
                os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, self.gpus))
                print(f"ğŸ¯ CUDA_VISIBLE_DEVICES ì„¤ì •: {','.join(map(str, self.gpus))}")
                torch.cuda.empty_cache()
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        self._load_model()
    
    def set_seed(self, seed: int):
        """Deterministic ì„¤ì •ì„ ìœ„í•œ seed ê³ ì •"""
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
        random.seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    def _load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            print(f"ğŸ“¥ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
            
            # PEFT ëª¨ë¸ì¸ì§€ í™•ì¸
            adapter_config_path = Path(self.model_path) / "adapter_config.json"
            is_peft_model = adapter_config_path.exists()
            
            if is_peft_model and PEFT_AVAILABLE:
                print(f"ğŸ”§ PEFT ì–´ëŒ‘í„° ëª¨ë¸ ê°ì§€ë¨")
                try:
                    # PEFT ëª¨ë¸ë¡œ ë¡œë“œ
                    self.model = AutoPeftModelForCausalLM.from_pretrained(
                        self.model_path,
                        torch_dtype=torch.float16,
                        device_map=self._get_device_map()
                    )
                    self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
                    self.model_type = "peft"
                    print("âœ… PEFT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    print(f"âš ï¸ PEFT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                    print("ï¿½ ì¼ë°˜ ëª¨ë¸ë¡œ fallback ì‹œë„...")
                    self._load_standard_model_fallback()
            else:
                # ì¼ë°˜ ëª¨ë¸ ë¡œë”©
                print(f"ğŸ”§ ì¼ë°˜ ëª¨ë¸ë¡œ ë¡œë”© ì¤‘...")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
                self._load_standard_model_with_device_map()
            
            self.model.eval()
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def _get_device_map(self):
        """ë””ë°”ì´ìŠ¤ ë§¤í•‘ ì„¤ì • ë°˜í™˜"""
        if self.gpus is not None and torch.cuda.is_available():
            if len(self.gpus) == 1:
                return {"": "cuda:0"}  # ë‹¨ì¼ GPU
            else:
                return "auto"  # ë‹¤ì¤‘ GPU
        elif self.device.type == "cuda":
            return "auto"
        else:
            return None  # CPU
    
    def _load_standard_model_with_device_map(self):
        """ì¼ë°˜ ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ ë§¤í•‘ê³¼ í•¨ê»˜ ë¡œë“œ"""
        device_map = self._get_device_map()
        
        # TRLì´ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ Value Headê°€ ìˆëŠ” ëª¨ë¸ ì‹œë„
        if TRL_AVAILABLE:
            try:
                self.model = AutoModelForCausalLMWithValueHead.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float16,
                    device_map=device_map
                )
                self.model_type = "value_head"
                print("âœ… Value Headê°€ ìˆëŠ” Reward Model ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ Value Head ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ì¼ë°˜ ëª¨ë¸ë¡œ ì‹œë„: {e}")
                self._load_standard_model(device_map)
        else:
            self._load_standard_model(device_map)
    
    def _load_standard_model_fallback(self):
        """PEFT ì‹¤íŒ¨ ì‹œ fallback ëª¨ë¸ ë¡œë”©"""
        try:
            import json
            adapter_config_path = Path(self.model_path) / "adapter_config.json"
            with open(adapter_config_path, 'r') as f:
                adapter_config = json.load(f)
            
            base_model_path = adapter_config.get("base_model_name_or_path", "Qwen/Qwen3-4B")
            print(f"ğŸ”„ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ fallback: {base_model_path}")
            
            # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.float16,
                device_map=self._get_device_map()
            )
            self.model_type = "fallback_base"
            print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ë¡œ fallback ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ Fallbackë„ ì‹¤íŒ¨: {e}")
            raise
    
    def _load_standard_model(self, device_map=None):
        """ì¼ë°˜ CausalLM ëª¨ë¸ ë¡œë”©"""
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map=device_map
        )
        self.model_type = "standard"
        print("âœ… ì¼ë°˜ CausalLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ (Logitì„ ì ìˆ˜ë¡œ ì‚¬ìš©)")
    
    def evaluate_text(self, text: str, max_length: int = 512) -> float:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€
        
        Args:
            text (str): í‰ê°€í•  í…ìŠ¤íŠ¸
            max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
            
        Returns:
            float: í’ˆì§ˆ ì ìˆ˜
        """
        # ì…ë ¥ í† í°í™”
        inputs = self.tokenizer(
            text, 
            return_tensors='pt', 
            truncation=True, 
            max_length=max_length,
            padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            # ì‹œë“œ ì¬ì„¤ì • (ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´)
            torch.manual_seed(self.seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(self.seed)
            
            outputs = self.model(**inputs)
            
            # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì ìˆ˜ ì¶”ì¶œ
            if self.model_type == "value_head":
                return self._extract_value_score(outputs)
            else:
                return self._extract_logit_score(outputs)
    
    def _extract_value_score(self, outputs) -> float:
        """Value Headì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
        values = None
        
        # ë‹¤ì–‘í•œ ì¶œë ¥ í˜•íƒœ ì²˜ë¦¬
        if isinstance(outputs, tuple) and len(outputs) >= 3:
            values = outputs[2]  # (logits, past_key_values, values)
        elif hasattr(outputs, 'value'):
            values = outputs.value
        elif hasattr(outputs, 'values'):
            values = outputs.values
        
        if values is not None:
            # ë§ˆì§€ë§‰ í† í°ì˜ reward ì ìˆ˜ ì¶”ì¶œ
            reward_score = values[0, -1].item()
            return reward_score
        else:
            print("âš ï¸ Valueë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ Logitì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._extract_logit_score(outputs)
    
    def _extract_logit_score(self, outputs) -> float:
        """ì¼ë°˜ ëª¨ë¸ì—ì„œ Logit ê¸°ë°˜ ì ìˆ˜ ì¶”ì¶œ"""
        if hasattr(outputs, 'logits'):
            # ë§ˆì§€ë§‰ í† í°ì˜ ëª¨ë“  logit ì¤‘ í‰ê· ê°’ì„ ì ìˆ˜ë¡œ ì‚¬ìš©
            last_token_logits = outputs.logits[0, -1, :]
            reward_score = last_token_logits.mean().item()
            return reward_score
        else:
            print(f"âŒ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì¶œë ¥ êµ¬ì¡°: {type(outputs)}")
            return 0.0
    
    def evaluate_multiple(self, texts: List[str], max_length: int = 512) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ì¼ê´„ í‰ê°€
        
        Args:
            texts (List[str]): í‰ê°€í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
            
        Returns:
            List[Dict[str, Any]]: í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for i, text in enumerate(texts):
            print(f"ğŸ“Š í…ìŠ¤íŠ¸ {i+1}/{len(texts)} í‰ê°€ ì¤‘...")
            
            score = self.evaluate_text(text, max_length)
            
            result = {
                "text": text,
                "score": score,
                "index": i
            }
            results.append(result)
        
        return results
    
    def compare_texts(self, chosen_text: str, rejected_text: str, max_length: int = 512) -> Dict[str, Any]:
        """
        ë‘ í…ìŠ¤íŠ¸ë¥¼ ë¹„êµí•˜ì—¬ ì–´ëŠ ê²ƒì´ ë” ì¢‹ì€ì§€ íŒë‹¨
        
        Args:
            chosen_text (str): ì„ íƒëœ(ì¢‹ì€) í…ìŠ¤íŠ¸
            rejected_text (str): ê±°ë¶€ëœ(ë‚˜ìœ) í…ìŠ¤íŠ¸
            max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
            
        Returns:
            Dict[str, Any]: ë¹„êµ ê²°ê³¼
        """
        chosen_score = self.evaluate_text(chosen_text, max_length)
        rejected_score = self.evaluate_text(rejected_text, max_length)
        
        return {
            "chosen_text": chosen_text,
            "rejected_text": rejected_text,
            "chosen_score": chosen_score,
            "rejected_score": rejected_score,
            "score_difference": chosen_score - rejected_score,
            "correct_preference": chosen_score > rejected_score
        }
    
    def batch_compare(self, pairs: List[Dict[str, str]], max_length: int = 512) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ìŒì„ ì¼ê´„ ë¹„êµ
        
        Args:
            pairs (List[Dict[str, str]]): [{"chosen": str, "rejected": str}, ...] í˜•íƒœì˜ ìŒ ë¦¬ìŠ¤íŠ¸
            max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
            
        Returns:
            Dict[str, Any]: ì¼ê´„ ë¹„êµ ê²°ê³¼
        """
        results = []
        correct_count = 0
        
        for i, pair in enumerate(pairs):
            print(f"ğŸ”„ ìŒ {i+1}/{len(pairs)} ë¹„êµ ì¤‘...")
            
            comparison = self.compare_texts(
                pair["chosen"], 
                pair["rejected"], 
                max_length
            )
            
            results.append(comparison)
            if comparison["correct_preference"]:
                correct_count += 1
        
        accuracy = correct_count / len(pairs) if pairs else 0
        
        return {
            "comparisons": results,
            "total_pairs": len(pairs),
            "correct_preferences": correct_count,
            "accuracy": accuracy,
            "model_info": {
                "model_path": self.model_path,
                "model_type": self.model_type,
                "device": str(self.device)
            }
        }


def evaluate_passages_with_reward_model(
    model_path: str,
    passages: List[Dict[str, Any]],
    device: str = "auto",
    max_length: int = 512,
    seed: int = 42,
    gpus: Optional[List[int]] = None
) -> List[Dict[str, Any]]:
    """
    Reward Modelì„ ì‚¬ìš©í•˜ì—¬ passageë“¤ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        model_path (str): Reward Model ê²½ë¡œ
        passages (List[Dict[str, Any]]): í‰ê°€í•  passage ë¦¬ìŠ¤íŠ¸
        device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
        seed (int): ëœë¤ ì‹œë“œ
        gpus (Optional[List[int]]): ì‚¬ìš©í•  GPU ë¦¬ìŠ¤íŠ¸
        
    Returns:
        List[Dict[str, Any]]: í‰ê°€ ê²°ê³¼ê°€ ì¶”ê°€ëœ passage ë¦¬ìŠ¤íŠ¸
    """
    print(f"ğŸ¯ Reward Modelì„ ì‚¬ìš©í•œ Passage í‰ê°€ ì‹œì‘...")
    print(f"   í‰ê°€í•  passage ê°œìˆ˜: {len(passages)}")
    
    # Reward Model ì´ˆê¸°í™”
    reward_model = RewardModel(model_path, device, seed, gpus)
    
    # í‰ê°€ ì‹¤í–‰
    evaluated_passages = []
    
    for i, passage_data in enumerate(passages):
        print(f"ğŸ“Š Passage {i+1}/{len(passages)} í‰ê°€ ì¤‘...")
        
        passage_text = passage_data.get("generated_passage", "")
        if not passage_text:
            print(f"âš ï¸ Passage {i+1}: ë¹ˆ í…ìŠ¤íŠ¸, ê±´ë„ˆëœ€")
            continue
        
        # ì ìˆ˜ ê³„ì‚°
        score = reward_model.evaluate_text(passage_text, max_length)
        
        # ê²°ê³¼ ì¶”ê°€
        evaluated_passage = passage_data.copy()
        evaluated_passage["reward_score"] = score
        evaluated_passage["evaluation_model"] = model_path
        
        evaluated_passages.append(evaluated_passage)
        
        print(f"   ì ìˆ˜: {score:.4f}")
    
    print(f"âœ… Reward Model í‰ê°€ ì™„ë£Œ!")
    return evaluated_passages


def compare_passage_pairs_with_reward_model(
    model_path: str,
    passage_pairs: List[Dict[str, Any]],
    device: str = "auto",
    max_length: int = 512,
    seed: int = 42,
    gpus: Optional[List[int]] = None
) -> Dict[str, Any]:
    """
    Reward Modelì„ ì‚¬ìš©í•˜ì—¬ passage pairë“¤ì„ ë¹„êµ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        model_path (str): Reward Model ê²½ë¡œ
        passage_pairs (List[Dict[str, Any]]): ë¹„êµí•  passage pair ë¦¬ìŠ¤íŠ¸
        device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
        seed (int): ëœë¤ ì‹œë“œ
        gpus (Optional[List[int]]): ì‚¬ìš©í•  GPU ë¦¬ìŠ¤íŠ¸
        
    Returns:
        Dict[str, Any]: ë¹„êµ í‰ê°€ ê²°ê³¼
    """
    print(f"ğŸ¯ Reward Modelì„ ì‚¬ìš©í•œ Passage Pair ë¹„êµ ì‹œì‘...")
    print(f"   ë¹„êµí•  pair ê°œìˆ˜: {len(passage_pairs)}")
    
    # Reward Model ì´ˆê¸°í™”
    reward_model = RewardModel(model_path, device, seed, gpus)
    
    # ë¹„êµí•  pair ë°ì´í„° ì¤€ë¹„
    comparison_pairs = []
    for pair_data in passage_pairs:
        chosen = pair_data.get("chosen", "")
        rejected = pair_data.get("rejected", "")
        
        if chosen and rejected:
            comparison_pairs.append({
                "chosen": chosen,
                "rejected": rejected
            })
    
    # ì¼ê´„ ë¹„êµ ì‹¤í–‰
    comparison_results = reward_model.batch_compare(comparison_pairs, max_length)
    
    print(f"âœ… Reward Model ë¹„êµ ì™„ë£Œ!")
    print(f"   ì •í™•ë„: {comparison_results['accuracy']:.2%}")
    
    return comparison_results


# --- ì‹¤í–‰ ì˜ˆì‹œ ---
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ì˜ˆì œ
    MODEL_PATH = str(Path("~/models/train_2025-07-26-12-04-23").expanduser())  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_passages = [
        {
            "generated_passage": "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ì‚¬íšŒ ì „ë°˜ì— í™•ì‚°ë¨ì— ë”°ë¼, ì•Œê³ ë¦¬ì¦˜ì˜ í¸í–¥ì„±ê³¼ ë¶ˆíˆ¬ëª…ì„±ìœ¼ë¡œ ì¸í•´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ìœ¤ë¦¬ì  ë¬¸ì œì— ëŒ€í•œ ì‹¬ë„ ìˆëŠ” ë…¼ì˜ê°€ ìš”êµ¬ë©ë‹ˆë‹¤.",
            "source_item": {"korean_topic": "AI ìœ¤ë¦¬"}
        },
        {
            "generated_passage": "AIëŠ” ë˜ê²Œ ì¢‹ì§€ë§Œ ê°€ë” ì´ìƒí•´ìš”. ê·¸ë˜ì„œ ì¡°ì‹¬í•´ì•¼ í•´ìš”.",
            "source_item": {"korean_topic": "AI ìœ¤ë¦¬"}
        }
    ]
    
    test_pairs = [
        {
            "chosen": "í™˜ê²½ ë³´í˜¸ëŠ” ìš°ë¦¬ ëª¨ë‘ì˜ ì±…ì„ì…ë‹ˆë‹¤. ì§€êµ¬ ì˜¨ë‚œí™”ë¥¼ ë§‰ê¸° ìœ„í•´ ì¬ìƒ ê°€ëŠ¥í•œ ì—ë„ˆì§€ ì‚¬ìš©ì„ ëŠ˜ë¦¬ê³ , í”Œë¼ìŠ¤í‹± ì‚¬ìš©ì„ ì¤„ì—¬ì•¼ í•©ë‹ˆë‹¤.",
            "rejected": "í™˜ê²½ì€ ì¤‘ìš”í•´ìš”. ê·¸ë˜ì„œ ì¢‹ì€ ì¼ì„ í•´ì•¼ ë¼ìš”. ë‚˜ë¬´ë„ ì‹¬ê³  ê·¸ëŸ° ê±°ìš”."
        }
    ]
    
    try:
        print("=" * 50)
        print("ğŸ§ª Reward Model í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 50)
        
        # 1. ê°œë³„ passage í‰ê°€ í…ŒìŠ¤íŠ¸
        print("\n1ï¸âƒ£ ê°œë³„ Passage í‰ê°€ í…ŒìŠ¤íŠ¸:")
        evaluated = evaluate_passages_with_reward_model(
            MODEL_PATH, 
            test_passages,
            max_length=256
        )
        
        for result in evaluated:
            score = result.get("reward_score", 0)
            text = result.get("generated_passage", "")[:50] + "..."
            print(f"   {text} â†’ ì ìˆ˜: {score:.4f}")
        
        # 2. Passage pair ë¹„êµ í…ŒìŠ¤íŠ¸
        print("\n2ï¸âƒ£ Passage Pair ë¹„êµ í…ŒìŠ¤íŠ¸:")
        comparison_result = compare_passage_pairs_with_reward_model(
            MODEL_PATH,
            test_pairs,
            max_length=256
        )
        
        print(f"   ì´ {comparison_result['total_pairs']}ê°œ pair ì¤‘ {comparison_result['correct_preferences']}ê°œ ì •í™•")
        print(f"   ì •í™•ë„: {comparison_result['accuracy']:.2%}")
        
        print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
