"""
ì§€ë¬¸ í‰ê°€ ëª¨ë“ˆ (Passage Evaluator)

ìƒì„±ëœ ì§€ë¬¸ì˜ í’ˆì§ˆì„ ë‹¤ê°ë„ë¡œ í‰ê°€í•˜ëŠ” ëª¨ë“ˆ:
- ì´ì§„ ë£¨ë¸Œë¦­: 5ê°€ì§€ í•­ëª© Pass/Fail í‰ê°€ (ë¹ ë¥¸ ê²€ìˆ˜)
- ìƒì„¸ ì ìˆ˜: ì¼ê´€ì„±, ì¼ì¹˜ì„±, ìì—°ìŠ¤ëŸ¬ì›€, í•œêµ­ì–´ í’ˆì§ˆ (1-5ì )
- OpenAI ê¸°ë°˜, ì¶”í›„ Reward Model ì§€ì› ì˜ˆì •
"""

import json
import re
import sys
import os
from pathlib import Path
from typing import Dict, List, Optional

# ê²½ë¡œ ì„¤ì •
sys.path.append(str(Path.cwd().parent.parent))

# ê°„ë‹¨í•œ ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ êµ¬í˜„
def levenshtein_distance(s1: str, s2: str) -> int:
    """ê°„ë‹¨í•œ ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê³„ì‚°"""
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = list(range(len(s2) + 1))
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
project_root = os.path.dirname(parent_dir)
sys.path.insert(0, parent_dir)
sys.path.insert(0, project_root)

# ëŸ°íƒ€ì„ì— import
try:
    from ..model_client import BaseModelClient, OpenAIModelClient
    from ..client_factory import ModelClientFactory
except ImportError:
    try:
        from src.modules.model_client import BaseModelClient, OpenAIModelClient
        from src.modules.client_factory import ModelClientFactory
    except ImportError:
        from modules.model_client import BaseModelClient, OpenAIModelClient
        from modules.client_factory import ModelClientFactory

try:
    from src.utils.prompt_loader import get_prompt
    from src.utils.settings_loader import get_settings
except ImportError:
    from utils.prompt_loader import get_prompt
    from utils.settings_loader import get_settings

class PassageEvaluator:
    """
    ì§€ë¬¸ í‰ê°€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ - OpenAI ê¸°ë°˜ í‰ê°€ ì‹œìŠ¤í…œ
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ì¼ê´€ì„±, ì‚¬ì‹¤ ì¼ì¹˜ì„±, ìì—°ìŠ¤ëŸ¬ì›€, í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€
    - ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ ì •ëŸ‰ì  í‰ê°€
    - ì¶”í›„ Reward Modelë¡œ êµì²´ ê°€ëŠ¥í•œ êµ¬ì¡°
    """
    
    def __init__(self, llm_client: Optional[BaseModelClient] = None, template_key: str = "iska", **kwargs):
        """
        PassageEvaluator ì´ˆê¸°í™”
        
        Args:
            llm_client: í‰ê°€ì— ì‚¬ìš©í•  LLM í´ë¼ì´ì–¸íŠ¸ (Noneì´ë©´ ê¸°ë³¸ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±)
            template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤ (ê¸°ë³¸: "iska")
            **kwargs: í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
        """
        self.template_key = template_key
        
        if llm_client is None:
            print("ğŸ”§ ê¸°ë³¸ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            try:
                # ì„¤ì •ì—ì„œ ê¸°ë³¸ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                try:
                    settings = get_settings()
                    openai_config = settings.get('external_services', {}).get('openai', {})
                    model_name = openai_config.get('model', 'gpt-4o-mini')
                except:
                    model_name = 'gpt-4o-mini'
                
                self.llm_client = ModelClientFactory.create_openai_client(
                    model_name=model_name,
                    **kwargs
                )
                print(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ: {model_name}")
            except Exception as e:
                print(f"âš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                print("ğŸ”„ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„...")
                try:
                    self.llm_client = ModelClientFactory.create_openai_client()
                except Exception as e2:
                    print(f"âŒ ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ ìƒì„±ë„ ì‹¤íŒ¨: {e2}")
                    raise RuntimeError(f"í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        else:
            self.llm_client = llm_client
            print("âœ… ì§€ë¬¸ í‰ê°€ê¸°ê°€ ì‚¬ìš©ì ì œê³µ í´ë¼ì´ì–¸íŠ¸ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    def calculate_normalized_score(self, original_text: str, corrected_text: str) -> float:
        """
        ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ë¥¼ ì´ìš©í•´ 0~1 ì‚¬ì´ì˜ ì •ê·œí™”ëœ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì™„ë²½í•œ ë¬¸ì¥ì…ë‹ˆë‹¤.
        """
        dist = levenshtein_distance(original_text, corrected_text)
        max_len = max(len(original_text), len(corrected_text))
        if max_len == 0:
            return 1.0
        return 1.0 - (dist / max_len)

    def scale_to_1_5(self, normalized_score: float) -> int:
        """0~1 ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ 1~5ì  ì²™ë„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if normalized_score >= 0.99:
            return 5  # ê±°ì˜ ì™„ë²½
        elif normalized_score >= 0.95:
            return 4  # ì‚¬ì†Œí•œ ì˜¤ë¥˜
        elif normalized_score >= 0.90:
            return 3  # ëˆˆì— ë„ëŠ” ì˜¤ë¥˜
        elif normalized_score >= 0.80:
            return 2  # ë§ì€ ì˜¤ë¥˜
        else:
            return 1  # ì‹¬ê°í•œ ì˜¤ë¥˜

    def evaluate_completeness_for_guidelines(self, passage: str, problem_types: List[str], eval_goals: List[str]) -> int:
        """
        ì™„ì„±ë„ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤ (í‰ê°€ ì§€ì¹¨ ë¶€í•©ì„±).
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            problem_types: ë¬¸ì œ ìœ í˜• ë¦¬ìŠ¤íŠ¸
            eval_goals: í‰ê°€ ëª©í‘œ ë¦¬ìŠ¤íŠ¸  
            
        Returns:
            int: ì™„ì„±ë„ ì ìˆ˜ (1-5ì )
        """        
        try:
            prompt = get_prompt(
                'passage_eval.completeness_for_guidelines',
                agent='iska',
                problem_type1=problem_types[0], eval_goal1=eval_goals[0],
                problem_type2=problem_types[1], eval_goal2=eval_goals[1],
                problem_type3=problem_types[2], eval_goal3=eval_goals[2],
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            
            # ì ìˆ˜ íŒŒì‹±
            score = self._parse_score_from_response(response, "completeness_for_guidelines")
            
            return score if score is not None else 3
            
        except Exception as e:
            print(f"âŒ ì™„ì„±ë„ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 3

    def evaluate_clarity_of_core_theme(self, passage: str, home_topic: str, foreign_topic: str) -> int:
        """
        í•µì‹¬ì£¼ì œ ëª…í™•ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            home_topic: í•œêµ­ ì£¼ì œ
            foreign_topic: ì™¸êµ­ ì£¼ì œ
            
        Returns:
            int: í•µì‹¬ì£¼ì œ ëª…í™•ì„± ì ìˆ˜ (1-5ì )
        """        
        try:
            prompt = get_prompt(
                'passage_eval.clarity_of_core_theme',
                agent='iska',
                home_topic=home_topic,
                foreign_topic=foreign_topic,
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            
            # ì ìˆ˜ íŒŒì‹±
            score = self._parse_score_from_response(response, "clarity_of_core_theme")
            
            return score if score is not None else 3
            
        except Exception as e:
            print(f"âŒ í•µì‹¬ì£¼ì œ ëª…í™•ì„± í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 3

    def evaluate_reference_groundedness(self, passage: str, home_context: Optional[str] = None, 
                           foreign_context: Optional[str] = None, home_topic: str = "", foreign_topic: str = "") -> int:
        """
        ì°¸ê³ ìë£Œ ê¸°ë°˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            home_context: í•œêµ­ ì»¨í…ìŠ¤íŠ¸ (None ê°€ëŠ¥)
            foreign_context: ì™¸êµ­ ì»¨í…ìŠ¤íŠ¸ (None ê°€ëŠ¥)
            home_topic: í•œêµ­ ì£¼ì œ
            foreign_topic: ì™¸êµ­ ì£¼ì œ
            
        Returns:
            int: ì°¸ê³ ìë£Œ ê¸°ë°˜ì„± ì ìˆ˜ (1-5ì )
        """        
        try:
            prompt = get_prompt(
                'passage_eval.reference_groundedness',
                agent='iska',
                korean_context=home_context or "N/A",
                foreign_context=foreign_context or "N/A",
                korean_topic=home_topic or "N/A",
                foreign_topic=foreign_topic or "N/A",
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            
            # ì ìˆ˜ íŒŒì‹±
            score = self._parse_score_from_response(response, "reference_groundedness")
            
            # ì ìˆ˜ ë²”ìœ„ ì œí•œ
            if score is not None:
                score = min(score, 5)
                score = max(score, 1)
                return score
            else:
                return 3  # ê¸°ë³¸ ì ìˆ˜
            
        except Exception as e:
            print(f"âŒ ì°¸ê³ ìë£Œ ê¸°ë°˜ì„± í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 3  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì ìˆ˜

    def evaluate_logical_flow(self, passage: str) -> int:
        """
        ë…¼ë¦¬ì  íë¦„ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            
        Returns:
            int: ë…¼ë¦¬ì  íë¦„ ì ìˆ˜ (1-5ì )
        """        
        try:
            prompt = get_prompt(
                'passage_eval.logical_flow',
                agent='iska',
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            
            # ì ìˆ˜ íŒŒì‹±
            score = self._parse_score_from_response(response, "logical_flow")
            
            return score if score is not None else 3
            
        except Exception as e:
            print(f"âŒ ë…¼ë¦¬ì  íë¦„ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 3

    def evaluate_l2_learner_suitability(self, passage: str) -> int:
        """
        L2 í•™ìŠµì ì í•©ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            
        Returns:
            int: L2 í•™ìŠµì ì í•©ì„± ì ìˆ˜ (1-5ì )
        """        
        try:
            prompt = get_prompt(
                'passage_eval.l2_learner_suitability',
                agent='iska',
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            
            # ì ìˆ˜ íŒŒì‹±
            score = self._parse_score_from_response(response, "l2_learner_suitability")
            
            return score if score is not None else 3
            
        except Exception as e:
            print(f"âŒ L2 í•™ìŠµì ì í•©ì„± í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 3

    def evaluate_korean_quality(self, passage: str) -> int:
        """
        í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            
        Returns:
            int: í•œêµ­ì–´ í’ˆì§ˆ ì ìˆ˜ (1-5ì )
        """        
        try:
            prompt = get_prompt(
                'passage_eval.korean_quality',
                agent='iska',
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            
            # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ êµì •ëœ ë¬¸ì¥ ì¶”ì¶œ
            original_text = passage.strip()
            corrected_text = self._extract_corrected_passage(response, original_text)
            
            # Calculate normalized score and convert to 1-5 scale
            normalized_score = self.calculate_normalized_score(original_text, corrected_text)
            grammar_score = self.scale_to_1_5(normalized_score)
            
            return grammar_score
            
        except Exception as e:
            print(f"âŒ í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 5  # ì˜¤ë¥˜ ì‹œ ë§Œì ìœ¼ë¡œ ê°„ì£¼ (ì˜¤ë¥˜ ì—†ìŒìœ¼ë¡œ íŒë‹¨)

    def evaluate_binary_rubric(self, passage: str, problem_types: List[str], eval_goals: List[str],
                              korean_topic: str, foreign_topic: str, 
                              korean_context: str, foreign_context: str,
                              template_key: Optional[str] = None) -> Dict:
        """
        ì´ì§„ ë£¨ë¸Œë¦­ì„ ì‚¬ìš©í•œ ì¢…í•© í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            problem_types: ë¬¸ì œ ìœ í˜• ë¦¬ìŠ¤íŠ¸ (3ê°œ)
            eval_goals: í‰ê°€ ëª©í‘œ ë¦¬ìŠ¤íŠ¸ (3ê°œ)
            korean_topic: í•œêµ­ ì£¼ì œ
            foreign_topic: ì™¸êµ­ ì£¼ì œ
            korean_context: í•œêµ­ ì°¸ê³  ìë£Œ
            foreign_context: ì™¸êµ­ ì°¸ê³  ìë£Œ
            template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
            
        Returns:
            Dict: ì´ì§„ í‰ê°€ ê²°ê³¼ (true/false + feedback)
        """
        template_key = template_key or self.template_key
        
        try:
            print("ğŸ” ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # 3ê°œì˜ ë¬¸ì œ ìœ í˜•ê³¼ í‰ê°€ ëª©í‘œê°€ í•„ìš”
            if len(problem_types) < 3 or len(eval_goals) < 3:
                print("âš ï¸ ë¬¸ì œ ìœ í˜•ê³¼ í‰ê°€ ëª©í‘œê°€ ê°ê° ìµœì†Œ 3ê°œì”© í•„ìš”í•©ë‹ˆë‹¤.")
                return {
                    "scores": {
                        "completeness_for_guidelines": False,
                        "clarity_of_core_theme": False,
                        "reference_groundedness": False,
                        "logical_flow": False,
                        "korean_quality": False,
                        "l2_learner_suitability": False
                    },
                    "feedback": "í‰ê°€ ë©”íƒ€ë°ì´í„°ê°€ ë¶ˆì¶©ë¶„í•©ë‹ˆë‹¤. ë¬¸ì œ ìœ í˜•ê³¼ í‰ê°€ ëª©í‘œê°€ ê°ê° 3ê°œì”© í•„ìš”í•©ë‹ˆë‹¤."
                }
            
            prompt = get_prompt(
                template_key,
                agent='iska',
                problem_type1=problem_types[0], eval_goal1=eval_goals[0],
                problem_type2=problem_types[1], eval_goal2=eval_goals[1],
                problem_type3=problem_types[2], eval_goal3=eval_goals[2],
                korean_topic=korean_topic, korean_context=korean_context,
                foreign_topic=foreign_topic, foreign_context=foreign_context,
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            # print(response)
            # JSON ì‘ë‹µ íŒŒì‹±
            result = self._parse_binary_rubric_response(response)
            
            print("âœ… ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ ì™„ë£Œ!")
            return result
            
        except Exception as e:
            print(f"âŒ ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "scores": {
                    "completeness_for_guidelines": False,
                    "clarity_of_core_theme": False,
                    "reference_groundedness": False,
                    "logical_flow": False,
                    "korean_quality": False,
                    "l2_learner_suitability": False
                },
                "feedback": f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }

    def _parse_binary_rubric_response(self, response: str) -> Dict:
        """ì´ì§„ ë£¨ë¸Œë¦­ ì‘ë‹µì—ì„œ JSONì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        import json
        import re
        
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                print(f"ğŸ” íŒŒì‹± ì‹œë„í•œ JSON: {json_str}")
                
                result = json.loads(json_str)
                
                # í‚¤ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬: API ì‘ë‹µ í‚¤ -> í‘œì¤€ í‚¤
                key_mapping = {
                    "is_completeness_for_guidelines": "completeness_for_guidelines",
                    "is_clarity_of_core_theme": "clarity_of_core_theme", 
                    "is_reference_grounded": "reference_groundedness",
                    "is_logical_flow": "logical_flow",
                    "has_high_korean_quality": "korean_quality",
                    "is_learner_appropriate": "l2_learner_suitability"
                }
                
                # Case 1: scores í‚¤ê°€ ìˆëŠ” ê²½ìš° (ì¤‘ì²© êµ¬ì¡°)
                if "scores" in result and isinstance(result["scores"], dict):
                    raw_scores = result["scores"]
                    scores = {}
                    
                    # í‚¤ ë§¤í•‘ ì ìš©
                    for api_key, standard_key in key_mapping.items():
                        if api_key in raw_scores:
                            value = raw_scores[api_key]
                            if isinstance(value, str):
                                scores[standard_key] = value.lower() == "true"
                            elif isinstance(value, bool):
                                scores[standard_key] = value
                            else:
                                scores[standard_key] = False
                        elif standard_key in raw_scores:
                            # ì´ë¯¸ í‘œì¤€ í‚¤ì¸ ê²½ìš°
                            value = raw_scores[standard_key]
                            if isinstance(value, str):
                                scores[standard_key] = value.lower() == "true"
                            elif isinstance(value, bool):
                                scores[standard_key] = value
                            else:
                                scores[standard_key] = False
                    
                    # ëª¨ë“  6ê°œ ë£¨ë¸Œë¦­ì´ ë§¤í•‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if len(scores) == 6:
                        return {
                            "scores": scores,
                            "feedback": result.get("feedback", "")
                        }
                
                # Case 2: í‰ê°€ í•­ëª©ì´ ìµœìƒìœ„ì— ì§ì ‘ ìˆëŠ” ê²½ìš° (í”Œë« êµ¬ì¡°)
                scores = {}
                
                # í‚¤ ë§¤í•‘ ì ìš©
                for api_key, standard_key in key_mapping.items():
                    if api_key in result:
                        value = result[api_key]
                        if isinstance(value, str):
                            scores[standard_key] = value.lower() == "true"
                        elif isinstance(value, bool):
                            scores[standard_key] = value
                        else:
                            scores[standard_key] = False
                    elif standard_key in result:
                        # ì´ë¯¸ í‘œì¤€ í‚¤ì¸ ê²½ìš°
                        value = result[standard_key]
                        if isinstance(value, str):
                            scores[standard_key] = value.lower() == "true"
                        elif isinstance(value, bool):
                            scores[standard_key] = value
                        else:
                            scores[standard_key] = False
                
                # ëª¨ë“  6ê°œ ë£¨ë¸Œë¦­ì´ ë§¤í•‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if len(scores) == 6:
                    return {
                        "scores": scores,
                        "feedback": result.get("feedback", "")
                    }
            
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            print("âš ï¸ JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return self._get_default_binary_result("JSON íŒŒì‹± ì‹¤íŒ¨")
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON ë””ì½”ë”© ì˜¤ë¥˜: {e}")
            return self._get_default_binary_result("JSON í˜•ì‹ ì˜¤ë¥˜")
        except Exception as e:
            print(f"âš ï¸ ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return self._get_default_binary_result(f"íŒŒì‹± ì˜¤ë¥˜: {str(e)}")

    def _get_default_binary_result(self, error_message: str) -> Dict:
        """ê¸°ë³¸ ì´ì§„ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (6ê°œ ë£¨ë¸Œë¦­)."""
        return {
            "scores": {
                "completeness_for_guidelines": False,
                "clarity_of_core_theme": False,
                "reference_groundedness": False,
                "logical_flow": False,
                "korean_quality": False,
                "l2_learner_suitability": False
            },
            "feedback": error_message
        }

    def evaluate_passage_metrics(self, passage: str, problem_types: List[str], eval_goals: List[str], 
                                home_context: Optional[str] = None, foreign_context: Optional[str] = None,
                                home_topic: str = "", foreign_topic: str = "") -> Dict:
        """
        ëª¨ë“  í‰ê°€ ì§€í‘œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            problem_types: ë¬¸ì œ ìœ í˜• ë¦¬ìŠ¤íŠ¸
            eval_goals: í‰ê°€ ëª©í‘œ ë¦¬ìŠ¤íŠ¸
            home_context: í•œêµ­ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
            foreign_context: ì™¸êµ­ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
            home_topic: í•œêµ­ ì£¼ì œ
            foreign_topic: ì™¸êµ­ ì£¼ì œ
            
        Returns:
            Dict: ì¢…í•© í‰ê°€ ê²°ê³¼
        """        
        print("ğŸ” ì§€ë¬¸ ì¢…í•© í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        results = {}
        
        # ê° í‰ê°€ ì§€í‘œë³„ ì‹¤í–‰ (6ê°œ ë£¨ë¸Œë¦­)
        print("   ï¿½ ì™„ì„±ë„ í‰ê°€ ì¤‘...")
        results['completeness_for_guidelines_score'] = self.evaluate_completeness_for_guidelines(passage, problem_types, eval_goals)
        
        print("   ğŸ¯ í•µì‹¬ì£¼ì œ ëª…í™•ì„± í‰ê°€ ì¤‘...")
        results['clarity_of_core_theme_score'] = self.evaluate_clarity_of_core_theme(passage, home_topic, foreign_topic)
        
        print("   ğŸ“š ì°¸ê³ ìë£Œ ê¸°ë°˜ì„± í‰ê°€ ì¤‘...")
        results['reference_groundedness_score'] = self.evaluate_reference_groundedness(passage, home_context, foreign_context, home_topic, foreign_topic)
        
        print("   ğŸ”— ë…¼ë¦¬ì  íë¦„ í‰ê°€ ì¤‘...")
        results['logical_flow_score'] = self.evaluate_logical_flow(passage)
        
        print("   ğŸ‡°ğŸ‡· í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€ ì¤‘...")
        results['korean_quality_score'] = self.evaluate_korean_quality(passage)
        
        print("   ğŸ“ L2 í•™ìŠµì ì í•©ì„± í‰ê°€ ì¤‘...")
        results['l2_learner_suitability_score'] = self.evaluate_l2_learner_suitability(passage)
        results['evaluation_timestamp'] = self._get_timestamp()
        
        print("âœ… ì§€ë¬¸ ì¢…í•© í‰ê°€ ì™„ë£Œ!")
        return results

    def evaluate_passage_comprehensive(self, passage: str, problem_types: List[str], eval_goals: List[str],
                                      korean_topic: str, foreign_topic: str,
                                      korean_context: str, foreign_context: str,
                                      use_binary_rubric: bool = True,
                                      template_key: Optional[str] = None) -> Dict:
        """
        ì§€ë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤ (ì´ì§„ ë£¨ë¸Œë¦­ + ê¸°ì¡´ ì ìˆ˜ í‰ê°€).
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            problem_types: ë¬¸ì œ ìœ í˜• ë¦¬ìŠ¤íŠ¸ (3ê°œ)
            eval_goals: í‰ê°€ ëª©í‘œ ë¦¬ìŠ¤íŠ¸ (3ê°œ)
            korean_topic: í•œêµ­ ì£¼ì œ
            foreign_topic: ì™¸êµ­ ì£¼ì œ
            korean_context: í•œêµ­ ì°¸ê³  ìë£Œ
            foreign_context: ì™¸êµ­ ì°¸ê³  ìë£Œ
            use_binary_rubric: ì´ì§„ ë£¨ë¸Œë¦­ ì‚¬ìš© ì—¬ë¶€
            template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
            
        Returns:
            Dict: ì¢…í•© í‰ê°€ ê²°ê³¼
        """
        template_key = template_key or self.template_key
        
        print("ğŸ” ì§€ë¬¸ ì¢…í•© í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        results = {
            "evaluation_type": "comprehensive",
            "template_used": template_key,
            "evaluation_timestamp": self._get_timestamp()
        }
        
        # 1. ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ (ìš°ì„  ì‹¤í–‰)
        if use_binary_rubric:
            print("   ğŸ“‹ ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ ì¤‘...")
            binary_result = self.evaluate_binary_rubric(
                passage=passage,
                problem_types=problem_types,
                eval_goals=eval_goals,
                korean_topic=korean_topic,
                foreign_topic=foreign_topic,
                korean_context=korean_context,
                foreign_context=foreign_context,
                template_key=template_key
            )
            results["binary_evaluation"] = binary_result
            
            # ì´ì§„ í‰ê°€ê°€ ëª¨ë‘ trueì¸ ê²½ìš°ì—ë§Œ ìƒì„¸ ì ìˆ˜ í‰ê°€ ì§„í–‰
            all_passed = all(binary_result["scores"].values())
            if not all_passed:
                print("âš ï¸ ì´ì§„ ë£¨ë¸Œë¦­ì—ì„œ ì‹¤íŒ¨ í•­ëª©ì´ ìˆì–´ ìƒì„¸ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                results["detailed_evaluation"] = {
                    "skipped": True,
                    "reason": "ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ì—ì„œ ì‹¤íŒ¨ í•­ëª© ì¡´ì¬"
                }
                results["overall_passed"] = False
                return results
        
        # 2. ìƒì„¸ ì ìˆ˜ í‰ê°€ (ì´ì§„ ë£¨ë¸Œë¦­ í†µê³¼ ì‹œì—ë§Œ)
        print("   ğŸ“Š ìƒì„¸ ì ìˆ˜ í‰ê°€ ì¤‘...")
        detailed_result = self.evaluate_passage_metrics(
            passage=passage,
            problem_types=problem_types,
            eval_goals=eval_goals,
            home_context=korean_context,
            foreign_context=foreign_context,
            home_topic=korean_topic,
            foreign_topic=foreign_topic,
            template_key=template_key
        )
        results["detailed_evaluation"] = detailed_result
        results["overall_passed"] = True
        
        print("âœ… ì§€ë¬¸ ì¢…í•© í‰ê°€ ì™„ë£Œ!")
        return results

    def get_binary_evaluation_summary(self, results: Dict) -> Dict:
        """ì´ì§„ í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if "binary_evaluation" not in results:
            return {"error": "ì´ì§„ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        binary_eval = results["binary_evaluation"]
        scores = binary_eval.get("scores", {})
        
        # í†µê³¼/ì‹¤íŒ¨ ê°œìˆ˜ ê³„ì‚°
        passed_count = sum(1 for value in scores.values() if value)
        total_count = len(scores)
        failed_items = [key for key, value in scores.items() if not value]
        
        # í•œêµ­ì–´ í•­ëª©ëª… ë§¤í•‘ (6ê°œ ë£¨ë¸Œë¦­)
        korean_labels = {
            "completeness_for_guidelines": "í‰ê°€ ì§€ì¹¨ ì™„ì„±ë„",
            "clarity_of_core_theme": "í•µì‹¬ì£¼ì œ ëª…í™•ì„±",
            "reference_groundedness": "ì°¸ê³ ìë£Œ ê¸°ë°˜ì„±",
            "logical_flow": "ë…¼ë¦¬ì  íë¦„",
            "korean_quality": "í•œêµ­ì–´ í’ˆì§ˆ",
            "l2_learner_suitability": "L2 í•™ìŠµì ì í•©ì„±"
        }
        
        summary = {
            "ì „ì²´ í†µê³¼ìœ¨": f"{passed_count}/{total_count}",
            "í†µê³¼ ì—¬ë¶€": passed_count == total_count,
            "ê²€ìˆ˜ í•­ëª©ë³„ ê²°ê³¼": {
                korean_labels.get(key, key): "âœ… í†µê³¼" if value else "âŒ ì‹¤íŒ¨"
                for key, value in scores.items()
            },
            "ì‹¤íŒ¨ í•­ëª©": [korean_labels.get(item, item) for item in failed_items],
            "í”¼ë“œë°±": binary_eval.get("feedback", ""),
            "í‰ê°€ ì‹œê°„": results.get("evaluation_timestamp", "N/A")
        }
        
        return summary

    def _parse_score_from_response(self, response: str, evaluation_type: str) -> Optional[int]:
        """ì‘ë‹µì—ì„œ ì ìˆ˜ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        import re
        
        # ë‹¤ì–‘í•œ ì ìˆ˜ íŒ¨í„´ ì‹œë„
        score_patterns = [
            r'(\d+)ì \s*\([^)]+\)',  # "4ì  (ìì—°ìŠ¤ëŸ¬ì›€)" format
            r'(\d+)ì ',  # "4ì " format
            r':\s*(\d+)',  # ": 4" format
            r'(\d+)\s*\(',  # "4 (ë§¤ìš° ì í•©)" format
            r'(\d+)$'  # Just number at end
        ]
        
        for pattern in score_patterns:
            matches = re.findall(pattern, response)
            if matches:
                try:
                    score = int(matches[-1] if evaluation_type == "coherence" else matches[0])
                    if 1 <= score <= 5:  # ìœ íš¨í•œ ì ìˆ˜ ë²”ìœ„ í™•ì¸
                        return score
                except ValueError:
                    continue
        
        # ë§ˆì§€ë§‰ ì‹œë„: ì²« ë²ˆì§¸ ë˜ëŠ” ë§ˆì§€ë§‰ ì¤„ì—ì„œ ìˆ«ì ì°¾ê¸°
        try:
            if evaluation_type == "coherence":
                score_line = response.strip().split('\n')[-1]
            else:
                score_line = response.strip().split('\n')[0]
            
            score_match = re.search(r'(\d+)', score_line)
            if score_match:
                score = int(score_match.group(1))
                if 1 <= score <= 5:
                    return score
        except:
            pass
        
        return None

    def _extract_corrected_passage(self, response: str, original_text: str) -> str:
        """êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ êµì •ëœ ë¬¸ì¥ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        import re
        
        # "êµì •ëœ ë¬¸ì¥:" ë’¤ì˜ ë‚´ìš© ì¶”ì¶œ
        corrected_match = re.search(r'\*\*êµì •ëœ ë¬¸ì¥:\*\*\s*\n?(.*?)(?=\n\*\*|$)', response, re.DOTALL)
        if corrected_match:
            corrected_text = corrected_match.group(1).strip()
            # ê´„í˜¸ë‚˜ ë¶€ì—°ì„¤ëª… ì œê±°
            corrected_text = re.sub(r'\(.*?\)', '', corrected_text).strip()
            if corrected_text and corrected_text != original_text:
                return corrected_text
        
        # "ì˜¤ë¥˜ ì—†ìŒ" ì²´í¬
        if "ì˜¤ë¥˜ ì—†ìŒ" in response or "ì˜¤ë¥˜ê°€ ì—†" in response:
            return original_text
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return original_text

    def _extract_explanation(self, response: str, score: Optional[int]) -> str:
        """ì‘ë‹µì—ì„œ ìƒì„¸ ì„¤ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if score is None:
            return response.strip()
        
        score_text = f"{score}ì "
        if score_text in response:
            parts = response.split(score_text, 1)
            if len(parts) > 1:
                return parts[1].strip().lstrip(':').strip()
        
        return response.strip()

    def _get_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        from datetime import datetime
        return datetime.now().isoformat()

    def get_evaluation_summary(self, results: Dict) -> Dict:
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if 'overall_score' not in results:
            return {"error": "ì „ì²´ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        summary = {
            "ì´ì ": results['overall_score'],
            "í‰ê°€ í•­ëª©ë³„ ì ìˆ˜": {
                "ì™„ì„±ë„": results.get('completeness_for_guidelines_score', 'N/A'),
                "í•µì‹¬ì£¼ì œ ëª…í™•ì„±": results.get('clarity_of_core_theme_score', 'N/A'),
                "ì°¸ê³ ìë£Œ ê¸°ë°˜ì„±": results.get('reference_groundedness_score', 'N/A'),
                "ë…¼ë¦¬ì  íë¦„": results.get('logical_flow_score', 'N/A'),
                "í•œêµ­ì–´ í’ˆì§ˆ": results.get('korean_quality_score', 'N/A'),
                "L2 í•™ìŠµì ì í•©ì„±": results.get('l2_learner_suitability_score', 'N/A')
            },
            "í‰ê°€ ì‹œê°„": results.get('evaluation_timestamp', 'N/A'),
            "ì‚¬ìš©ëœ í…œí”Œë¦¿": results.get('template_used', 'N/A')
        }
        
        return summary



# ========================= ì‚¬ìš© ì˜ˆì‹œ =========================

if __name__ == "__main__":
    print("ğŸ” PassageEvaluator ì‚¬ìš© ì˜ˆì‹œ")
    
    # ê¸°ë³¸ OpenAI í‰ê°€ê¸° ìƒì„±
    llm_client = OpenAIModelClient(model_name="gpt-4o-mini")
    evaluator = PassageEvaluator(llm_client)

    # ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë” ê°„ê²°í•˜ê²Œ
    sample_passage = "í•œêµ­ì˜ ì¶”ì„ì€ ê°€ì¡±ì´ ëª¨ì—¬ ì¡°ìƒì„ ê¸°ë¦¬ëŠ” ì „í†µ ëª…ì ˆì…ë‹ˆë‹¤. ë¯¸êµ­ì˜ ì¶”ìˆ˜ê°ì‚¬ì ˆê³¼ ë¹„ìŠ·í•˜ê²Œ ê°€ì¡±ì˜ í™”í•©ì„ ì¤‘ì‹œí•˜ì§€ë§Œ, ì¢…êµì  ìƒ‰ì±„ë³´ë‹¤ëŠ” ìœ êµì  ì „í†µì´ ê°•í•©ë‹ˆë‹¤."
    
    sample_problem_types = ["ì œëª©ì„ ë¶™ì¸ ê·¼ê±° ì„¤ëª…í•˜ê¸°", "ìë¬¸í™”ì™€ ë¹„êµí•˜ê¸°", "ì›ì¸ê³¼ ì „ë§ ì˜ˆì¸¡í•˜ê¸°"]
    sample_eval_goals = [
        "ê¸€ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³ , ì œëª©ì˜ íƒ€ë‹¹ì„±ì„ ì„¤ëª…í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•œë‹¤.",
        "ë¬¸í™” í˜„ìƒì„ ìì‹ ì˜ ë¬¸í™”ì™€ ë¹„êµ ì„¤ëª…í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•œë‹¤.",
        "ì‚¬íšŒ/ë¬¸í™”ì  í˜„ìƒì˜ ì›ì¸ê³¼ ë¯¸ë˜ ë³€í™”ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•œë‹¤."
    ]
    
    # ì¢…í•© í‰ê°€ ì‹¤í–‰
    try:
        results = evaluator.evaluate_passage_metrics(
            passage=sample_passage,
            problem_types=sample_problem_types,
            eval_goals=sample_eval_goals,
            home_context="í•œêµ­ì˜ ì¶”ì„ ê´€ë ¨ ì „í†µê³¼ ì˜ë¯¸ì— ëŒ€í•œ ìƒì„¸ ì •ë³´",
            foreign_context="ë¯¸êµ­ì˜ ì¶”ìˆ˜ê°ì‚¬ì ˆ ê´€ë ¨ ì „í†µê³¼ ì˜ë¯¸ì— ëŒ€í•œ ìƒì„¸ ì •ë³´",
            home_topic="í•œêµ­ì˜ ì¶”ì„",
            foreign_topic="ë¯¸êµ­ì˜ ì¶”ìˆ˜ê°ì‚¬ì ˆ"
        )
        
        # ê²°ê³¼ ì¶œë ¥ (6ê°œ ë£¨ë¸Œë¦­)
        print(f"ğŸ“Š í‰ê°€ ê²°ê³¼:")
        print(f"   ì™„ì„±ë„: {results.get('completeness_for_guidelines_score', 'N/A')}ì ")
        print(f"   í•µì‹¬ì£¼ì œ ëª…í™•ì„±: {results.get('clarity_of_core_theme_score', 'N/A')}ì ")
        print(f"   ì°¸ê³ ìë£Œ ê¸°ë°˜ì„±: {results.get('reference_groundedness_score', 'N/A')}ì ")
        print(f"   ë…¼ë¦¬ì  íë¦„: {results.get('logical_flow_score', 'N/A')}ì ")
        print(f"   í•œêµ­ì–´ í’ˆì§ˆ: {results.get('korean_quality_score', 'N/A')}ì ")
        print(f"   L2 í•™ìŠµì ì í•©ì„±: {results.get('l2_learner_suitability_score', 'N/A')}ì ")
        print(f"   ì´ì : {results.get('overall_score', 'N/A'):.2f}ì ")
        
        # ê²°ê³¼ ìš”ì•½
        summary = evaluator.get_evaluation_summary(results)
        print(f"ï¿½ í‰ê°€ ìš”ì•½:")
        for key, value in summary.items():
            print(f"   {key}: {value}")
            
    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
