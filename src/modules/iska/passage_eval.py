"""
ì§€ë¬¸ í‰ê°€ ëª¨ë“ˆ (Passage Evaluator)

ìƒì„±ëœ ì§€ë¬¸ì˜ í’ˆì§ˆì„ ë‹¤ê°ë„ë¡œ í‰ê°€í•˜ëŠ” ëª¨ë“ˆ:
- ì´ì§„ ë£¨ë¸Œë¦­: 5ê°€ì§€ í•­ëª© Pass/Fail í‰ê°€ (ë¹ ë¥¸ ê²€ìˆ˜)
- ìƒì„¸ ì ìˆ˜: ì¼ê´€ì„±, ì¼ì¹˜ì„±, ìì—°ìŠ¤ëŸ¬ì›€, í•œêµ­ì–´ í’ˆì§ˆ (1-5ì )
- OpenAI ê¸°ë°˜, ì¶”í›„ Reward Model ì§€ì› ì˜ˆì •
"""

import json
import re
import sys
import os
from pathlib import Path
from typing import Dict, List, Optional

# ê²½ë¡œ ì„¤ì •
sys.path.append(str(Path.cwd().parent.parent))

# ê°„ë‹¨í•œ ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ êµ¬í˜„
def levenshtein_distance(s1: str, s2: str) -> int:
    """ê°„ë‹¨í•œ ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê³„ì‚°"""
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = list(range(len(s2) + 1))
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
project_root = os.path.dirname(parent_dir)
sys.path.insert(0, parent_dir)
sys.path.insert(0, project_root)

# ëŸ°íƒ€ì„ì— import
try:
    from ..model_client import BaseModelClient, OpenAIModelClient
    from ..client_factory import ModelClientFactory
except ImportError:
    try:
        from src.modules.model_client import BaseModelClient, OpenAIModelClient
        from src.modules.client_factory import ModelClientFactory
    except ImportError:
        from modules.model_client import BaseModelClient, OpenAIModelClient
        from modules.client_factory import ModelClientFactory

try:
    from src.utils.prompt_loader import get_prompt
    from src.utils.settings_loader import get_settings
except ImportError:
    from utils.prompt_loader import get_prompt
    from utils.settings_loader import get_settings

class PassageEvaluator:
    """
    ì§€ë¬¸ í‰ê°€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ - OpenAI ê¸°ë°˜ í‰ê°€ ì‹œìŠ¤í…œ
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ì¼ê´€ì„±, ì‚¬ì‹¤ ì¼ì¹˜ì„±, ìì—°ìŠ¤ëŸ¬ì›€, í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€
    - ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ ì •ëŸ‰ì  í‰ê°€
    - ì¶”í›„ Reward Modelë¡œ êµì²´ ê°€ëŠ¥í•œ êµ¬ì¡°
    """
    
    def __init__(self, llm_client: Optional[BaseModelClient] = None, template_key: str = "iska", **kwargs):
        """
        PassageEvaluator ì´ˆê¸°í™”
        
        Args:
            llm_client: í‰ê°€ì— ì‚¬ìš©í•  LLM í´ë¼ì´ì–¸íŠ¸ (Noneì´ë©´ ê¸°ë³¸ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±)
            template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤ (ê¸°ë³¸: "iska")
            **kwargs: í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
        """
        self.template_key = template_key
        
        if llm_client is None:
            print("ğŸ”§ ê¸°ë³¸ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            try:
                # ì„¤ì •ì—ì„œ ê¸°ë³¸ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                try:
                    settings = get_settings()
                    openai_config = settings.get('external_services', {}).get('openai', {})
                    model_name = openai_config.get('model', 'gpt-4o-mini')
                except:
                    model_name = 'gpt-4o-mini'
                
                self.llm_client = ModelClientFactory.create_openai_client(
                    model_name=model_name,
                    **kwargs
                )
                print(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ: {model_name}")
            except Exception as e:
                print(f"âš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                print("ğŸ”„ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„...")
                try:
                    self.llm_client = ModelClientFactory.create_openai_client()
                except Exception as e2:
                    print(f"âŒ ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ ìƒì„±ë„ ì‹¤íŒ¨: {e2}")
                    raise RuntimeError(f"í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        else:
            self.llm_client = llm_client
            print("âœ… ì§€ë¬¸ í‰ê°€ê¸°ê°€ ì‚¬ìš©ì ì œê³µ í´ë¼ì´ì–¸íŠ¸ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # í‰ê°€ ì§€í‘œë³„ ê¸°ë³¸ ì„¤ì •
        self.evaluation_config = {
            "coherence": {"weight": 0.3, "max_score": 5},
            "consistency": {"weight": 0.3, "max_score": 5},  # factual_consistency -> consistency
            "naturalness": {"weight": 0.2, "max_score": 5},
            "korean_quality": {"weight": 0.2, "max_score": 5}
        }

    def calculate_normalized_score(self, original_text: str, corrected_text: str) -> float:
        """
        ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ë¥¼ ì´ìš©í•´ 0~1 ì‚¬ì´ì˜ ì •ê·œí™”ëœ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì™„ë²½í•œ ë¬¸ì¥ì…ë‹ˆë‹¤.
        """
        dist = levenshtein_distance(original_text, corrected_text)
        max_len = max(len(original_text), len(corrected_text))
        if max_len == 0:
            return 1.0
        return 1.0 - (dist / max_len)

    def scale_to_1_5(self, normalized_score: float) -> int:
        """0~1 ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ 1~5ì  ì²™ë„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if normalized_score >= 0.99:
            return 5  # ê±°ì˜ ì™„ë²½
        elif normalized_score >= 0.95:
            return 4  # ì‚¬ì†Œí•œ ì˜¤ë¥˜
        elif normalized_score >= 0.90:
            return 3  # ëˆˆì— ë„ëŠ” ì˜¤ë¥˜
        elif normalized_score >= 0.80:
            return 2  # ë§ì€ ì˜¤ë¥˜
        else:
            return 1  # ì‹¬ê°í•œ ì˜¤ë¥˜

    def evaluate_coherence(self, passage: str, problem_types: List[str], eval_goals: List[str], 
                          template_key: Optional[str] = None) -> int:
        """
        ì¼ê´€ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            problem_types: ë¬¸ì œ ìœ í˜• ë¦¬ìŠ¤íŠ¸
            eval_goals: í‰ê°€ ëª©í‘œ ë¦¬ìŠ¤íŠ¸  
            template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            
        Returns:
            int: ì¼ê´€ì„± ì ìˆ˜ (1-5ì )
        """
        template_key = template_key or self.template_key
        
        try:
            prompt = get_prompt(
                'passage_eval.coherence',
                agent='iska',
                problem_type1=problem_types[0], eval_goal1=eval_goals[0],
                problem_type2=problem_types[1], eval_goal2=eval_goals[1],
                problem_type3=problem_types[2], eval_goal3=eval_goals[2],
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            
            # ì ìˆ˜ íŒŒì‹±
            score = self._parse_score_from_response(response, "coherence")
            
            return score if score is not None else 3
            
        except Exception as e:
            print(f"âŒ ì¼ê´€ì„± í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 3

    def evaluate_consistency(self, passage: str, home_context: Optional[str] = None, 
                           foreign_context: Optional[str] = None, template_key: Optional[str] = None) -> int:
        """
        ì¼ì¹˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤ (ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜).
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            home_context: í•œêµ­ ì»¨í…ìŠ¤íŠ¸ (None ê°€ëŠ¥)
            foreign_context: ì™¸êµ­ ì»¨í…ìŠ¤íŠ¸ (None ê°€ëŠ¥)
            template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
            
        Returns:
            int: ì¼ì¹˜ì„± ì ìˆ˜ (1-5ì )
        """
        template_key = template_key or self.template_key
        
        try:
            prompt = get_prompt(
                'passage_eval.consistency',
                agent='iska',
                home_context=home_context or "N/A",
                foreign_context=foreign_context or "N/A",
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            
            # ì ìˆ˜ íŒŒì‹±
            score = self._parse_score_from_response(response, "consistency")
            
            # ì ìˆ˜ ë²”ìœ„ ì œí•œ
            if score is not None:
                score = min(score, 5)
                score = max(score, 1)
                return score
            else:
                return 3  # ê¸°ë³¸ ì ìˆ˜
            
        except Exception as e:
            print(f"âŒ ì¼ì¹˜ì„± í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 3  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì ìˆ˜

    def evaluate_naturalness(self, passage: str, home_topic: str, foreign_topic: str, 
                           template_key: Optional[str] = None) -> int:
        """
        ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            home_topic: í•œêµ­ ì£¼ì œ
            foreign_topic: ì™¸êµ­ ì£¼ì œ
            template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
            
        Returns:
            int: ìì—°ìŠ¤ëŸ¬ì›€ ì ìˆ˜ (1-5ì )
        """
        template_key = template_key or self.template_key
        
        try:
            prompt = get_prompt(
                'passage_eval.naturalness',
                agent='iska',
                home_topic=home_topic,
                foreign_topic=foreign_topic,
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            
            # ì ìˆ˜ íŒŒì‹±
            score = self._parse_score_from_response(response, "naturalness")
            
            return score if score is not None else 3
            
        except Exception as e:
            print(f"âŒ ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 3

    def evaluate_korean_quality(self, passage: str, template_key: Optional[str] = None) -> int:
        """
        í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
            
        Returns:
            int: í•œêµ­ì–´ í’ˆì§ˆ ì ìˆ˜ (1-5ì )
        """
        template_key = template_key or self.template_key
        
        try:
            prompt = get_prompt(
                'passage_eval.korean_quality',
                agent='iska',
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            
            # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ êµì •ëœ ë¬¸ì¥ ì¶”ì¶œ
            original_text = passage.strip()
            corrected_text = self._extract_corrected_passage(response, original_text)
            
            # Calculate normalized score and convert to 1-5 scale
            normalized_score = self.calculate_normalized_score(original_text, corrected_text)
            grammar_score = self.scale_to_1_5(normalized_score)
            
            return grammar_score
            
        except Exception as e:
            print(f"âŒ í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 5  # ì˜¤ë¥˜ ì‹œ ë§Œì ìœ¼ë¡œ ê°„ì£¼ (ì˜¤ë¥˜ ì—†ìŒìœ¼ë¡œ íŒë‹¨)

    def evaluate_binary_rubric(self, passage: str, problem_types: List[str], eval_goals: List[str],
                              korean_topic: str, foreign_topic: str, 
                              korean_context: str, foreign_context: str,
                              template_key: Optional[str] = None) -> Dict:
        """
        ì´ì§„ ë£¨ë¸Œë¦­ì„ ì‚¬ìš©í•œ ì¢…í•© í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            problem_types: ë¬¸ì œ ìœ í˜• ë¦¬ìŠ¤íŠ¸ (3ê°œ)
            eval_goals: í‰ê°€ ëª©í‘œ ë¦¬ìŠ¤íŠ¸ (3ê°œ)
            korean_topic: í•œêµ­ ì£¼ì œ
            foreign_topic: ì™¸êµ­ ì£¼ì œ
            korean_context: í•œêµ­ ì°¸ê³  ìë£Œ
            foreign_context: ì™¸êµ­ ì°¸ê³  ìë£Œ
            template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
            
        Returns:
            Dict: ì´ì§„ í‰ê°€ ê²°ê³¼ (true/false + feedback)
        """
        template_key = template_key or self.template_key
        
        try:
            print("ğŸ” ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # 3ê°œì˜ ë¬¸ì œ ìœ í˜•ê³¼ í‰ê°€ ëª©í‘œê°€ í•„ìš”
            if len(problem_types) < 3 or len(eval_goals) < 3:
                print("âš ï¸ ë¬¸ì œ ìœ í˜•ê³¼ í‰ê°€ ëª©í‘œê°€ ê°ê° ìµœì†Œ 3ê°œì”© í•„ìš”í•©ë‹ˆë‹¤.")
                return {
                    "scores": {
                        "is_guideline_compliant": False,
                        "is_factually_consistent": False,
                        "is_natural": False,
                        "has_high_korean_quality": False,
                        "is_learner_appropriate": False
                    },
                    "feedback": "í‰ê°€ ë©”íƒ€ë°ì´í„°ê°€ ë¶ˆì¶©ë¶„í•©ë‹ˆë‹¤. ë¬¸ì œ ìœ í˜•ê³¼ í‰ê°€ ëª©í‘œê°€ ê°ê° 3ê°œì”© í•„ìš”í•©ë‹ˆë‹¤."
                }
            
            prompt = get_prompt(
                template_key,
                agent='iska',
                problem_type1=problem_types[0], eval_goal1=eval_goals[0],
                problem_type2=problem_types[1], eval_goal2=eval_goals[1],
                problem_type3=problem_types[2], eval_goal3=eval_goals[2],
                korean_topic=korean_topic, korean_context=korean_context,
                foreign_topic=foreign_topic, foreign_context=foreign_context,
                passage=passage
            )
            
            response = self.llm_client.call([{"role": "user", "content": prompt}])
            # print(response)
            # JSON ì‘ë‹µ íŒŒì‹±
            result = self._parse_binary_rubric_response(response)
            
            print("âœ… ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ ì™„ë£Œ!")
            return result
            
        except Exception as e:
            print(f"âŒ ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "scores": {
                    "is_guideline_compliant": False,
                    "is_factually_consistent": False,
                    "is_natural": False,
                    "has_high_korean_quality": False,
                    "is_learner_appropriate": False
                },
                "feedback": f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }

    def _parse_binary_rubric_response(self, response: str) -> Dict:
        """ì´ì§„ ë£¨ë¸Œë¦­ ì‘ë‹µì—ì„œ JSONì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        import json
        import re
        
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                result = json.loads(json_str)
                
                # í•„ìˆ˜ í‰ê°€ í•­ëª© í‚¤ë“¤
                required_keys = [
                    "is_guideline_compliant", "is_factually_consistent", "is_natural", 
                    "has_high_korean_quality", "is_learner_appropriate"
                ]
                
                # Case 1: scores í‚¤ê°€ ìˆëŠ” ê²½ìš° (ì¤‘ì²© êµ¬ì¡°)
                if "scores" in result and isinstance(result["scores"], dict):
                    scores = result["scores"]
                    if all(key in scores for key in required_keys):
                        # boolean ê°’ í™•ì¸ ë° ë³€í™˜
                        for key in required_keys:
                            if isinstance(scores[key], str):
                                scores[key] = scores[key].lower() == "true"
                            elif not isinstance(scores[key], bool):
                                scores[key] = False
                        
                        return {
                            "scores": scores,
                            "feedback": result.get("feedback", "")
                        }
                
                # Case 2: í‰ê°€ í•­ëª©ì´ ìµœìƒìœ„ì— ì§ì ‘ ìˆëŠ” ê²½ìš° (í”Œë« êµ¬ì¡°)
                elif all(key in result for key in required_keys):
                    scores = {}
                    for key in required_keys:
                        if isinstance(result[key], str):
                            scores[key] = result[key].lower() == "true"
                        elif isinstance(result[key], bool):
                            scores[key] = result[key]
                        else:
                            scores[key] = False
                    
                    return {
                        "scores": scores,
                        "feedback": result.get("feedback", "")
                    }
            
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            print("âš ï¸ JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            print(f"ğŸ” íŒŒì‹± ì‹œë„í•œ JSON: {json_str if 'json_str' in locals() else 'JSON ì¶”ì¶œ ì‹¤íŒ¨'}")
            return self._get_default_binary_result("JSON íŒŒì‹± ì‹¤íŒ¨")
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON ë””ì½”ë”© ì˜¤ë¥˜: {e}")
            return self._get_default_binary_result("JSON í˜•ì‹ ì˜¤ë¥˜")
        except Exception as e:
            print(f"âš ï¸ ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return self._get_default_binary_result(f"íŒŒì‹± ì˜¤ë¥˜: {str(e)}")

    def _get_default_binary_result(self, error_message: str) -> Dict:
        """ê¸°ë³¸ ì´ì§„ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "scores": {
                "is_guideline_compliant": False,
                "is_factually_consistent": False,
                "is_natural": False,
                "has_high_korean_quality": False,
                "is_learner_appropriate": False
            },
            "feedback": error_message
        }

    def evaluate_passage_metrics(self, passage: str, problem_types: List[str], eval_goals: List[str], 
                                home_context: Optional[str] = None, foreign_context: Optional[str] = None,
                                home_topic: str = "", foreign_topic: str = "",
                                template_key: Optional[str] = None) -> Dict:
        """
        ëª¨ë“  í‰ê°€ ì§€í‘œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            problem_types: ë¬¸ì œ ìœ í˜• ë¦¬ìŠ¤íŠ¸
            eval_goals: í‰ê°€ ëª©í‘œ ë¦¬ìŠ¤íŠ¸
            home_context: í•œêµ­ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
            foreign_context: ì™¸êµ­ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
            home_topic: í•œêµ­ ì£¼ì œ
            foreign_topic: ì™¸êµ­ ì£¼ì œ
            template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
            
        Returns:
            Dict: ì¢…í•© í‰ê°€ ê²°ê³¼
        """
        template_key = template_key or self.template_key
        
        print("ğŸ” ì§€ë¬¸ ì¢…í•© í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        results = {}
        
        # ê° í‰ê°€ ì§€í‘œë³„ ì‹¤í–‰
        print("   ğŸ“Š ì¼ê´€ì„± í‰ê°€ ì¤‘...")
        results['coherence_score'] = self.evaluate_coherence(passage, problem_types, eval_goals, template_key)
        
        print("   ğŸ“‹ ì¼ì¹˜ì„± í‰ê°€ ì¤‘...")
        results['consistency_score'] = self.evaluate_consistency(passage, home_context, foreign_context, template_key)
        
        print("   ğŸŒŠ ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€ ì¤‘...")
        results['naturalness_score'] = self.evaluate_naturalness(passage, home_topic, foreign_topic, template_key)
        
        print("   ğŸ‡°ğŸ‡· í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€ ì¤‘...")
        results['korean_quality_score'] = self.evaluate_korean_quality(passage, template_key)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_score_simple(results)
        
        # ë©”íƒ€ ì •ë³´ ì¶”ê°€
        results['overall_score'] = overall_score
        results['template_used'] = template_key
        results['evaluation_timestamp'] = self._get_timestamp()
        
        print("âœ… ì§€ë¬¸ ì¢…í•© í‰ê°€ ì™„ë£Œ!")
        return results

    def evaluate_passage_comprehensive(self, passage: str, problem_types: List[str], eval_goals: List[str],
                                      korean_topic: str, foreign_topic: str,
                                      korean_context: str, foreign_context: str,
                                      use_binary_rubric: bool = True,
                                      template_key: Optional[str] = None) -> Dict:
        """
        ì§€ë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤ (ì´ì§„ ë£¨ë¸Œë¦­ + ê¸°ì¡´ ì ìˆ˜ í‰ê°€).
        
        Args:
            passage: í‰ê°€í•  ì§€ë¬¸
            problem_types: ë¬¸ì œ ìœ í˜• ë¦¬ìŠ¤íŠ¸ (3ê°œ)
            eval_goals: í‰ê°€ ëª©í‘œ ë¦¬ìŠ¤íŠ¸ (3ê°œ)
            korean_topic: í•œêµ­ ì£¼ì œ
            foreign_topic: ì™¸êµ­ ì£¼ì œ
            korean_context: í•œêµ­ ì°¸ê³  ìë£Œ
            foreign_context: ì™¸êµ­ ì°¸ê³  ìë£Œ
            use_binary_rubric: ì´ì§„ ë£¨ë¸Œë¦­ ì‚¬ìš© ì—¬ë¶€
            template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
            
        Returns:
            Dict: ì¢…í•© í‰ê°€ ê²°ê³¼
        """
        template_key = template_key or self.template_key
        
        print("ğŸ” ì§€ë¬¸ ì¢…í•© í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        results = {
            "evaluation_type": "comprehensive",
            "template_used": template_key,
            "evaluation_timestamp": self._get_timestamp()
        }
        
        # 1. ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ (ìš°ì„  ì‹¤í–‰)
        if use_binary_rubric:
            print("   ğŸ“‹ ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ ì¤‘...")
            binary_result = self.evaluate_binary_rubric(
                passage=passage,
                problem_types=problem_types,
                eval_goals=eval_goals,
                korean_topic=korean_topic,
                foreign_topic=foreign_topic,
                korean_context=korean_context,
                foreign_context=foreign_context,
                template_key=template_key
            )
            results["binary_evaluation"] = binary_result
            
            # ì´ì§„ í‰ê°€ê°€ ëª¨ë‘ trueì¸ ê²½ìš°ì—ë§Œ ìƒì„¸ ì ìˆ˜ í‰ê°€ ì§„í–‰
            all_passed = all(binary_result["scores"].values())
            if not all_passed:
                print("âš ï¸ ì´ì§„ ë£¨ë¸Œë¦­ì—ì„œ ì‹¤íŒ¨ í•­ëª©ì´ ìˆì–´ ìƒì„¸ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                results["detailed_evaluation"] = {
                    "skipped": True,
                    "reason": "ì´ì§„ ë£¨ë¸Œë¦­ í‰ê°€ì—ì„œ ì‹¤íŒ¨ í•­ëª© ì¡´ì¬"
                }
                results["overall_passed"] = False
                return results
        
        # 2. ìƒì„¸ ì ìˆ˜ í‰ê°€ (ì´ì§„ ë£¨ë¸Œë¦­ í†µê³¼ ì‹œì—ë§Œ)
        print("   ğŸ“Š ìƒì„¸ ì ìˆ˜ í‰ê°€ ì¤‘...")
        detailed_result = self.evaluate_passage_metrics(
            passage=passage,
            problem_types=problem_types,
            eval_goals=eval_goals,
            home_context=korean_context,
            foreign_context=foreign_context,
            home_topic=korean_topic,
            foreign_topic=foreign_topic,
            template_key=template_key
        )
        results["detailed_evaluation"] = detailed_result
        results["overall_passed"] = True
        
        print("âœ… ì§€ë¬¸ ì¢…í•© í‰ê°€ ì™„ë£Œ!")
        return results

    def get_binary_evaluation_summary(self, results: Dict) -> Dict:
        """ì´ì§„ í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if "binary_evaluation" not in results:
            return {"error": "ì´ì§„ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        binary_eval = results["binary_evaluation"]
        scores = binary_eval.get("scores", {})
        
        # í†µê³¼/ì‹¤íŒ¨ ê°œìˆ˜ ê³„ì‚°
        passed_count = sum(1 for value in scores.values() if value)
        total_count = len(scores)
        failed_items = [key for key, value in scores.items() if not value]
        
        # í•œêµ­ì–´ í•­ëª©ëª… ë§¤í•‘
        korean_labels = {
            "is_guideline_compliant": "í‰ê°€ ì§€ì¹¨ ë¶€í•©ì„±",
            "is_factually_consistent": "ì‚¬ì‹¤ ê¸°ë°˜ì„±",
            "is_natural": "ì—°ê²° ìì—°ìŠ¤ëŸ¬ì›€",
            "has_high_korean_quality": "í•œêµ­ì–´ í’ˆì§ˆ",
            "is_learner_appropriate": "L2 í•™ìŠµì ì í•©ì„±"
        }
        
        summary = {
            "ì „ì²´ í†µê³¼ìœ¨": f"{passed_count}/{total_count}",
            "í†µê³¼ ì—¬ë¶€": passed_count == total_count,
            "ê²€ìˆ˜ í•­ëª©ë³„ ê²°ê³¼": {
                korean_labels.get(key, key): "âœ… í†µê³¼" if value else "âŒ ì‹¤íŒ¨"
                for key, value in scores.items()
            },
            "ì‹¤íŒ¨ í•­ëª©": [korean_labels.get(item, item) for item in failed_items],
            "í”¼ë“œë°±": binary_eval.get("feedback", ""),
            "í‰ê°€ ì‹œê°„": results.get("evaluation_timestamp", "N/A")
        }
        
        return summary

    def _parse_score_from_response(self, response: str, evaluation_type: str) -> Optional[int]:
        """ì‘ë‹µì—ì„œ ì ìˆ˜ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        import re
        
        # ë‹¤ì–‘í•œ ì ìˆ˜ íŒ¨í„´ ì‹œë„
        score_patterns = [
            r'(\d+)ì \s*\([^)]+\)',  # "4ì  (ìì—°ìŠ¤ëŸ¬ì›€)" format
            r'(\d+)ì ',  # "4ì " format
            r':\s*(\d+)',  # ": 4" format
            r'(\d+)\s*\(',  # "4 (ë§¤ìš° ì í•©)" format
            r'(\d+)$'  # Just number at end
        ]
        
        for pattern in score_patterns:
            matches = re.findall(pattern, response)
            if matches:
                try:
                    score = int(matches[-1] if evaluation_type == "coherence" else matches[0])
                    if 1 <= score <= 5:  # ìœ íš¨í•œ ì ìˆ˜ ë²”ìœ„ í™•ì¸
                        return score
                except ValueError:
                    continue
        
        # ë§ˆì§€ë§‰ ì‹œë„: ì²« ë²ˆì§¸ ë˜ëŠ” ë§ˆì§€ë§‰ ì¤„ì—ì„œ ìˆ«ì ì°¾ê¸°
        try:
            if evaluation_type == "coherence":
                score_line = response.strip().split('\n')[-1]
            else:
                score_line = response.strip().split('\n')[0]
            
            score_match = re.search(r'(\d+)', score_line)
            if score_match:
                score = int(score_match.group(1))
                if 1 <= score <= 5:
                    return score
        except:
            pass
        
        return None

    def _extract_corrected_passage(self, response: str, original_text: str) -> str:
        """êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ êµì •ëœ ë¬¸ì¥ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        import re
        
        # "êµì •ëœ ë¬¸ì¥:" ë’¤ì˜ ë‚´ìš© ì¶”ì¶œ
        corrected_match = re.search(r'\*\*êµì •ëœ ë¬¸ì¥:\*\*\s*\n?(.*?)(?=\n\*\*|$)', response, re.DOTALL)
        if corrected_match:
            corrected_text = corrected_match.group(1).strip()
            # ê´„í˜¸ë‚˜ ë¶€ì—°ì„¤ëª… ì œê±°
            corrected_text = re.sub(r'\(.*?\)', '', corrected_text).strip()
            if corrected_text and corrected_text != original_text:
                return corrected_text
        
        # "ì˜¤ë¥˜ ì—†ìŒ" ì²´í¬
        if "ì˜¤ë¥˜ ì—†ìŒ" in response or "ì˜¤ë¥˜ê°€ ì—†" in response:
            return original_text
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return original_text

    def _extract_explanation(self, response: str, score: Optional[int]) -> str:
        """ì‘ë‹µì—ì„œ ìƒì„¸ ì„¤ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if score is None:
            return response.strip()
        
        score_text = f"{score}ì "
        if score_text in response:
            parts = response.split(score_text, 1)
            if len(parts) > 1:
                return parts[1].strip().lstrip(':').strip()
        
        return response.strip()

    def _calculate_overall_score_simple(self, results: Dict) -> float:
        """ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ ì „ì²´ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (ë‹¨ìˆœ ë²„ì „)."""
        total_weighted_score = 0.0
        total_weight = 0.0
        
        score_keys = {
            "coherence_score": 0.3,
            "consistency_score": 0.3,
            "naturalness_score": 0.2,
            "korean_quality_score": 0.2
        }
        
        for score_key, weight in score_keys.items():
            if score_key in results and isinstance(results[score_key], (int, float)):
                total_weighted_score += results[score_key] * weight
                total_weight += weight
        
        return total_weighted_score / total_weight if total_weight > 0 else 0.0

    def _get_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        from datetime import datetime
        return datetime.now().isoformat()

    def get_evaluation_summary(self, results: Dict) -> Dict:
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if 'overall_score' not in results:
            return {"error": "ì „ì²´ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        summary = {
            "ì´ì ": results['overall_score'],
            "í‰ê°€ í•­ëª©ë³„ ì ìˆ˜": {
                "ì¼ê´€ì„±": results.get('coherence_score', 'N/A'),
                "ì¼ì¹˜ì„±": results.get('consistency_score', 'N/A'),
                "ìì—°ìŠ¤ëŸ¬ì›€": results.get('naturalness_score', 'N/A'),
                "í•œêµ­ì–´ í’ˆì§ˆ": results.get('korean_quality_score', 'N/A')
            },
            "í‰ê°€ ì‹œê°„": results.get('evaluation_timestamp', 'N/A'),
            "ì‚¬ìš©ëœ í…œí”Œë¦¿": results.get('template_used', 'N/A')
        }
        
        return summary

# ========================= í¸ì˜ ê¸°ëŠ¥ë“¤ =========================

def create_passage_evaluator(model_type: str = "openai", model_name: Optional[str] = None, 
                            template_key: str = "iska", **kwargs) -> PassageEvaluator:
    """
    ì§€ë¬¸ í‰ê°€ê¸°ë¥¼ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜
    
    Args:
        model_type: ëª¨ë¸ íƒ€ì… ("openai", "local")
        model_name: ëª¨ë¸ ì´ë¦„ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
        **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
        
    Returns:
        PassageEvaluator: ì´ˆê¸°í™”ëœ í‰ê°€ê¸°
    """
    if model_type == "openai":
        client = ModelClientFactory.create_openai_client(
            model_name=model_name or "gpt-4o-mini", **kwargs
        )
    elif model_type == "local":
        # ì¶”í›„ Reward Model ì‚¬ìš© ì‹œ
        client = ModelClientFactory.create_local_client(
            model_name=model_name or "reward-model-korean", **kwargs
        )
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
    
    return PassageEvaluator(llm_client=client, template_key=template_key)


def create_binary_passage_evaluator(model_type: str = "openai", model_name: Optional[str] = None, 
                                   template_key: str = "iska", **kwargs) -> PassageEvaluator:
    """
    ì´ì§„ ë£¨ë¸Œë¦­ ì „ìš© ì§€ë¬¸ í‰ê°€ê¸°ë¥¼ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜
    
    Args:
        model_type: ëª¨ë¸ íƒ€ì… ("openai", "local")
        model_name: ëª¨ë¸ ì´ë¦„ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
        **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
        
    Returns:
        PassageEvaluator: ì´ì§„ ë£¨ë¸Œë¦­ìš© í‰ê°€ê¸°
    """
    print("ğŸ“‹ ì´ì§„ ë£¨ë¸Œë¦­ ì „ìš© í‰ê°€ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    return create_passage_evaluator(model_type, model_name, template_key, **kwargs)


def evaluate_passage_with_binary_rubric(passage: str, problem_types: List[str], eval_goals: List[str],
                                       korean_topic: str, foreign_topic: str,
                                       korean_context: str, foreign_context: str,
                                       model_type: str = "openai", model_name: Optional[str] = None) -> Dict:
    """
    ì´ì§„ ë£¨ë¸Œë¦­ìœ¼ë¡œ ì§€ë¬¸ì„ í‰ê°€í•˜ëŠ” ì›ìŠ¤í†± í•¨ìˆ˜
    
    Args:
        passage: í‰ê°€í•  ì§€ë¬¸
        problem_types: ë¬¸ì œ ìœ í˜• ë¦¬ìŠ¤íŠ¸ (3ê°œ)
        eval_goals: í‰ê°€ ëª©í‘œ ë¦¬ìŠ¤íŠ¸ (3ê°œ)
        korean_topic: í•œêµ­ ì£¼ì œ
        foreign_topic: ì™¸êµ­ ì£¼ì œ
        korean_context: í•œêµ­ ì°¸ê³  ìë£Œ
        foreign_context: ì™¸êµ­ ì°¸ê³  ìë£Œ
        model_type: ëª¨ë¸ íƒ€ì…
        model_name: ëª¨ë¸ ì´ë¦„
        
    Returns:
        Dict: í‰ê°€ ê²°ê³¼
    """
    evaluator = create_binary_passage_evaluator(model_type, model_name)
    
    return evaluator.evaluate_binary_rubric(
        passage=passage,
        problem_types=problem_types,
        eval_goals=eval_goals,
        korean_topic=korean_topic,
        foreign_topic=foreign_topic,
        korean_context=korean_context,
        foreign_context=foreign_context
    )


def create_reward_model_evaluator(model_name: str = "reward-model-korean", 
                                gpus: Optional[List[int]] = None,
                                template_key: str = "iska") -> PassageEvaluator:
    """
    Reward Model ê¸°ë°˜ í‰ê°€ê¸°ë¥¼ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜ (ì¶”í›„ ì‚¬ìš©)
    
    Args:
        model_name: Reward Model ì´ë¦„
        gpus: ì‚¬ìš©í•  GPU ë¦¬ìŠ¤íŠ¸
        template_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‚¤
        
    Returns:
        PassageEvaluator: Reward Model ê¸°ë°˜ í‰ê°€ê¸°
    """
    print("ğŸ¯ Reward Model ê¸°ë°˜ í‰ê°€ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    
    client = ModelClientFactory.create_local_client(
        model_name=model_name,
        gpus=gpus
    )
    
    evaluator = PassageEvaluator(llm_client=client, template_key=template_key)
    print(f"âœ… Reward Model í‰ê°€ê¸° ìƒì„± ì™„ë£Œ: {model_name}")
    
    return evaluator


# ========================= ì‚¬ìš© ì˜ˆì‹œ =========================

if __name__ == "__main__":
    print("ğŸ” PassageEvaluator ì‚¬ìš© ì˜ˆì‹œ")
    
    # ê¸°ë³¸ OpenAI í‰ê°€ê¸° ìƒì„±
    evaluator = create_passage_evaluator("openai", "gpt-4o-mini")
    
    # ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë” ê°„ê²°í•˜ê²Œ
    sample_passage = "í•œêµ­ì˜ ì¶”ì„ì€ ê°€ì¡±ì´ ëª¨ì—¬ ì¡°ìƒì„ ê¸°ë¦¬ëŠ” ì „í†µ ëª…ì ˆì…ë‹ˆë‹¤. ë¯¸êµ­ì˜ ì¶”ìˆ˜ê°ì‚¬ì ˆê³¼ ë¹„ìŠ·í•˜ê²Œ ê°€ì¡±ì˜ í™”í•©ì„ ì¤‘ì‹œí•˜ì§€ë§Œ, ì¢…êµì  ìƒ‰ì±„ë³´ë‹¤ëŠ” ìœ êµì  ì „í†µì´ ê°•í•©ë‹ˆë‹¤."
    
    sample_problem_types = ["ì œëª©ì„ ë¶™ì¸ ê·¼ê±° ì„¤ëª…í•˜ê¸°", "ìë¬¸í™”ì™€ ë¹„êµí•˜ê¸°", "ì›ì¸ê³¼ ì „ë§ ì˜ˆì¸¡í•˜ê¸°"]
    sample_eval_goals = [
        "ê¸€ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³ , ì œëª©ì˜ íƒ€ë‹¹ì„±ì„ ì„¤ëª…í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•œë‹¤.",
        "ë¬¸í™” í˜„ìƒì„ ìì‹ ì˜ ë¬¸í™”ì™€ ë¹„êµ ì„¤ëª…í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•œë‹¤.",
        "ì‚¬íšŒ/ë¬¸í™”ì  í˜„ìƒì˜ ì›ì¸ê³¼ ë¯¸ë˜ ë³€í™”ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•œë‹¤."
    ]
    
    # ì¢…í•© í‰ê°€ ì‹¤í–‰
    try:
        results = evaluator.evaluate_passage_metrics(
            passage=sample_passage,
            problem_types=sample_problem_types,
            eval_goals=sample_eval_goals,
            home_context="í•œêµ­ì˜ ì¶”ì„ ê´€ë ¨ ì „í†µê³¼ ì˜ë¯¸ì— ëŒ€í•œ ìƒì„¸ ì •ë³´",
            foreign_context="ë¯¸êµ­ì˜ ì¶”ìˆ˜ê°ì‚¬ì ˆ ê´€ë ¨ ì „í†µê³¼ ì˜ë¯¸ì— ëŒ€í•œ ìƒì„¸ ì •ë³´",
            home_topic="í•œêµ­ì˜ ì¶”ì„",
            foreign_topic="ë¯¸êµ­ì˜ ì¶”ìˆ˜ê°ì‚¬ì ˆ"
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"ğŸ“Š í‰ê°€ ê²°ê³¼:")
        print(f"   ì¼ê´€ì„±: {results.get('coherence_score', 'N/A')}ì ")
        print(f"   ì¼ì¹˜ì„±: {results.get('consistency_score', 'N/A')}ì ")
        print(f"   ìì—°ìŠ¤ëŸ¬ì›€: {results.get('naturalness_score', 'N/A')}ì ")
        print(f"   í•œêµ­ì–´ í’ˆì§ˆ: {results.get('korean_quality_score', 'N/A')}ì ")
        print(f"   ì´ì : {results.get('overall_score', 'N/A'):.2f}ì ")
        
        # ê²°ê³¼ ìš”ì•½
        summary = evaluator.get_evaluation_summary(results)
        print(f"ï¿½ í‰ê°€ ìš”ì•½:")
        for key, value in summary.items():
            print(f"   {key}: {value}")
            
    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
