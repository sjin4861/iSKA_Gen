# 실험 설계: 루브릭 기반 Reward Model의 지문 선정 능력 평가

## 1. 연구 목표

본 연구의 핵심 목표는, 특정 품질 기준(Rubric)에 따라 학습된 여러 **Reward Model(RM)을 활용하여, 주어진 지문 후보군에서 품질이 가장 우수한 상위 25개의 지문을 효과적으로 선정**하는 최적의 방법론을 탐색하는 것이다.

---

## 2. 실험 변인 (Independent Variables)

Reward Model의 성능에 영향을 미칠 것으로 예상되는 아래 3가지 변인을 설정하여 실험을 진행한다.

#### 2.1. Qwen3 모델 파라미터 크기
Student 모델로 사용될 Qwen3의 파라미터 크기를 세 가지로 나누어 실험한다.
-   **0.6B**
-   **1.7B**
-   **4B**

#### 2.2. RM 훈련 데이터셋 유형
각 루브릭을 학습시키기 위해, 서로 다른 방식으로 구축된 3종류의 선호도 쌍(Pairwise) 데이터셋을 사용한다. 각 데이터셋은 루브릭 당 1,000개의 데이터 쌍을 목표로 한다.

-   **Supervised Preference Dataset by Filtering (SPF):**
    -   **Positive (+)**: 5,000개의 지문 중, GPT-4o를 이용한 O/X 루브릭 **6개 모두를 통과**한 지문.
        -   **Negative (-)**: 위 평가에서 **6개 중 하나라도 통과하지 못한** 지문.

-   **Inter-Model Performance Preference Dataset (IMP):**
    -   **Positive (+)**: 사전 성능 평가에서 98%의 높은 루브릭 달성률을 보인 7B sLLM이 생성한 지문.
    -   **Negative (-)**: 40%의 낮은 달성률을 보인 3B sLLM이 생성한 지문.

-   **Intra-Model Contrastive Preference Dataset (ICP):**
    -   **Positive (+)**: sLLM에게 의도적으로 **모든 품질 기준을 충족**하도록 지시하여 생성한 지문.
    -   **Negative (-)**: sLLM에게 의도적으로 **특정 품질 기준을 위반**하도록 지시하여 생성한 지문 (예: 어색한 번역체, 사실 불일치 등).

#### 2.3. 최종 평가 시 점수 합산 전략
각 루브릭에 대해 개별적으로 학습된 6개의 Reward Model이 출력한 점수를 종합하여 최종 순위를 매기는 두 가지 전략을 비교한다.

-   **앙상블 (Ensemble):** 6개 RM의 점수를 평균내어 최종 점수로 사용한다.
-   **최소 점수 기준 (Min Score):** 6개 RM의 점수 중 **가장 낮은 점수**를 해당 지문의 최종 점수로 사용하여, 모든 기준을 안정적으로 통과하는 지문에 높은 순위를 부여한다.

#### 2.4. 평가에 사용될 루브릭 (Rubrics for Evaluation)
본 연구의 모든 자동 평가 및 RM 훈련에는 아래 6가지 O/X 루브릭이 일관되게 사용된다.

### Ⅰ. 기획 의도 부합성 (Task Fulfillment & Coherence)
*이 지문이 주어진 기획 의도와 평가 목표를 충실히 따르고 있는가?*

1.  **평가 지침 완전성 (Completeness for Guidelines)**
    * **기준**: 지문이 주어진 3개의 `평가 목표` 각각에 대한 질문을 모두 만들 수 있을 만큼 충분하고 균형 잡힌 정보를 포함하고 있는가? 특정 목표에 대한 내용만 편중되어 있지는 않은가?
    * **세부 질문**:
        * 이 지문으로 "자문화와 비교하기" 문항을 만들 수 있는가?
        * "원인과 전망 예측하기"에 필요한 단서가 지문에 포함되어 있는가?

2.  **핵심 주제 명확성 (Clarity of Core Theme)**
    * **기준**: 지문이 하나의 통일되고 명확한 핵심 메시지를 전달하고 있는가? 내용이 산만하거나 주제에서 벗어나는 부분이 있지는 않은가?
    * **세부 질문**:
        * 지문을 한 문장으로 요약했을 때, 주제가 명확히 드러나는가?
        * 주제와 관련 없는 불필요한 정보가 포함되어 있지는 않은가?

---

### Ⅱ. 내용의 신뢰성 및 품질 (Content Fidelity & Quality)

#### 3. 참고 자료 기반성 (Reference-Groundedness)
* **기준**: 생성된 지문의 모든 내용이 함께 제공된 참고 자료의 정보에 의해서만 뒷받침되는가? 참고 자료에 없거나, 그 내용과 상충되는 내용이 포함되지는 않았는가?
* **세부 질문**:
    * 지문에 포함된 모든 주장과 설명이 참고 자료에서 근거를 찾을 수 있는가?
    * 모델이 자신의 배경지식을 사용하여 참고 자료에 없는 내용을 추가한 것으로 의심되는 부분이 있지는 않는가?

4.  **논리적 흐름 및 구조 (Logical Flow & Structure)**
    * **기준**: 글이 도입-본론-결론과 같은 명확한 구조를 가지고 있으며, 문장과 문단 간의 연결이 논리적으로 자연스러운가? (단순 정보 나열이 아닌가?)
    * **세부 질문**:
        * 첫 문장이 주제를 효과적으로 소개하고 있는가?
        * 접속어나 지시어가 올바르게 사용되어 문장 간의 관계를 명확히 보여주는가?

---

### Ⅲ. 언어적 표현 및 교육적 적합성 (Linguistic Expression & Pedagogical Suitability)
*이 지문이 L2 학습자가 이해하고 학습하기에 적절한 언어로 작성되었는가?*

5.  **한국어 품질 (Korean Quality)**
    * **기준**: 지문에 명백한 문법, 맞춤법, 띄어쓰기 오류가 없는가? 원어민이 읽었을 때 어색하거나 번역투로 느껴지는 표현은 없는가?
    * **세부 질문**:
        * 주어와 서술어의 호응이 올바른가?
        * 문맥에 맞지 않는 어색한 어휘나 한자어가 사용되지는 않았는가?

6.  **L2 학습자 적합성 (L2 Learner Suitability)**
    * **기준**: 지문의 전반적인 어휘 수준과 문장 복잡도가 L2 학습자(중급~고급)에게 적절한가? 별도의 설명 없이 이해하기 어려운 전문 용어나 배경지식이 필요한 내용은 없는가?
    * **세부 질문**:
        * 지나치게 길거나 복잡한 문장 구조가 사용되지는 않았는가?
        * 학습자가 문맥만으로 의미를 추론하기 어려운 관용 표현이나 신조어가 사용되었는가?
---

## 3. 평가 방법 (Evaluation Protocol)

1.  **평가 대상**: 사전에 별도로 생성된 100개의 테스트용 지문을 대상으로 한다.
2.  **과제**: 위에서 설정된 각 실험 조건(모델 크기, 데이터셋, 합산 전략)에 따라 훈련된 RM을 사용하여, 100개의 지문 중 **상위 25개**를 선정한다.
3.  **비교 기준 (Gold Standard)**:
    -   **상용 LLM 선정 결과**: GPT-4o가 자체적인 판단으로 선정한 상위 25개 지문.
    -   **인간 전문가 선정 결과**: 한국어 교육 전문가가 직접 선정한 상위 25개 지문.
4.  **평가 지표**:
    -   **재현율 (Recall@25)**: RM이 선정한 25개 지문이, 비교 기준(LLM 또는 전문가)이 선정한 25개 지문과 얼마나 겹치는지를 측정한다.
    -   **순위 편향 중복 (Rank-Biased Overlap, RBO)**: 단순히 겹치는 개수뿐만 아니라, 순위의 유사성까지 고려하여 두 목록이 얼마나 유사한지를 측정한다.


### 훈련 및 평가 전략 (Training and Evaluation Strategy)

본 연구의 실험은 총 3개의 라운드로 구성된 체계적인 절차를 통해 진행된다.

---
#### **Round 1: 대규모 데이터셋 구축 (Dataset Construction)**

첫 단계는 Reward Model(RM) 훈련의 기반이 될 대규모 선호도 데이터셋(Preference Dataset)을 구축하는 것이다. iSKA-Gen 지문 생성 벤치마크를 활용하여, 각기 다른 특성을 가진 sLLM으로 총 5,000개의 초기 지문 풀(pool)을 생성한다. 이후, 이 초기 풀을 바탕으로 아래 3가지 방법론에 따라 최종 훈련 데이터셋을 구축한다.

-   **Supervised Preference by Filtering (SPF):** GPT-4o를 평가자로 활용하여, 6개의 O/X 루브릭을 모두 통과한 지문을 Positive로, 하나라도 통과하지 못한 지문을 Negative로 레이블링하여 선호도 쌍을 구성한다.

-   **Inter-Model Performance Preference (IMP):** 사전 성능 평가에서 우수한 성능을 보인 sLLM(7B)이 생성한 지문을 Positive로, 저조한 성능을 보인 sLLM(3B)이 생성한 지문을 Negative로 하여 선호도 쌍을 구성한다.

-   **Intra-Model Contrastive Preference (ICP):** 동일한 sLLM에게 의도적으로 고품질 지문(Positive)과 저품질 지문(Negative)을 생성하도록 다른 지침을 주어 선호도 쌍을 구성한다.

---
#### **Round 2: 보상 모델 파인튜닝 (Reward Model Finetuning)**

두 번째 단계에서는 1단계에서 구축한 3가지 유형의 데이터셋을 사용하여, 각 루브릭에 특화된 Reward Model들을 파인튜닝한다. 실험 변인으로 설정된 Qwen3의 파라미터 크기(0.6B, 1.7B, 4B)별로, 각 데이터셋(SPF, IMP, ICP)과 각 루브릭(6개)에 대해 개별적인 RM을 모두 훈련시킨다.

---
#### **Round 3: 선호도 예측 성능 평가 (Preference Prediction Evaluation)**

마지막 단계에서는 2단계에서 훈련된 RM들을 활용하여, 사전에 준비된 100개의 테스트 지문 중 가장 우수한 상위 25개를 선정하는 평가를 진행한다.

-   **선정 전략**: 6개 루브릭 RM들의 점수를 **앙상블(Ensemble)** 또는 **최소 점수(Min Score)** 기준으로 합산하여 최종 순위를 결정한다.
-   **비교 대조군 (Gold Standard)**:
    -   **LLM 평가**: GPT-4o가 자체 기준으로 선정한 상위 25개 지문.
    -   **인간 평가**: 한국어 교육 전문가가 직접 선정한 상위 25개 지문.
-   **평가 지표**: RM의 선정 결과가 비교 대조군의 결과와 얼마나 일치하는지를 **재현율(Recall@25)**과 **순위 편향 중복(RBO)** 지표를 사용하여 정량적으로 평가한다.